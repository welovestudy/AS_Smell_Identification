tittle,question,answer,url,ans_count,tag
A type with __anager_ in the name - candidate for refactoring?,"I found a thesis on forums:

If you have a type with ""Manager"" in
  the name, it's a candidate for
  refactoring.

One answer:

I know it's considered a code ""smell""

So... Why? Is this thesis correct?
There are many managers out there. For example, Ogre3d uses them a lot, and this engine really has a clean architecture.","Ogre3d uses them a lot, and this engine really has a clean architecture.

It does everywhere except the ""manager"" classes.  Look at DefaultSceneManager as an example.  These are incredibly huge, nightmarish classes to ""manage.""
The problem is that most classes with a name including ""Manager"" typically violate the Single Responsibility Principle.  This isn't always true - as a class may have the single responsibility of managing one other aspect, but typically, those will be named differently.  When a class gets the name ""Manager"", it's typically because it's the class overseeing everything - and really should be broken into distinct pieces based on their individual responsibilities.",https://stackoverflow.com/questions/3983113/a-type-with-manager-in-the-name-candidate-for-refactoring,2,0
One-to-one mapping in S#arp Architecture,"There's a distinct smell of burned out circuits coming from my head, so forgive my ignorance.
I'm trying to setup a one-to-one relationship (well, let Automapper do it) in S#arp Architecture.
I have
public class User : Entity
{
    public virtual Profile Profile { get; set; }
    public virtual Basket Basket { get; set; }
    public virtual IList<Order> Orders { get; set; }
    public virtual IList<Role> Roles { get; set; }  
    ...
}

public class Basket : Entity
{
    public virtual User User { get; set; }  
    ...
}

public class Profile : Entity
{
    public virtual User User { get; set; }
    ...
}

And my db schema is
CREATE TABLE [dbo].[Users](
    [Id] [int] IDENTITY(1,1) NOT NULL,
    ...

CREATE TABLE [dbo].[Profiles](
    [Id] [int] IDENTITY(1,1) NOT NULL,
    [UserFk] [int] NULL,
    ...

CREATE TABLE [dbo].[Baskets](
    [Id] [int] IDENTITY(1,1) NOT NULL,
    [UserFk] [int] NULL,
    ...

When I run the unit test CanConfirmDatabaseMatchesMappings in MappingIntegrationTests I get the following error

NHibernate.ADOException : could not
  execute query ...
  System.Data.SqlClient.SqlException :
  Invalid column name 'ProfileFk'.
  Invalid column name 'BasketFk'.

and the sql it's trying to execute is
SELECT TOP 0
    this_.Id AS Id6_1_ ,
    ..
    user2_.ProfileFk AS ProfileFk9_0_ ,
    user2_.BasketFk AS BasketFk9_0_
FROM
    Profiles this_
    LEFT OUTER JOIN Users user2_
        ON this_.UserFk = user2_.Id

So it's looking for a ProfileFk and BasketFk field in the Users table. 
I haven't setup any customer override mappings and as far as I can see I've followed the default conventions setup in S#.
The two other mappings for IList Orders and Roles seem to map fine. So I'm guessing that it've missed something for setting up a one-to-one relationship.
What am I missing?","Got it. This is really an NHibernate problem to solve with Fluent NHibernate syntax, but it happens to be relevant for S#.
Background reading: NHibernate Mapping  and Fluent NHibernate HasOne
What you do is override the mapping for User and give it two .HasOne mappings. Then set a unique reference to the user on the Profile and Basket class:
public class UserMap : IAutoMappingOverride<User>
    {
        #region Implementation of IAutoMappingOverride<User>
        /// <summary>
        /// Alter the automapping for this type
        /// </summary>
        /// <param name=""mapping"">Automapping</param>
        public void Override(AutoMapping<User> mapping)
        {
            mapping.HasOne(u => u.Profile);
            mapping.HasOne(u => u.Basket);
        }
        #endregion
    }

public class ProfileMap : IAutoMappingOverride<Profile>
    {
        #region Implementation of IAutoMappingOverride<Profile>
        /// <summary>
        /// Alter the automapping for this type
        /// </summary>
        /// <param name=""mapping"">Automapping</param>
        public void Override(AutoMapping<Profile> mapping)
        {
            mapping.References(p => p.User).Unique().Column(""UserFk"");
        }
        #endregion
    }

public class BasketMap : IAutoMappingOverride<Basket>
    {
        #region Implementation of IAutoMappingOverride<Basket>
        /// <summary>
        /// Alter the automapping for this type
        /// </summary>
        /// <param name=""mapping"">Automapping</param>
        public void Override(AutoMapping<Basket> mapping)
        {
            mapping.References(b => b.User).Unique().Column(""UserFk"");
        }
        #endregion
    }

As a side note, at the time of writing this, NHibernate 3 has just been released. There's a great book out called NHibernate 3.0 Cookbook which I've just bought and it looks extremely useful for working with S#.",https://stackoverflow.com/questions/4368212/one-to-one-mapping-in-sarp-architecture,1,0
Java inheritance: Is there a way to specify that only one of two methods must be overridden,"For some derived classes, I want to ensure that one of two overloaded abstract methods gets overridden, but not both. Is this possible?

abstract void move();
abstract void move(int x, int y);

There is an abstract particle class that is extended by several classes. One of the derived classes receives mouse input to calculate its movement, while the others do not. All of the derived classes have a move function. What is a good way to go about coding the inheritance for this?","No, Java requires that all abstract methods be implemented by concrete subclasses.
Leave the decision of whether the parameters should be ignored to the callee, not the caller. Take for example a hypothetical 3D rendering system with multiple rendering engines:
abstract class Renderer {
  boolean isPointVisible(int x, int y);
}

class SimpleRenderer {
  @Override
  public boolean isPointVisible(int x, int y) {
    return true;
  }
}

class ComplexRenderer {
  @Override
  public boolean isPointVisible(int x, int y) {
    return x > 0 && x < 100 && y < 0 && y < 100;
  }
}",https://stackoverflow.com/questions/4567510/java-inheritance-is-there-a-way-to-specify-that-only-one-of-two-methods-must-be/4567521,4,0
File permissions with glass gdk,"I can create a JSON file in DCIM directory of Glass but the file is not visible when I access it on glass.
Here is the code:
File jsonFile = new File(Environment.getExternalStorageDirectory()
                + File.separator + ""DCIM/test/file.json"");
//file address    

String json = ""{\""id\"":1}"";
//text to be written on the file

FileWriter fw = new FileWriter(jsonFile);
BufferedWriter bw = new BufferedWriter(fw);
bw.write(json);
bw.close();
fw.close();
//writing the file

I can only access the file in DDMS view in Eclipse.
I have added the following permission:
android.permission.WRITE_EXTERNAL_STORAGE","You can view it with DDMS that means the file was created. I think there is no problem with your app.
If you want to open the file, you can pull the file out from your Glass using adb:
adb pull /sdcard/DCIM/test/file.json

And view it with whatever application you want.
Also see http://developer.android.com/tools/help/adb.html

To copy a file or directory (and its sub-directories) from the
  emulator or device, use

    adb pull <remote> <local>",https://stackoverflow.com/questions/26326272/file-permissions-with-glass-gdk/26334754,2,0
In which cases Idempotent PUT would be useful in REST?,"Ok. I have read at several places that using the same PUT URL several times should have the same effect. PUTting data to Api/Student/1234 should either create or update the same student with id 1234.
Now every Url will have a payload of the actual student data that needs to be added or updated in the database. Why not the ID be part of the payload? I can simply PUT data to Api/Student with the payload containing the ID.
You would say that the Url is not idempotent. My question is how will I benefit by making a PUT url idempotent? I understand that GET must be idempotent. But why would PUT need to be idempotent because the client is the one putting the data. Calling the same URL Api/Student with payload containing the same student Id will have the same effect each time. So the purpose is solved right?
Please help.","You need to have the id in the URI to allow caching proxies to function correctly.  If you do
PUT /Api/Student/23

The intermediary cache knows that any copy of /Api/student/23 that it is storing is now out of date and can be flushed from the cache.  The same goes for GETs if you don't uniquely identify the resource with the ID then the cache will not be able to store that resource representation independently.
If you follow your argument to the logical extreme then you could say that you don't need student in the URI because that information could be held in the payload.  And you are absolutely correct and it is a valid approach, it just is not REST at that point.  The REST uniform interface constraint includes ""identification of resources"" as one of the sub-constraints.",https://stackoverflow.com/questions/5523725/in-which-cases-idempotent-put-would-be-useful-in-rest/5528314,3,0
Naming Classes - How to avoid calling everything a _<WhatEver>Manager_? [closed],"As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,   or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question   can be improved and possibly reopened, visit the help center for guidance.
                            
                        


Closed 7 years ago.


A long time ago I have read an article (I believe a blog entry) which put me on the ""right"" track on naming objects: Be very very scrupulous about naming things in your program.
For example if my application was (as a typical business app) handling users, companies and addresses I'd have a User, a Company and an Address domain class - and probably somewhere a UserManager, a CompanyManager and an AddressManager would pop up that handles those things.
So can you tell what those UserManager, CompanyManager and AddressManager do? No, because Manager is a very very generic term that fits to anything you can do with your domain objects.
The article I read recommended using very specific names. If it was a C++ application and the UserManager's job was allocating and freeing users from the heap it would not manage the users but guard their birth and death. Hmm, maybe we could call this a UserShepherd.
Or maybe the UserManager's job is to examine each User object's data and sign the data cryptographically. Then we'd have a UserRecordsClerk.
Now that this idea stuck with me I try to apply it. And find this simple idea amazingly hard.
I can describe what the classes do and (as long as I don't slip into quick & dirty coding) the classes I write do exactly one thing. What I miss to go from that description to the names is a kind of catalogue of names, a vocabulary that maps the concepts to names.
Ultimately I'd like to have something like a pattern catalogue in my mind (frequently design patterns easily provide the object names, e.g. a factory)

Factory - Creates other objects (naming taken from the design pattern)
Shepherd - A shepherd handles the lifetime of objects, their creation and shutdown
Synchronizer - Copies data between two or more objects (or object hierarchies)
Nanny - Helps objects reach ""usable"" state after creation - for example by wiring to other objects
etc etc.

So, how do you handle that issue? Do you have a fixed vocabulary, do you invent new names on the fly or do you consider naming things not-so-important or wrong?
P.S.: I'm also interested in links to articles and blogs discussing the issue. As a start, here is the original article that got me thinking about it: Naming Java Classes without a 'Manager'

Update: Summary of answers
Here's a little summary of what I learned from this question in the meantime.

Try not to create new metaphors (Nanny)
Have a look at what other frameworks do

Further articles/books on this topic:

What names do you find yourself prepending/appending to classes regularly? 
What__ the best approach to naming classes?
Book: Design Patterns: Elements of Reusable Object-Oriented Software (Hardcover)
Book: Patterns of Enterprise Application Architecture (Hardcover)
Book: Implementation Patterns (Paperback)

And a current list of name prefixes/suffixes I collected (subjectively!) from the answers:

Coordinator
Builder
Writer
Reader
Handler
Container
Protocol
Target
Converter
Controller
View
Factory
Entity
Bucket

And a good tip for the road:

Don't get naming paralysis. Yes, names are very important but they're not important   enough to waste huge amounts of time on. If you can't think up a good name in 10 minutes, move on.","I asked a similar question, but where possible I try to copy the names already in the .NET framework, and I look for ideas in the Java and Android frameworks.
It seems Helper, Manager, and Util are the unavoidable nouns you attach for coordinating classes that contain no state and are generally procedural and static. An alternative is Coordinator.
You could get particularly purple prosey with the names and go for things like Minder, Overseer, Supervisor, Administrator, and Master, but as I said I prefer keeping it like the framework names you're used to.
Some other common suffixes (if that is the correct term) you also find in the .NET framework are:

Builder
Writer
Reader
Handler
Container",https://stackoverflow.com/questions/1866794/naming-classes-how-to-avoid-calling-everything-a-whatevermanager/10439145,13,0
32 bit pointer in a 64 bit Solaris compile,I know this is a strange questions but I was wondering if it was possible to make a 32 bit pointer in 64 bit compile on Solaris using g++.  The final object would need to be 64 bit however one of my pointers offsets is becomming larger on Solaris then it is in windows if I do use 64 bit to compile.  This is causing a big problem.  I was wondering if it was possible to make a 32bit pointer within my 64 bit compiled object.,"Pointer size is a property of your target architecture, so you cannot mix and match 32- and 64-bit pointers. I would strongly suggest re-thinking your design (which smells like usual mistake of casting pointers to integers and back.) You can theoretically work with ""limited-reach"" offsets, but again please ask yourself why, and what would be a better way of doing it.",https://stackoverflow.com/questions/1639349/32-bit-pointer-in-a-64-bit-solaris-compile/1639387,4,0
redux initial state as first parameter is a bad practice,"In redux the initial state is provided by using a default parameter for the first argument state, while the second argument action does not have a default. However, I consider this a code smell  (and so do others such as Airbnb guidelines) since you always need to provide the first argument (and then would use undefined to get the default which is cumbersome). I totally understand why you would want to provide that initial state, but why do they not switch the arguments so the default is 2nd to bring code in compliance with common guidelines? 
// bad
function someReducer(state = 0, action) {
  switch (action.type) {
  default: return state;
}

// good
function someReducer(action, state = 0) {
  switch (action.type) {
  default: return state;
}

redux forces our developers to use a bad practice which encourages them to do it in other places. Why would redux-developers still decide for such architecture?
UPDATE
The reason, why it is a bad practice to have defaults going first comes from language design itself. In JS the interpreter cannot tell what the intention of the developer was. As an example:
const giveMe = (goodHuman = true, candy) => (goodHuman && candy) ? 'candy' : 'tea';

So, will I get candy?
giveMe(true) // not clear! (but it is 'tea')

I cannot access the candy with a single parameter! But I guess the developer would have liked me to get some, right? However, candy is always undefined making the conditional expression evaluate to undefined. As a fallback the else-part is executed. It is not a clear intention and hard to guess what the developer wanted to express. Such code would not come through our review-process / QA.
So, in the following example, the developer made sure that we can access that candy if we want it and he beliefs that we by default are good humans who should get that candy.:
candy giveMe = (candy, goodHuman = true) => (goodHuman && candy) ? 'candy' : 'tea';
giveMe(true) // finally we get the candy!

Since people say that putting the default in front is a best practice with redux, I would consequently argue that redux has a bad design on the reducers' signature! It might origin from the fact that reducer-functions in general have a similar signature, but they do not make the first argument optional which would not make any sense! It argue that it is a bad design decision of redux to make it a ""best practice"" calling a function using undefined.","The signature of a ""reducer"" function as (state, action) is based on the signature of a callback you would pass to Array.prototype.reduce(), such as:
const sum = [1, 2, 3].reduce( (sumInProgress, currentNumber) => {
    return sumInProgress + currentNumber;
});

The first argument is the ""accumulator"" or ""previous state"" value, and the second argument is the ""current item"" value.  In Redux, those values correspond to the existing state, and the current action.  So, there is explicit precedent for that function signature.
As for initializing the state: you could always provide the app-wide initial state as the second argument to createStore(rootReducer, preloadedState).  However, the encouraged approach is that each slice reducer should take care of providing its own initial state, and do so when it is called with a state value of undefined.  The built-in combineReducers function will ensure that each slice reducer is called on startup, thus neatly initializing each piece of the overall app state.
While you could explicitly check for if(state === undefined) return initialState inside of the function, it's shorter and simpler to declare the initial state as a default value for the state argument.
So, overall, this is not a bad practice, but rather a best practice for use of Redux.  Remember that style rulesets like the AirBNB guidelines are merely opinions, not strict rules that everyone must follow.
For more info on how Redux state initialization works, see the Structuring Reducers - Initializing State section of the Redux docs.",https://stackoverflow.com/questions/45658610/redux-initial-state-as-first-parameter-is-a-bad-practice,1,0
Selecting class depending on variable,"I have two classes ClassOne and ClassTwo. I want to initialize a different one depending on a variable value, i want to do something like:
if(a == ""0"") {
let b = ClassOne();
}else{
let b = ClassTwo();
}

without having to write it everytime I need it. Something like:
let b = MainClass()
and gets called ClassOne() or ClassTwo() depending on the case, a is a global variable.","In order for this to work the two types should be related either by extending a common base class or by implementing the same protocol. Further, subsequent operations on b would be restricted to these the two classes have in common.
If you are fine with that restriction, you can do it like this:
protocol CommonProtocol {
    func foo() -> Double
    var bar : Int { get }
}
class ClassOne : CommonProtocol {
    ...
}
class ClassTwo : CommonProtocol {
    ...
}
func MainClass() -> CommonProtocol {
    if(a == ""0"") {
        return ClassOne()
    } else {
        return ClassTwo()
    }
}
...
let b = MainClass()
b.foo()
print(b.bar)

Note: You could forego all of the above in favor of a completely dynamic approach  by following matt's advise.",https://stackoverflow.com/questions/36137346/selecting-class-depending-on-variable/36138756,4,0
C# ref is it like a pointer in C/C++ or a reference in C++?,"I'm working with the ref and don't understand clearly ""Is it like a pointer as in C/C++ or it's like a reference in C++?""
Why did I ask such a weak question as you thought for a moment?
Because, when I'm reading C#/.NET books, msdn or talking to C# developers I'm becoming confused by the following reasons:

C# developers suggest NOT to use ref in the arguments of a function, e.g. ...(ref Type someObject) doesn't smell good for them and they suggest ...(Type someObject), I really don't understand clearly this suggestion. The reasons I heard: better to work with the copy of object, then use it as a return value, not to corrupt memory by a reference etc... Often I hear such explanation about DB connection objects. As on my plain C/C++ experience, I really don't understand why to use a reference is a bad stuff in C#? I control the life of object and its memory allocations/re-allocations etc... I read in books and forums only advises it's bad, because you can corrupt your connection and cause a memory leak by a reference lose, so I control the life of object, I may control manually what I really want, so why is it bad?
Nowadays reading different books and talk to different people, I don't clearly understand is ref a pointer (*) or a reference like in C++ by & ? As I remember pointers in C/C++ always do allocate a space with a size of void* type - 4 bytes (the valid size depends on architecture), where hosts an address to a structure or variable. In C++ by passing a reference & there is no new allocations from the heap/stack and you work with already defined objects in memory space and there is no sub-allocating memory for a pointer externally like in plain C. So what's the ref in C#? Does .NET VM handle it like a pointer in plain C/C++ and its GC allocates temporary space for a pointer or it does a work like reference in C++? Does ref work only with a managed types correctly or for value types like bool, int it's better to switch an unsafe code and pass through a pointer in unmanaged style?","In C#, when you see something referring to a reference type (that is, a type declared with class instead of struct), then you're essentially always dealing with the object through a pointer. In C++, everything is a value type by default, whereas in C# everything is a reference type by default.
When you say ""ref"" in the C# parameter list, what you're really saying is more like a ""pointer to a pointer."" You're saying that, in the method, that you want to replace not the contents of the object, but the reference to the object itself, in the code calling your method.
Unless that is your intent, then you should just pass the reference type directly; in C#, passing reference types around is cheap (akin to passing a reference in C++).
Learn/understand the difference between value types and reference types in C#. They're a major concept in that language and things are going to be really confusing if you try to think using the C++ object model in C# land.
The following are essentially semantically equivalent programs:
#include <iostream>

class AClass
{
    int anInteger;
public:
    AClass(int integer)
        : anInteger(integer)
    {  }

    int GetInteger() const
    {
        return anInteger;
    }

    void SetInteger(int toSet)
    {
        anInteger = toSet;
    }
};

struct StaticFunctions
{
    // C# doesn't have free functions, so I'll do similar in C++
    // Note that in real code you'd use a free function for this.

    static void FunctionTakingAReference(AClass *item)
    {
        item->SetInteger(4);
    }

    static void FunctionTakingAReferenceToAReference(AClass **item)
    {
        *item = new AClass(1729);
    }
};

int main()
{
    AClass* instanceOne = new AClass(6);
    StaticFunctions::FunctionTakingAReference(instanceOne);
    std::cout << instanceOne->GetInteger() << ""\n"";

    AClass* instanceTwo;
    StaticFunctions::FunctionTakingAReferenceToAReference(&instanceTwo);
    // Note that operator& behaves similar to the C# keyword ""ref"" at the call site.
    std::cout << instanceTwo->GetInteger() << ""\n"";

    // (Of course in real C++ you're using std::shared_ptr and std::unique_ptr instead,
    //  right? :) )
    delete instanceOne;
    delete instanceTwo;
}

And for C#:
using System;

internal class AClass
{
    public AClass(int integer)
        : Integer(integer)
    {  }

    int Integer { get; set; }
}

internal static class StaticFunctions
{
    public static void FunctionTakingAReference(AClass item)
    {
        item.Integer = 4;
    }

    public static void FunctionTakingAReferenceToAReference(ref AClass item)
    {
        item = new AClass(1729);
    }
}

public static class Program
{
    public static void main()
    {
        AClass instanceOne = new AClass(6);
        StaticFunctions.FunctionTakingAReference(instanceOne);
        Console.WriteLine(instanceOne.Integer);

        AClass instanceTwo  = new AClass(1234); // C# forces me to assign this before
                                                // it can be passed. Use ""out"" instead of
                                                // ""ref"" and that requirement goes away.
        StaticFunctions.FunctionTakingAReferenceToAReference(ref instanceTwo);
        Console.WriteLine(instanceTwo.Integer);
    }
}",https://stackoverflow.com/questions/16207455/c-sharp-ref-is-it-like-a-pointer-in-c-c-or-a-reference-in-c,6,0
ReactJs in AngularJs,"As I don't know anything about React jet, I am wondering, what are the main benefits of introducing React in Angular's directives and what are down sides?
Basically I am working on some complex software with lot's of renderings in one cycle. Obviously I already excluded ngRepeat, etc. but I am wondering if React can speed up rendering in this case even further?
Is react better only in rendering lot's of element's or is it better in rendering of small amount of them as well?","That's a subject I've studied at work and here are my conclusions about adopting an hybrid solution (AngularJS 1.x and React) :
Pros :

best of both worlds (speed, testability, a lot of convenient things already are implemented in AngularJS _ $http, $cookies, whatever)
probably forces to uncouple more and more the code. Since the two worlds can't really share components/services, you'll have to adopt a nice architecture which will lilely makes the two parts easy to plug/unplug. Say you wanna use Angular2 components after all, a lot of uncoupling work will already be done.

Cons :

You'll have to glue stuff together. The glue code can end being more complicated than expected
You'll have to load two frameworks/libs. Although that might not be a problem for SAAS apps, it can very well be for regular websites with a lot of unique visitors.
You'll have to learn another framework (should that be in the ""Pros"" section ?)
Maintainance bad smell : the less different technologies, the better
An overall more complicated stack


In the end I think that if you don't absolutely need React, try to implement your component in regular JavaScript to avoid excessive $apply cycles. It's cleaner in my opinion.",https://stackoverflow.com/questions/34785268/reactjs-in-angularjs/34786662,2,0
Is there a tool to analyse makefiles?,"I want to analyse some parameters of C makefiles, e.g. compiler flags and what are the requirements to set this flag. Is there a tool helping me to analyse makefiles in this manner?
UPDATE
I am looking for what command line will be executed in which circumstances (e.g. different optimization for specific architectures, ...), especially what compiler flags are set in which circumstances.","Yes and no.
Yes there are Makefile Analysis tools available:
SYMake provides a symbolic evaluation algorithm that processes Makefiles and produces a single symbolic dependency graph (SDG) to represent all possible build rules and dependencies among files via commands.
MAKAO provides the following features:

Visualization of the build dependency graph
Querying of build targets and dependencies for their build commands and variables
Filtering of the build dependency graph using Prolog rules to reduce clutter or to identify build idioms
Verification of the presence of bad smells in the build dependency graph

remake is an enahanced version of GNU Make that adds improved error reporting, better tracing, profiling and a debugger.
No in the sense that these tools don't spell out which targets are executed and when, and they don't optimize for specific architectures, although cmake is known to be useful when dealing with multiple platforms. You can determine this by - get ready for the disappointing answer - looking at  your Makefile. 
You can manually debug your Makefile using make -n and make -np, and  make -nd(gmake specific) when you build. 
Also here, and here are some more links to help debug",https://stackoverflow.com/questions/33920654/is-there-a-tool-to-analyse-makefiles/33975437,1,0
What is a hack? [closed],"Closed. This question needs to be more focused. It is not currently accepting answers.
                            
                        










Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                        
Closed last year.



I use the term all the time... but I was just sort of thinking that I don't really have a solid denotational sense behind the term (or at least the term in the sense I want to discuss here). I'm interested in the sense of the word related to code, not the anthropomorphic idea. I'm also not interested here in the sense of the word related to intentional malicious computing (i.e. a hack to unlock secret powers in a game). What I want to explore is what it means to 'hack' in terms of writing software to solve a problem
wikipedia's def of 'hack' to me is a bit vague, but a decent starting point. It considers a hack


can refer to a solution or method which functions correctly but which is ""ugly"" in its concepion


works outside the accepted structures and norms of the environment


is not easily extendable or maintainable


can be slang for ""copy"", ""imitation"" or ""rip-off.""


These traits of a hack conform to my usage of the word--when applied to code it is always a term of derision. To my mind, a hack


Is likely to be difficult to maintain & hard to understand in the context of the rest of the code.


Is likely to cause failure of the app.


tends to indicate a poor understanding by the coder either of the problem space, usage of the language or both


tends to be the byproduct of aggressive schedules


suggests potential changes in requirements that have not been fully incorporated into the architecture of the solution (requiring an 'inorganic' workaround).


smells


all bad, bad, bad. To me, a hack in this sense is always negative, indicating either lack of time, incompetence, or sloth on the part of the developer, though a decent percentage of hacks must be written to compensate for ill-conceived designs or systems that have gained requirements which their original design cannot handle 'organically'. 
I don't think I've really captured it totally though--it's like pornography a bit: I can't really define it, but I know it when I see it. So I ask you: what does it mean to 'hack' when you are trying to solve a problem in software?","I've always preferred Paul Graham's definition:

To add to the confusion, the noun ""hack"" also has two senses. It can be either a compliment or an insult. It's called a hack when you do something in an ugly way. But when you do something so clever that you somehow beat the system, that's also called a hack. The word is used more often in the former than the latter sense, probably because ugly solutions are more common than brilliant ones.",https://stackoverflow.com/questions/3527174/what-is-a-hack,8,0
What are the implications of using _!important_ in CSS?,"I've been working on a website for a few months, and a lot of times when I've been trying to edit something, I have to use !important, for example:
div.myDiv { 
    width: 400px !important;
}

in order to make it display as expected. Is this bad practice? Or is the !important command okay to use? Can this cause anything undesired further down the line?","Yes, I'd say your example of using !important is bad practice, and it's very likely it would cause undesired effects further down the line. That doesn't mean it's never okay to use though.
What's wrong with !important:
Specificity is one of the main forces at work when the browser decides how CSS affects the page. The more specific a selector is, the more importance is added to it. This usually coincides with how often the selected element occurs. For example:
button { 
    color: black; 
}
button.highlight { 
    color: blue; 
    font-size: 1.5em;
}
button#buyNow { 
    color: green; 
    font-size: 2em;
}

On this page, all buttons are black. Except the buttons with the class ""highlight"", which are blue. Except that one unique button with the ID ""buyNow"", which is green. The importance of the entire rule (both the color and font-size in this case) is managed by the specificity of the selector.
!important, however, is added at a property level, not a selector level. If, for instance, we used this rule:
button.highlight {
    color: blue !important;
    font-size: 1.5em;
}

then the color property would have a higher importance than the font-size. In fact, the color is more important than the color in the button#buyNow selector, as opposed to the font-size (which is still governed by the regular ID vs class specificity).
An element <button class=""highlight"" id=""buyNow""> would have a font-size of 2em, but a color blue.
This means two things: 

The selector does not accurately convey the importance of all the rules inside it
The only way to override the color blue is to use another !important declaration, for example in the button#buyNow selector.

This not only makes your stylesheets a lot harder to maintain and debug, it starts a snowball effect. One !important leads to another to override it, to yet another to override that, et cetera. It almost never stays with just one. Even though one !important can be a useful short-term solution, it will come back to bite you in the ass in the long run. 
When is it okay to use:

Overriding styles in a user stylesheet.

This is what !important was invented for in the first place: to give the user a means to override website styles. It's used a lot by accessibility tools like screen readers, ad blockers, and more.

Overriding 3rd party code & inline styles.

Generally I'd say this is a case of code smell, but sometimes you just have no option. As a developer, you should aim to have as much control over your code as possible, but there are cases when your hands are tied and you just have to work with whatever is present. Use !important sparingly. 

Utility classes

Many libraries and frameworks come with utility classes like .hidden, .error, or .clearfix. They serve a single purpose, and often apply very few, but very important, rules. (display: none for a .hidden class, for example). These should override whatever other styles are currently on the element, and definitely warrant an !important if you ask me. 
Conclusion
Using the !important declaration is often considered bad practice because it has side effects that mess with one of CSS's core mechanisms: specificity. In many cases, using it could indicate poor CSS architecture. 
There are cases in which it's tolerable or even preferred, but make sure you double check that one of those cases actually applies to your situation before using it.",https://stackoverflow.com/questions/3706819/what-are-the-implications-of-using-important-in-css/3706876,9,0
Does an API based on inheritance violate OCP? Can this be achieved with a provider model/dependency injection?,"I'm designing an API for the first time, and trying to follow SOLID guidelines. One of the things I find myself struggling with is balancing OCP and testability with simplicity and ease of extensibility.
This open-source API is geared toward scientific modeling and computation. The aim is that various groups will be able to easily import their particular models into this ""pluggable"" architecture. Thus, its success as a project will depend on the ease by which these scientists can impart their domain-specific knowledge without unnecessary overhead or too steep of a learning curve.
For example, our compute engine relies on ""vectorized"" computations - we rarely need just one scalar value computed. Many models can take advantage of this and perform ""overhead"" calculations to be re-used in each scalar sub-computation. BUT, I'd like a user to be able to define a simple scalar operation which will inherit (or otherwise be provided with) a default vectorization behavior.
My goals were to make it
1) as simple as possible for novice user to implement their basic computational model
2) as simple as possible for advanced user to override the vectorization behavior
...of course while maintaining SoC, testability, etc.
After a few revisions, I have something simple and object-oriented. Computation contracts are defined via interfaces, but users are encouraged to derive from an abstract ComputationBase class that will provide the default vectorization. Here's a scaled-down representation of the design:
public interface IComputation<T1, T2, TOut>
{
    TOut Compute(T1 a, T2 b);
}

public interface IVectorizedComputation<T1, T2, TOut>
{
    IEnumerable<TOut> Compute(IEnumerable<T1> a, IEnumerable<T2> b);
}

public abstract class ComputationBase<T1, T2, TOut> :
    IComputation<T1, T2, TOut>,
    IVectorizedComputation<T1, T2, TOut>
{
    protected ComputationBase() { }

    // the consumer must implement this core method
    public abstract TOut Compute(T1 a, T2 b);

    // the consumer can optimize by overriding this ""dumb"" vectorization
    // use an IVectorizationProvider for vectorization capabilities instead?
    public virtual IEnumerable<TOut> Compute(IEnumerable<T1> a, IEnumerable<T2> b)
    {
        return
            from ai in a
            from bi in b
            select Compute(ai, bi);
    }
}

public class NoobMADCalculator
    : ComputationBase<double, double, double>
{
    // novice user implements a simple calculation model
    // CalculatorBase will use a ""dumb"" vectorization
    public override double Compute(double a, double b)
    {
        return a * b + 1337;
    }
}

public class PwnageMADCalculator
    : ComputationBase<double, double, double>
{
    public override double Compute(double a, double b)
    {
        var expensive = PerformExpensiveOperation();
        return ComputeInternal(a, b, expensive);
    }

    public override IEnumerable<double> Compute(IEnumerable<double> a, IEnumerable<double> b)
    {
        foreach (var ai in a)
        {
            // example optimization: only perform this operation once
            var expensive = PerformExpensiveOperation();
            foreach (var bi in b)
            {
                yield return ComputeInternal(ai, bi, expensive);
            }
        }
    }

    private static double PerformExpensiveOperation() { return 1337; }

    private static double ComputeInternal(double a, double b, double expensive)
    {
        return a * b + expensive;
    }
}

For the vectorized Compute in ComputationBase I had originally used a provider pattern (via constructor DI), but kept the scalar Compute as abstract. The rationale was that this was good ""protected variation"" - the base class would always ""own"" the vectorization operation, but delegate the computation to the injected provider. This also seemed generally beneficial from a testability and vectorization code re-use standpoint. I had the following problems with this, however:
1) The heterogeneity of the approaches for scalar (inheritance) and vector (provider) computations seemed likely to confuse the user, seemed overly complex for the requirements, and just had bad code smell.
2) Creating a ""separate"" provider for the vectorization was a leaky abstraction - if a provider was to do anything smart, it would typically need inside knowledge of the class' implementation. I found myself creating private nested classes to implement them, which told me it was a concern that couldn't be separated
Is this a good approach w/r/t OCP vs testability vs simplicity? How have others designed their API for extension at various levels of complexity? Would you use more dependency injection mechanisms than I've included? I'm also just as interested in good general references on good API design than I am answers to this particular example. Thanks.
Thanks,
David","If you can live without inheritance, you can just use Funcs. They offer a simple way to pass around arbitrary code and can offer something much simpler. Basically this:
Func<double, double, double> pwnageComputation;//takes 2 doubles and returns one double
pwnageComputation = (num1, num2) => 
{
    if (num1 + num2 > 1337)
        return 1;
    else if (num1 + num2 < 1337)
        return -1;
    return 0;
}

Func<>s are an implementation of Lambda expressions, which are basically wrappers around delegates to make them easier to use (, at least in c#). In this way, you can have your users write ad-hoc functions (similar to your code) but without the complexity of a class definition (they only need to provide a function). 
You can learn more about them here (second half) or here.",https://stackoverflow.com/questions/1485610/does-an-api-based-on-inheritance-violate-ocp-can-this-be-achieved-with-a-provid,1,0
Why Model.objects.create() updates existing records,"I am using django 1.4. But i don't understand why Model.objects.create() method updates existing records while creating new value.
I was trying the same case using Mode.objects.create() but failed as it also updates existing records. It seem to be defect in django ORM architecture.","This should not be the case. Model.objects.create should not be updating existing records. I think you have some other issue here, maybe a custom create that does this or django signals that is responsible for side effects. 
To debug this issue I suggest you to use connection.queries to see what kind of SQL is issued from ipython or django shell. Run python manage.py shell and run:
> from django.db import connection
> # Clear query list first
> connection.queries = []
> Model.objects.create()
> # See which queries are executed:
> print connection.queries",https://stackoverflow.com/questions/21498651/why-model-objects-create-updates-existing-records,1,0
OpenCV Neural network for images processing,"I new in AI world and try some practice.
It looks like I need some third-party experience.
Let's say I need to get rid of image defects (actually the task more tricky).
I hope that trained NN will be able to interpolate defect area.
For these reasons I try to create simple neural network.
It has input : grayscale image with deffect(72*54) and the same image with no defect.
Hidden layer has 2*72*54 neurons.
Main piece of code
   cv::Ptr<cv::ml::ANN_MLP> ann = cv::ml::ANN_MLP::create();

   int inputsCount = imageSizes.width * imageSizes.height;
   std::vector<int> layerSizes = { inputsCount, inputsCount * 2, inputsCount};
   ann->setLayerSizes(layerSizes);
   ann->setActivationFunction(cv::ml::ANN_MLP::SIGMOID_SYM);

   cv::TermCriteria tc(cv::TermCriteria::MAX_ITER + cv::TermCriteria::EPS, 50, 0.1);
   ann->setTermCriteria(tc);

   ann->setTrainMethod(cv::ml::ANN_MLP::BACKPROP, 0.0001);

   std::cout << ""Result : "" << ann->train(trainData, cv::ml::ROW_SAMPLE, resData) << std::endl;
   ann->predict(trainData, predicted);

My training dataset looks like

Trained on 10 items dataset NN gives bad results on this(same) inputs. I tried different params 

But trained on only 2 images NN gets close output (on trained data).

I suppose that it's not inappropriate approach and solution is not so easy. 
Maybe someone has some advice about parameters or neural network architecture or whole approach.","It seems that the termination criteria were fine for just two samples but were not good enough when training with a larger number of samples. Do try adjusting them, and also the learning rate.
Judging by the quality of the pixels that have been restored properly, the network architecture seems to be fine for this task. Once the network works well on 10 samples, I strongly recommend adding more training samples.",https://stackoverflow.com/questions/44640599/opencv-neural-network-for-images-processing,2,0
Automating HP Quality Center with Python or Java,"We have a project that uses HP Quality Center and one of the regular issues we face is people not updating comments on the defect.
So I was thinkingif we could come up with a small script or tool that could be used to periodically throw up a reminder and force the user to update the comments.
I came across the Open Test Architecture API and was wondering if there are any good Python or java examples for the same that I could see.
Thanks
Hari","I'm not sure there are any good samples for Java, because OTA can't be consumed by Java directly, it needs a Java to COM bridnge like JIntegra.
About Python, well you can use Python COM api's. And then any OTA example will do. You got plenty in QC documentation of OTA.
But I think the real question here is, why would you want to do it in Python or Java. Why not write what you need directly in QC using it's Workflow feature. Which will allow you to write your logic in VBScript, and have it invoked inside QC UI on user actions. For instance you can bind to the Post event of a Defect / Bug and check if there is a comment and if there is not prompt the user directly with a message.",https://stackoverflow.com/questions/2627419/automating-hp-quality-center-with-python-or-java,6,0
How can I find the location of a Javascript Syntax error when using eval?,"I have a long string that I am calling eval on. It's resulting in a syntax error. I'm trying to figure out where in the eval string the syntax error is. The page is executing in IE 9.
This code string is the result of a large process and is very legacy code. The architecture can't change now as that is out of the scope of the defect I'm working on. It's not a great system and certainly not a system I would write, but it is what it is.","You could replace the eval with console.log, run the logged string through jsbeautifier and spot the syntax error.
As in:
eval( ""very huge string"" )
-->
console.log( ""very huge string"" )",https://stackoverflow.com/questions/8233050/how-can-i-find-the-location-of-a-javascript-syntax-error-when-using-eval,4,0
Alternatives to passing a flag into a method?,"Sometimes when fixing a defect in an existing code base I might (often out of laziness) decide to change a method from:
void
MyClass::foo(uint32_t aBar)
{
    // Do something with aBar...
}

to:
void
MyClass::foo(uint32_t aBar, bool aSomeCondition)
{
    if (aSomeCondition)
    {
        // Do something with aBar...
    }
}

During a code review a colleague mentioned that a better approach would be to sub-class MyClass to provide this specialized functionality.
However, I would argue that as long as aSomeCondition doesn't violate the purpose or cohesion of MyClass it is an acceptable pattern to use.  Only if the code became infiltrated with flags and if statements would inheritance be a better option, otherwise we would be potentially be entering architecture astronaut territory.
What's the tipping point here?
Note: I just saw this related answer which suggests that an enum may be a better
choice than a bool, but I think my question still applies in this case.","There is not only one solution for this kind of problem.
Boolean has a very low semantic. If you want to add in the future a new condition you will have to add a new parameter...
After four years of maintenance your method may have half a dozen of parameters, if these parameters are all boolean it is very nice trap for maintainers.
Enum is a good choice if cases are exclusive. 
Enums can be easily migrated to a bit-mask or a context object.
Bit mask : C++ includes C language, you can use some plain old practices. Sometime a bit mask on an unsigned int is a good choice (but you loose type checking) and you can pass by mistake an incorrect mask. It is a convenient way to move smoothly from a boolean or an enum argument to this kind of pattern. 
Bit mask can be migrated with some effort to a context-object. You may have to implement some kind of bitwise arithmetics such as operator | and  operator & if you have to keep a buildtime compatibility.
Inheritence is sometime a good choice if the split of behavior is big and this behavior IS RELATED to the lifecycle of the instance. Note that you also have to use polymorphism and this is may slow down the method if this method is heavily used.
And finally inheritence induce change in all your factory code... And what will you do if you have several methods to change in an exclusive fashion ? You will clutter your code of specific classes... 
In fact, I think that this generally not a very good idea.
Method split : Another solution is sometime to split the method in several private and provide two or more public methods.
Context object : C++ and C lack of named parameter can be bypassed by adding a context parameter. I use this pattern very often, especially when I have to pass many data across level of a complex framework.
class Context{
public:
  // usually not a good idea to add public data member but to my opinion this is an exception
  bool setup:1; 
  bool foo:1;
  bool bar:1;
  ...
  Context() : setup(0), foo(0), bar(0) ... {}
};
...    

Context ctx;
ctx.setup = true; ...
MyObj.foo(ctx);

Note: 
That this is also useful to minimize access (or use) of static data or query to singleton object, TLS ...
Context object can contain a lot more of caching data related to an algorithm.
...
I let your imagination run free...
Anti patterns
I add here several anti pattern (to prevent some change of signature):
*NEVER DO THIS *

*NEVER DO THIS * use a static int/bool for argument passing (some people that do that, and this is a nightmare to remove this kind of stuff). Break at least multithreading...
*NEVER DO THIS * add a data member to pass parameter to method.",https://stackoverflow.com/questions/6107221/alternatives-to-passing-a-flag-into-a-method,6,0
How to build a streamlined development environment for micro-services,"I was thinking about the micro services architecture and wondering if folks have a good best practice for development environments.
My working assumption is that each micro service will live in it's own git repository for isolation and ease of deployment. I'm also assuming that each developer will create a fork of any repo that they are working on. 
The issue I'm considering arises where you are working on an issue that involves multiple micro-services. For example, there is a defect that impacts one micro service and how it appropriately consumes another micro-service. 
Assuming n projects are involved in the defect, you would have to check out n git repositories and configure them to work together. If they each have a Vagratefile and Dockerfile, you end up running n VMs. Ideally you'd only have 1 Vagrant VM and each serivce would just be a new Docker instance in that same VM.
A master repo/project with git sub modules could work. The problem with that is if we create a generic master repo/project then the sub module will point to the upstream not the fork for the developer.
I'm currently thinking that a master project that had some configs, vagrant and fig might do the trick.  I'm currently considering two methods of implementing this approach.

Provide a config with some defaults i.e. project_1 should be located
at ../project_id, etc 
Provide a script that will create submodules
based on the user's github account, this would create the remote for
the user's fork as well as the remote for the upstream project.

Has anybody else solved this problem or have a good workflow?","I decided to go with fig. 
If a microservice depends on another microservice fig will reference this service using ../
This assumes that all services are checked out as siblings. This can also easily be accomplished using git sub trees.
My dev_ops repro contains my Vagrant file for boot2docker, etc.",https://stackoverflow.com/questions/27891019/how-to-build-a-streamlined-development-environment-for-micro-services,1,0
List of environment based defects in manual testing,"I am doing manual testing from 1 year, but need support to know what kind of environment based defects did you encounter while testing. Kindly share your experiences.","First of all, you need to understand what is the requirement of the client. So first understand requirement then make a test plan according to it.
Prepare test cases,and perform them.",https://stackoverflow.com/questions/22377298/list-of-environment-based-defects-in-manual-testing/25383753,6,0
SslStream authentication failure,"everyone, I'm trying to write something about SSL and here's the question:
I've built things below:

CA certs (a self-made CA)
Server pfx, Server cert, Server key (signed by the self-made CA to ""localhost"")

Now I'm using .Net SslStream to test the connection:(Client and Server are in different thread, and the TCP connection was built already)
Client:
sslStream.AuthenticateAsClient(""localhost"");

Server:
sslStream.AuthenticateAsServer(serverCert); 
//serverCert is X509Certificate2 built from ""server.pfx""

the client's AuthenticateAsClient Method will throw a Exception 
""The remote certificate is invalid according to the validation procedure.""
I guess the reason is that the Server's certificate is signed by a untrusted CA, so the authentication failed, then how could I add the CA certificate to my trust list?
I tried to add code below in client code, but it won't work
        X509Store store = new X509Store(StoreName.TrustedPublisher, StoreLocation.CurrentUser);
        store.Open(OpenFlags.ReadWrite);
        store.Add(new X509Certificate2(Resources.CACertPath));
        store.Close();
        sslStream.AuthenticateAsClient(""localhost"");","The following code will avoid the Windows certificate stores and validate the chain. 
I see no reason to add the CA certificate needed to verify a chain to the hundreds in the certificate store already. That means Windows will try to verify the chain with ""hundreds + 1"" certificates rather than the one certificate truly required.
I have not figured out how to use this chain (chain2 below) by default such that there's no need for the callback. That is, install it on the ssl socket and the connection will ""just work"". And I have not figured out how install it such that its passed into the callback. That is, I have to build the chain for each invocation of the callback. I think these are architectural defects in .Net, but I might be missing something obvious.
The name of the function does not matter. Below, VerifyServerCertificate is the same callback as RemoteCertificateValidationCallback in SslStream class. You can also use it for the ServerCertificateValidationCallback in ServicePointManager.
static bool VerifyServerCertificate(object sender, X509Certificate certificate,
    X509Chain chain, SslPolicyErrors sslPolicyErrors)
{
    try
    {
        String CA_FILE = ""ca-cert.der"";
        X509Certificate2 ca = new X509Certificate2(CA_FILE);

        X509Chain chain2 = new X509Chain();
        chain2.ChainPolicy.ExtraStore.Add(ca);

        // Check all properties
        chain2.ChainPolicy.VerificationFlags = X509VerificationFlags.NoFlag;

        // This setup does not have revocation information
        chain2.ChainPolicy.RevocationMode = X509RevocationMode.NoCheck;

        // Build the chain
        chain2.Build(new X509Certificate2(certificate));

        // Are there any failures from building the chain?
        if (chain2.ChainStatus.Length == 0)
            return true;

        // If there is a status, verify the status is NoError
        bool result = chain2.ChainStatus[0].Status == X509ChainStatusFlags.NoError;
        Debug.Assert(result == true);

        return result;
    }
    catch (Exception ex)
    {
        Console.WriteLine(ex);
    }

    return false;
}",https://stackoverflow.com/questions/19206528/sslstream-authentication-failure/22703138,1,0
How do you handle a pool of unrelated small bugs in Scrum? [closed],"Closed. This question is opinion-based. It is not currently accepting answers.
                            
                        










Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.
                        
Closed 2 years ago.



We've recently adopted Scrum on the job and are running into trouble with a bunch of tiny bugs that appear after code has been accepted.  These include things such as spelling errors, and other single line fixes.  To create stories of size 0.5 for every little thing seems like a waste of time.  It takes more time to write the story and point it than it does to make the fix.  If there were only one or two of these per sprint, it would be easy to just fix them and not worry about creating stories for them.  However, if there are 10 or 20 or more because the application is large, this can start to add up to significant amounts of developer time that aren't being accounted for via Scrum.  While it may be easy to say the QA staff and product owners should be more thorough before the original story is accepted in the first place, I'm the developer so that is essentially out of my hands.
A couple imperfect ideas we've come up with so far:

Have a story that says ""90% of bugs fixed in the app"" where you then guess how many bugs will emerge in that sprint and how many can be fixed and then point it based on the anticipated workload
Have a story of size, say, 8 that is ALWAYS accepted at the end of the sprint where you fix as many bugs as you can.  This obviously requires a great deal of trust that everybody is actually doing an 8's worth of work
Record bugs but do not work on them until the next sprint.  They can be pointed individually or as a group.  This has the advantage of being more ""Scrummy"" but causes a three week delay for what are essentially 1 hour fixes.

Any suggestions?","Your third answer is the best method. A sprint is simply a commitment by the team to complete a specified amount of work in a defined period of time. If you're accepting additional work in the middle of the sprint, you're deviating from that original commitment by working on things that were not committed to by the team at the beginning of the sprint.
Here's what we do:

All stories within a sprint must be defect free in order to be considered ""done""
Any defects that are found during a sprint for a previously completed story are logged and put into the backlog. They're estimated just like anything else and prioritized by the product owner. If a product owner prioritizes new features over defects, they're choosing functionality over quality and vice-versa.
We don't assign story points to defects, but we do estimate every defect as it gets accepted into a sprint as part of planning. The team shouldn't get credit for broken functionality, but by the same token the time it takes to fix them needs to be recognized -- this accomplishes both.

Here's the problem with your other solutions:
Have a story that says ""90% of bugs fixed in the app"" where you then guess how many bugs will emerge in that sprint and how many can be fixed and then point it based on the anticipated workload
Again, see above. You want to avoid empty buckets of work that can be filled during the sprint. This defeats the purpose of a defined commitment by the team. How can the team commit to something they don't know about or haven't estimated?
Plus, this can easily spiral out of control into a product owner that will ""design by defect"" by filling that bucket with nice-to-have functionality that is really masquerading as defects.
Have a story of size, say, 8 that is ALWAYS accepted at the end of the sprint where you fix as many bugs as you can. This obviously requires a great deal of trust that everybody is actually doing an 8's worth of work
This sounds strange. The team should be accepting work at the beginning of the new sprint planning, not at the end of the previous sprint. Additionally, this will really skew your velocity over the long term. Scrum refers to Product Backlog Items, not just Stories, so there's nothing to say that you can't include defects as PBI's.
Record bugs but do not work on them until the next sprint. They can be pointed individually or as a group. This has the advantage of being more ""Scrummy"" but causes a three week delay for what are essentially 1 hour fixes.
You make an interesting point and we had some concern about this as well. However, that 1 hour fix (regardless of how quick it is) may not be time well spent when stacked up against the other things in the backlog. The bottom line is you want to push these decisions off to the product owner and give them the freedom to prioritize EVERYTHING the team spends effort on.",https://stackoverflow.com/questions/1034107/how-do-you-handle-a-pool-of-unrelated-small-bugs-in-scrum/1035428,8,0
App crashes upon launch,"I submitted my app to the app store but it got rejected it was crashing at the startup. Below are the crash logs:
{""app_name"":""my app"",""timestamp"":""2016-10-14 11:23:40.71 -0700"",""app_version"":""3.7"",""slice_uuid"":""d7f7e7c6-eedf-34d3-87a4-f20da6f38209"",""adam_id"":0,""build_version"":""1.5.3"",""bundleID"":""com.mycompany"",""share_with_app_devs"":false,""is_first_party"":false,""bug_type"":""109"",""os_version"":""iPhone OS 10.0.2 (14A456)"",""incident_id"":""73C54050-9529-407C-9323-C5C609A2EB7F"",""name"":""my app""}
Incident Identifier: 73C54050-9529-407C-9323-C5C609A2EB7F
CrashReporter Key:   2c0ba505c2c6082ea09a6f57e4d847f80be99b41
Hardware Model:      xxx
Process:             my app [330]
Path:                /private/var/containers/Bundle/Application/B7F24EF4-8716-484F-A83A-20CCCCDCFA3A/my app.app/my app
Identifier:          com.mycompany
Version:             1.5.3 (3.7)
Code Type:           ARM-64 (Native)
Role:                Foreground
Parent Process:      launchd [1]
Coalition:           com.mycompany [404]


Date/Time:           2016-10-14 11:23:40.5372 -0700
Launch Time:         2016-10-14 11:23:40.4466 -0700
OS Version:          iPhone OS 10.0.2 (14A456)
Report Version:      104

Exception Type:  EXC_CRASH (SIGABRT)
Exception Codes: 0x0000000000000000, 0x0000000000000000
Exception Note:  EXC_CORPSE_NOTIFY
Termination Description: DYLD, Library not loaded: @rpath/IBMMobileFirstPlatformFoundation.framework/IBMMobileFirstPlatformFoundation | Referenced from: /var/containers/Bundle/Application/B7F24EF4-8716-484F-A83A-20CCCCDCFA3A/my app.app/my app | Reason: image not found
Triggered by Thread:  0

Filtered syslog:
None found

Thread 0 Crashed:
0   dyld                            0x0000000100361cd8 0x10033c000 + 154840
1   dyld                            0x0000000100361668 0x10033c000 + 153192
2   dyld                            0x00000001003616b0 0x10033c000 + 153264
3   dyld                            0x0000000100340514 0x10033c000 + 17684
4   dyld                            0x00000001003429a8 0x10033c000 + 27048
5   dyld                            0x000000010033d044 0x10033c000 + 4164

Thread 0 crashed with ARM Thread State (64-bit):
    x0: 0x0000000000000006   x1: 0x0000000000000001   x2: 0x000000016fd221b0   x3: 0x00000000000000c7
    x4: 0x000000016fd21db0   x5: 0x0000000000000000   x6: 0x0000000000000000   x7: 0x00000000000001d0
    x8: 0x0000000000000020   x9: 0x0000000000000009  x10: 0x67616d69203a6e6f  x11: 0x6f6620746f6e2065
   x12: 0x20200a495520656c  x13: 0x203a6e6f73616552  x14: 0x6f6e206567616d69  x15: 0x00646e756f662074
   x16: 0x0000000000000209  x17: 0x0000000000000010  x18: 0x0000000000000000  x19: 0x0000000000000000
   x20: 0x000000016fd21db0  x21: 0x00000000000000c7  x22: 0x000000016fd221b0  x23: 0x0000000000000001
   x24: 0x0000000000000006  x25: 0x000000010036dab8  x26: 0x000000010036fbb8  x27: 0xe6afae36b31a0072
   x28: 0x000000010036d000   fp: 0x000000016fd21d80   lr: 0x0000000100361668
    sp: 0x000000016fd21d40   pc: 0x0000000100361cd8 cpsr: 0x00000000

Binary Images:
0x1000dc000 - 0x100263fff my app arm64  <d7f7e7c6eedf34d387a4f20da6f38209> /var/containers/Bundle/Application/B7F24EF4-8716-484F-A83A-20CCCCDCFA3A/my app.app/my app
0x10033c000 - 0x10036bfff dyld arm64  <fc8715469f7b30228b67a5ae12cadf10> /usr/lib/dyld

EOF

The termination description says that : 
Termination Description: DYLD, Library not loaded: @rpath/IBMMobileFirstPlatformFoundation.framework/IBMMobileFirstPlatformFoundation | Referenced from: /var/containers/Bundle/Application/B7F24EF4-8716-484F-A83A-20CCCCDCFA3A/my app.app/my app | Reason: image not found
Triggered by Thread:  0

I am using XCode 8 and iOS 10 to test my app. When I am installing it through the XCode, then it is working fine. This crash is only coming if I install the .ipa file either from iTunes or TestFlight or ApplicationCenter.
Following plugins are installed in the app:

cordova-plugin-console 1.0.4 ""Console""
cordova-plugin-device 1.1.3 ""Device""
cordova-plugin-dialogs 1.3.0 ""Notification""
cordova-plugin-globalization 1.0.4 ""Globalization""
cordova-plugin-mfp 8.0.2016080320 ""IBM MobileFirst Platform Foundation""
cordova-plugin-okhttp 2.0.0 ""OkHttp""
cordova-plugin-splashscreen 4.0.0 ""Splashscreen""
cordova-plugin-statusbar 2.2.0 ""StatusBar""
cordova-plugin-whitelist 1.3.0 ""Whitelist""

When I am validating the app after archiving it, I am getting below erro:","IBMMobilefirstPlatformFoundation.framework is packaged with i386 and x86_64 architecture slices so that applications with these framework added can be run on simulators as well. 
But these architectures in dynamic libraries are unsupported while submitting to AppStore/Generating Archive. This is a reported Xcode defect which you can find here. 
Present workaround is to remove those architectures from IBMMobilefirstPlatformFoundation.framework while submitting to AppStore/Generating Archive. This blog describes the same issues and contains a script using which one can strip off unwanted architectures ( i386 / x86_64) from dynamic libraries.",https://stackoverflow.com/questions/40050813/app-crashes-upon-launch/40054536,1,0
Fail to secure the SSL in tomcat,"Our architecture is:
external users<---https--->web server(Apache HTTP server)<----->webapp server (tomcat)
We fail to pass the IBM AppScan, which is used to detect any security defects in webapp server, because it finds our tomcat server.xml file is not added the secure=""yes"" attribute in our port.
However the secure=""yes"" attribute should not be added to the tomcat server.xml file because we do not need a secure connection between web server and webapp server.
How can we fix the issue?
Are there any secure=""yes"" attribute can be added to the configuration file of web server(Apache HTTP server)?
Thanks & Regards,
Gordon","If your users are accessing Tomcat (indirectly) through Apache httpd using TLS (https:// URL) then it is entirely appropriate to set secure=""true"" in your <Connector>. This tells your web application that the request being received is secure even when it is not (e.g. you are using plain-HTTP between httpd and Tomcat).
So, if you have set scheme=""https"" on your <Connector> then you probably want to also set secure=""true"".
This is not a configuration change that you can make on the Apache httpd side... it must be done in Tomcat.",https://stackoverflow.com/questions/27501507/fail-to-secure-the-ssl-in-tomcat,1,0
"what maintainable,scalable and modular actually means?","What does it mean actually when i read in any article/book that it will make your application maintainable,scalable and modular. 
I do have my vague understanding about them but i am am clear what actually they mean. For example :- I heard/read lot of times that mvc architecture
makes app more maintainable,scalable and modular. Here is my understanding :-
Maintainable :- If we need to accomodate minor change/ or some defect, its easy to handle that. Also code should be unit testable.
Prior to MVC world, view,controller and model roles were played by servlet. So it is difficult to acoomdodate any change. Also it was difficult 
from unit testing point of view
Scalable :- How application responds to increment in user load.
Modular :- To me this term looks very abstract. Modularization is about decomposing the application in to the modules which
can be developed by independent teams without affecting each other. Decomposition can be done based on other factors like
reusability, feature wise so that sub team  can work independently, testability.
Now how MVC makes the code modular. Probably becoz team can work independently on views, controller and models.
Please Correct me if i am wrong some where or i have missed any important aspect here","Maintainable: Other than what you have mentioned, a maintainable system also means that doing major changes should not be much of a problem (obviously this depends on the size of the change itself). That is, if the client wants to make an overhaul of the entire UI layer, then, doing so would mean that you do not need to rewrite half of your business logic and data layers.
Scalable: Not really. An application scales well if it is able to accommodate an increasing number of users without any major impacts on its ability to perform. If your application handles requests under 10ms for 1000 users but takes 1000ms for 2000, then it might be that your application is not scaling well. Scalability is usually achieved through a clever design will allows clever usage of resources, such as database connections and other mechanisms such as caching, which can reduce the usage of heavy operations.
Modular: Not exactly. An application is modular if it is loosely coupled but tightly coheased. What this means is that modules are independent from each other however, they work well in unison. So, as per my previous example, if you build your application in a modular manner, doing changes in the UI layer, should not affect your business layer, and the same goes the other way round.

As per this statement:

Now how MVC makes the code modular. Probably becoz team can work
  independently on views, controller and models.

it is my opinion that MVC makes it easier to break the system into modules. That being said, I do not think that if a system does not use MVC then the system is not modular etc. You can encounter applications which where built with the MVC pattern but are a nightmare to maintain, and the same goes the other way round.",https://stackoverflow.com/questions/29341158/what-maintainable-scalable-and-modular-actually-means,1,0
How to build a grid with AppSDK 2 where columns are dropdown values of resolution?,I want to show a summary of closed defects filtered by iteration with the total count of defects per resolution that looks similar to this:,"Here is an example of a grid that does that. Replace '111' in
workspace: '/workspace/1111'

with ObjectID of your workspace. This post shows how to find OID of workspace.
<!DOCTYPE html>
<html>
<head>
<title>closed defects by iteration</title>
<script type=""text/javascript"" src=""/apps/2.0rc1/sdk.js""></script>
<script type=""text/javascript"">
    Rally.onReady(function () {
    Ext.define('CustomApp', {
        extend: 'Rally.app.TimeboxScopedApp',
        componentCls: 'app',
        scopeType: 'iteration',
        comboboxConfig: {
        fieldLabel: 'Select an Iteration:',
        labelWidth: 100,
        width: 300
        },

        addContent: function() {
        this._arr = [];
        var that = this;
        console.log('_addContent');

        Rally.data.ModelFactory.getModel({
            type: 'Defect',
            context: {
            workspace: '/workspace/1111'
            },
            success: function(model) {
            that._allowedValuesStore = model.getField( 'Resolution' ).getAllowedValueStore( );
            console.log(""allowed values count"", that._allowedValuesStore.getCount());
            that._getDropdownValues();
            that._makeStore();
            }
        });
        },

        _getDropdownValues: function(){
        var that = this;
        this._allowedValuesStore.load({
        scope: this,
        callback: function(records, operation, success){
            console.log(records[1].get('StringValue'));
            Ext.Array.each(records, function(val) {
                    var s = val.get('StringValue');
                    that._arr.push(s);
            });
            console.log(""arr[2]"", this._arr[2])
            }
        });
        },

        _makeStore: function(){

         console.log('_makeStore');
         var filter = Ext.create('Rally.data.QueryFilter', {
                    property: 'Priority',
                    operator: '=',
                    value: 'Resolve Immediately'
                    });
                    filter = filter.or({
                    property: 'Priority',
                    operator: '=',
                    value: 'High Attention'  
                    });

                    filter = filter.and({
                    property: 'State',
                    operator: '=',
                    value: 'Closed'  
                    });


                    filter = filter.and(this.getContext().getTimeboxScope().getQueryFilter());

                    filter.toString();



         Ext.create('Rally.data.WsapiDataStore', {
            model: 'Defect',
            fetch: ['FormattedID','Name','Tasks','State','Resolution','Priority'],
            pageSize: 100,
            autoLoad: true,
            filters: [filter],
            listeners: {
            load: this._onDataLoaded,
            scope: this
            }
        }); 
        },

       onScopeChange: function() {
        console.log('onScopeChange');
        this._makeStore();
        },

        _onDataLoaded: function(store, data){
            console.log('_onDataLoaded');
            var defects = [];
            if (data.length === 0) {
                this._createGrid(defects);  
            }

            var countArchitecture=0;
            var countCodeChange=0;
            var countNotADefect=0;
            var countNone = 0;
            var countConfigurationChange =0;
            var countDatabaseChange = 0;
            var countDuplicate = 0;
            var countNeedMoreInformation =0;
            var countSoftwareLimitation =0;
            var countUserInterface = 0;


            Ext.Array.each(data, function(defect) {

                    var resolution = defect.get('Resolution');
                    switch(resolution)
                    {
                    case ""Architecture"":
                      countArchitecture++;
                      break;
                    case ""Code Change"":
                      countCodeChange++;
                      break;
                    case ""Not a Defect"":
                      countNotADefect++;
                      break;
                    case ""None"":
                      countNone++;
                      break;
                    case ""Configuration Change"":
                      countConfigurationChange++;
                      break;
                    case ""Database Change"":
                      countDatabaseChange++;
                      break;
                    case ""Duplicate"":
                      countDuplicate++;
                      break;
                    case ""Need More Information"":
                      countDuplicate++;
                      break;
                    case ""Software Limitation"":
                      countSoftwareLimitation++;
                      break;
                    case ""User Interface"":
                      countUserInterface++;
                      break;
                    default:
                      countNone++;
                    }
            });

                    var d  = {
                    'Architecture': countArchitecture,
                    'Code Change': countCodeChange,
                    'Not a Defect' : countNotADefect,
                    '' : countNone, 
                    'Configuration Change': countConfigurationChange,
                    'Database Change': countDatabaseChange,
                    'Duplicate': countDuplicate,
                    'Need More Information': countNeedMoreInformation,
                    'Software Limitation': countSoftwareLimitation,
                    'User Interface':countUserInterface,
                    };

                   defects.push(d);
                   this._createGrid(defects);         
        },

        _createGrid: function(defects) {
        console.log('_createGrid');
        console.log('this._arr[3]', this._arr[3]);
        var that = this;

        var myStore = Ext.create('Rally.data.custom.Store', {
            data: defects,
            pageSize: 100,  
            });

        var columnConfig = [];

            for (var i=0;i<this._arr.length; i++) {
                var columnConfigElement = {}; 
                columnConfigElement['text'] = that._arr[i];
                columnConfigElement['dataIndex'] = that._arr[i];
                columnConfig.push(columnConfigElement);
            }

        console.log('columnConfig', columnConfig);

        if (!this.grid) {
        this.grid = this.add({
            xtype: 'rallygrid',
            itemId: 'mygrid',
            store: myStore,
            columnCfgs: columnConfig
        });

         }else{
            this.grid.reconfigure(myStore);
         }
        }

});


            Rally.launchApp('CustomApp', {
                name:""closed defects by iteration"",
                //parentRepos:""""
            });

        });
    </script>
    <style type=""text/css"">
.app {
    margin: 10px;
}

.header {
    margin: 5px;
}

    </style>

</head>
<body></body>
</html>

This is how it looks:",https://stackoverflow.com/questions/18861941/how-to-build-a-grid-with-appsdk-2-where-columns-are-dropdown-values-of-resolutio/18862034,1,0
how to integrate bugzilla and HP quality center?,I'm working on integrating Bugzilla with HP Qc.  I'm performing this by using perl script by directly manipulating the database using sql commands.  I want to use the web services of Bugzilla. I have gone through the Bugzilla webservice API but tat wasn't enough to get started. I'm a beginner and this is the first project of my career. How do I go about this?,"Check out the Perl script bz_webservice_demo.pl in Bugzilla's contrib directory, it shows how to talk to Bugzilla via XMLRPC.",https://stackoverflow.com/questions/1871677/how-to-integrate-bugzilla-and-hp-quality-center/4445978,2,0
Why does Intel's compiler prefer NEG+ADD over SUB?,"In examining the output of various compilers for a variety of code snippets, I've noticed that Intel's C compiler (ICC) has a strong tendency to prefer emitting a pair of NEG+ADD instructions where other compilers would use a single SUB instruction.
As a simple example, consider the following C code:
uint64_t Mod3(uint64_t value)
{
    return (value % 3);
}

ICC translates this to the following machine code (regardless of optimization level):
mov       rcx, 0xaaaaaaaaaaaaaaab
mov       rax, rdi
mul       rcx
shr       rdx, 1
lea       rsi, QWORD PTR [rdx+rdx*2]
neg       rsi                            ; \  equivalent to:
add       rdi, rsi                       ; /    sub  rdi, rsi
mov       rax, rdi
ret         

Whereas other compilers (including MSVC, GCC, and Clang) will all generate essentially equivalent code, except that the NEG+ADD sequence is replaced by a single SUB instruction.
Like I said, this isn't just a quirk of how ICC compiles this particular snippet. It's a pattern I've observed repeatedly when analyzing the disassembly for arithmetic operations. I normally wouldn't think much of this, except that ICC is known to be a pretty good optimizing compiler and it is developed by folks that have insider information about their microprocessors.
Could there be something that Intel knows about the implementation of the SUB instruction on their processors that makes it more optimal to decompose it into NEG+ADD instructions? Using RISC-style instructions that decode into simpler 碌ops is well-known optimization advice for modern microarchitectures, so is it possible that SUB is broken down internally into individual NEG and ADD 碌ops, and that it is actually more efficient for the front-end decoder to use these ""simpler"" instructions? Modern CPUs are complicated, so anything is possible.
Agner Fog's comprehensive instruction tables confirm my intuition, though, that this is actually a pessimization. SUB is equally as efficient as ADD on all processors, so the additional required NEG instruction just serves to slow things down.
I also ran the two sequences through Intel's own Architecture Code Analyzer to analyze the throughput. Though the exact cycle counts and port bindings vary from one microarchitecture to another, a single SUB appears to be superior in every respect from Nehalem to Broadwell. Here are the two reports generated by the tool for Haswell:
SUB
Intel(R) Architecture Code Analyzer Version - 2.2 build:356c3b8 (Tue, 13 Dec 2016 16:25:20 +0200)
Binary Format - 64Bit
Architecture  - HSW
Analysis Type - Throughput

Throughput Analysis Report
--------------------------
Block Throughput: 1.85 Cycles       Throughput Bottleneck: Dependency chains (possibly between iterations)

Port Binding In Cycles Per Iteration:
---------------------------------------------------------------------------------------
|  Port  |  0   -  DV  |  1   |  2   -  D   |  3   -  D   |  4   |  5   |  6   |  7   |
---------------------------------------------------------------------------------------
| Cycles | 1.0    0.0  | 1.5  | 0.0    0.0  | 0.0    0.0  | 0.0  | 1.8  | 1.7  | 0.0  |
---------------------------------------------------------------------------------------

| Num Of |                    Ports pressure in cycles                     |    |
|  Uops  |  0  - DV  |  1  |  2  -  D  |  3  -  D  |  4  |  5  |  6  |  7  |    |
---------------------------------------------------------------------------------
|   1    | 0.1       | 0.2 |           |           |     | 0.3 | 0.4 |     | CP | mov rax, 0xaaaaaaaaaaaaaaab
|   2    |           | 1.0 |           |           |     |     | 1.0 |     | CP | mul rcx
|   1    | 0.9       |     |           |           |     |     | 0.1 |     | CP | shr rdx, 0x1
|   1    |           |     |           |           |     | 1.0 |     |     | CP | lea rax, ptr [rdx+rdx*2]
|   1    |           | 0.3 |           |           |     | 0.4 | 0.2 |     | CP | sub rcx, rax
|   1*   |           |     |           |           |     |     |     |     |    | mov rax, rcx
Total Num Of Uops: 7

NEG+ADD
Intel(R) Architecture Code Analyzer Version - 2.2 build:356c3b8 (Tue, 13 Dec 2016 16:25:20 +0200)
Binary Format - 64Bit
Architecture  - HSW
Analysis Type - Throughput

Throughput Analysis Report
--------------------------
Block Throughput: 2.15 Cycles       Throughput Bottleneck: Dependency chains (possibly between iterations)

Port Binding In Cycles Per Iteration:
---------------------------------------------------------------------------------------
|  Port  |  0   -  DV  |  1   |  2   -  D   |  3   -  D   |  4   |  5   |  6   |  7   |
---------------------------------------------------------------------------------------
| Cycles | 1.1    0.0  | 2.0  | 0.0    0.0  | 0.0    0.0  | 0.0  | 2.0  | 2.0  | 0.0  |
---------------------------------------------------------------------------------------

| Num Of |                    Ports pressure in cycles                     |    |
|  Uops  |  0  - DV  |  1  |  2  -  D  |  3  -  D  |  4  |  5  |  6  |  7  |    |
---------------------------------------------------------------------------------
|   1    | 0.1       | 0.9 |           |           |     | 0.1 | 0.1 |     |    | mov rax, 0xaaaaaaaaaaaaaaab
|   2    |           | 1.0 |           |           |     |     | 1.0 |     | CP | mul rcx
|   1    | 1.0       |     |           |           |     |     |     |     | CP | shr rdx, 0x1
|   1    |           |     |           |           |     | 1.0 |     |     | CP | lea rax, ptr [rdx+rdx*2]
|   1    |           | 0.1 |           |           |     | 0.8 | 0.1 |     | CP | neg rax
|   1    | 0.1       |     |           |           |     | 0.1 | 0.9 |     | CP | add rcx, rax
|   1*   |           |     |           |           |     |     |     |     |    | mov rax, rcx
Total Num Of Uops: 8

So, as far as I can tell, NEG+ADD increases the code size, increases the number of 碌ops, increases pressure for execution ports, and increases the number of cycles, thus resulting in a net decrease in throughput compared to SUB. So why is Intel's compiler doing this?
Is this just some quirk of the code generator that should be reported as a defect, or am I missing some merit in my analysis?","Strangely I have a simple answer: Because ICC isn't optimal.
When you write own compiler you get started with some very basic set of operation codes: NOP, MOV, ADD... up to 10 opcodes. You don't use SUB for a while because it might easily be replaced by: ADD NEGgative operand. NEG isn't basic as well, as it might be replaced by: XOR FFFF...; ADD 1.
So you implement rather complex bit-based addressing of operand types and sizes. You do it for a single machine code instruction (eg. ADD) and plan to use it further for most other instructions. But by this time your co-worker finishes implementation of optimal calculation of remainder without use of SUB! Imagine - it's already called ""Optimal_Mod"" so you miss some inoptimal thing inside not because you're a bad guy and hate AMD but just because you see - it's already called optimal, optimized.
Intel Compiler is pretty good in general, but it has a long version history, so it can behave strange in some rare cases. I suggest you inform Intel about this issue and look what will happen.",https://stackoverflow.com/questions/44330079/why-does-intels-compiler-prefer-negadd-over-sub,1,0
Verify Remote Server X509Certificate using CA Certificate File,"I've generated a CA and multiple certificates (signed by CA) using OpenSSL and I have a .NET/C# client and server both using SslStream which each have their own certificates/keys, mutual authentication is enabled and revocation is disabled.
I'm using RemoteCertificateValidationCallback for SslStream to validate the remote server's certificate and I was hoping I could just load the CA's public certificate (as a file) in the program and use it to verify the remote certificate rather then actually installing the CA in the Windows Certificate Store. The problem is the X509Chain won't show anything else unless I install the CA into the store, either will the Windows CryptoAPI shell when I open a PEM version of one of the certificates.
My question is, how can I verify a certificate has been signed by my specific CA just by using the CA's public certificate file without using Windows certificate store or WCF when RemoteCertificateValidationCallback, X509Certificate and X509Chain don't seem to give me anything to work with?","Because the CA certificate is NOT in the root certificate store, you will have within the RemoteCertificateValidationCallback() an error flag of SslPolicyErrors.RemoteCertificateChainErrors ; a possibility is to validate explicitely the certificate chain against your own X509Certificate2Collection, since you are not using the local store.

if (sslPolicyErrors == SslPolicyErrors.RemoteCertificateChainErrors)
{
    X509Chain chain0 = new X509Chain();
    chain0.ChainPolicy.RevocationMode = X509RevocationMode.NoCheck;
    // add all your extra certificate chain
    chain0.ChainPolicy.ExtraStore.Add(new X509Certificate2(PublicResource.my_ca));
    chain0.ChainPolicy.VerificationFlags = X509VerificationFlags.AllowUnknownCertificateAuthority;
    isValid = chain0.Build((X509Certificate2)certificate);
}

You can also re-use the chain passed in the callback, add your extra certificate(s) in the ExtraStore collection, and validate with the AllowUnknownCertificateAuthority flag which is needed since you add untrusted certificate(s) to the chain.
You could also prevent the original error by adding programmatically the CA certificate in the trusted root store (of course it opens a popup, for it is a major security problem to globally add a new trusted CA root) :
var store = new X509Store(StoreName.Root, StoreLocation.CurrentUser);
store.Open(OpenFlags.ReadWrite);
X509Certificate2 ca_cert = new X509Certificate2(PublicResource.my_ca);
store.Add(ca_cert);
store.Close();

EDIT: For those who want to clearly test the chain with your CA :
Another possibility is to use the library BouncyCastle to build the certificate chain and validate the trust. The options are clear and errors are easy to understand. In cas of success it will build the chain, otherwise an exception is returned. Sample below :
        // rootCerts : collection of CA
        // currentCertificate : the one you want to test
        var builderParams = new PkixBuilderParameters(rootCerts, 
                                new X509CertStoreSelector { Certificate = currentCertificate });
        // crls : The certificate revocation list
        builderParams.IsRevocationEnabled = crls.Count != 0;
        // validationDate : probably ""now""
        builderParams.Date = new DateTimeObject(validationDate);

        // The indermediate certs are items necessary to create the certificate chain
        builderParams.AddStore(X509StoreFactory.Create(""Certificate/Collection"", new X509CollectionStoreParameters(intermediateCerts)));
        builderParams.AddStore(X509StoreFactory.Create(""CRL/Collection"", new X509CollectionStoreParameters(crls)));

        try
        {
            PkixCertPathBuilderResult result = builder.Build(builderParams);
            return result.CertPath.Certificates.Cast<X509Certificate>();
            ...",https://stackoverflow.com/questions/7695438/verify-remote-server-x509certificate-using-ca-certificate-file/22703198,2,0
WCF Communication through SSL : Issue with WCF access due to Third-party Trusted Root Certification Authorities,"I have a WCF service which uses certificate authentication with Transport security. The service is deployed in a windows service, uses wshttp binding.
When I try to access the service in debug mode or after deployed as a service in local machine, I always get the error "" The HTTP request was forbidden with client authentication scheme Anonymous""
After breaking my head for a week, I stumbled upon this KB article from Microsoft
http://support.microsoft.com/kb/2801679

KB 931125 Package installed more than 330 Third-party Root Certication Authorities. Currently, the maximum size of the trusted certificate authorities list that the Schannel security package supports is 16 kilobytes (KB). Having a large amount of Third-party Root Certication Authorities will go over the 16k limit, and you will experience TLS/SSL communication problems.

I tried their solution of deleting the third party trusted authorities and found that I cant even browse to Google or any https enabled site.
But the solution worked and I was able to make the calls to my WCF service.
Right now I have restored the third party trusted authorities from the KB link http://support.microsoft.com/kb/931125 since I cannot access many sites, and vpn, email does not work.
After the restore, again I am unable to access my WCF service.
What is the best way to resolve this?
I got the solution now, need to do the following
Source http://windowssecrets.com/forums/showthread.php/156768-KB-931125-being-installed-with-no-easy-removal","What is the best way to resolve this?

In .Net, we can use one CA rather than the 330+ form the entire store. Programming WCF Services by Juval Lowy discusses using the server's certificate around page 570. Lowy talks about validation and IEndpointBehavior and X509ServiceCertificateAuthentication.
However, I can't tell if WCF has the RemoteCertificateValidationCallback like in SslStream and ServicePointManager? If its available, use the one CA you need and load it from the filesystem:
static bool VerifyServerCertificate(object sender, X509Certificate certificate,
    X509Chain chain, SslPolicyErrors sslPolicyErrors)
{
    try
    {
        String CA_FILE = ""ca-cert.der"";
        X509Certificate2 ca = new X509Certificate2(CA_FILE);

        X509Chain chain2 = new X509Chain();
        chain2.ChainPolicy.ExtraStore.Add(ca);

        // Check all properties
        chain2.ChainPolicy.VerificationFlags = X509VerificationFlags.NoFlag;

        // This setup does not have revocation information
        chain2.ChainPolicy.RevocationMode = X509RevocationMode.NoCheck;

        // Build the chain
        chain2.Build(new X509Certificate2(certificate));

        // Are there any failures from building the chain?
        if (chain2.ChainStatus.Length == 0)
            return true;

        // If there is a status, verify the status is NoError
        bool result = chain2.ChainStatus[0].Status == X509ChainStatusFlags.NoError;
        Debug.Assert(result == true);

        return result;
    }
    catch (Exception ex)
    {
        Console.WriteLine(ex);
    }

    return false;
}

I have not figured out how to use this chain (chain2 above) by default such that there's no need for the callback. That is, install it on the ssl socket and the connection will ""just work"". And I have not figured out how install it such that its passed into the callback. That is, I have to build the chain for each invocation of the callback. I think these are architectural defects in .Net, but I might be missing something obvious.
A similar answer shows up in a couple of questions like Verify Remote Server X509Certificate using CA Certificate File.",https://stackoverflow.com/questions/22773480/wcf-communication-through-ssl-issue-with-wcf-access-due-to-third-party-trusted/22777771,1,0
"Export links, defects and test results from Excel to QC","I have a question about HPQC. I have the Excel add-in but it doesn't seem like I can export the links between defects and Test cases. Is there a way to export the links between defects and Test cases from Excel? 
Also, I would like to know if I can export the pass/fail status of test cases using the add in? If not, I'm not a coder but I could a colleague. Is it possible to bypass the add-in and load data from code instead?","Excel Add-in will not support uploading test results into QC from Excel. Excel add-in can be used to upload tests, requirement and defects.
You have to write your own code using Open Test Architecture (OTA) API's to upload test results to QC.  You can bypass the add-in if you have your own code to upload test results. OTA API documentation is available in the QC documentation library.",https://stackoverflow.com/questions/29804013/export-links-defects-and-test-results-from-excel-to-qc/32367982,1,0
Development Philosophies for Dev Teams [closed],"Closed. This question is opinion-based. It is not currently accepting answers.
                            
                        










Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.
                        
Closed 3 months ago.



I've been developing for a few years now and have been frustrated with how some of the older programmers in my team have proceeded to design and implement without any kind of development philosophy, which generally leads to problems further down the development track, generally due to a lack of flexibility and validation.  I raised this irk with my colleagues and they replied by saying ""Well then, which of the hundreds of development philosophies do you suggest we use?""
This was a hard question to answer as using development philosophies when working by yourself is a lot easier than when working in a team. I am a bit partial to test driven development as it seems to provide me with the greatest feeling of security that my code is working correctly, however TDD does have its limitations, mainly in the overhead required to write all the tests and also in making sure that there are no gaps in our testing framework (which is hard to achieve when working in a team). A friend of mine swears by the Agile Unified Process (AUP) and refuses to use anything else.
My questions are

Which development philosophies have you found to work well when developing in a team, and how have they helped you to deliver your product on time and under budget?
What issues did you come across and how did you overcome them?
Do you think they are even required at all?","As a contractor, you may have little influence to affect a change in development philosphy at a particular company. That's part of the price for being what most companies would consider a temp. For you as a person, sometimes the best you can do is learn to adapt to the current corporate culture and learn what works and what doesn't for when you are in a position to influence.
You can always do the test driven parts yourself so at least you know your code will pass the tests. I wouldn't look at that as overhead either, it is part of writing code to test it make sure it works. There is far more overhead associated with not writing tests.
If a place has no particular development methodology, then there are two ways to get one. First is if the developers mostly agree on what philosphy to use and they just start using it and show management it works and improves the product. Developers are remarkably free to figure out how to do their work, more so than many other professional specialties. Use this to your advantage. Get a small group of like minded folks together and just start using a method you all agree on. Indoctrinate anyone new to the group in how you do things. Show real progress and management wil get on board with it (managers like anything that makes them look good).
The second way to change the corporate culture is from the top down. Change one senior manager's mind on doing this and he can mandate that the new methodology be used. It ain't pretty and people will fight doing it (this is called resistance to change and it is a normal condition and you need to expect to deal with it.) Again build a few successes and it it gets easier, but your manager must have the  gumption to stick with the policy through the hard phase of getting people to use the new method and there must be consequences for those who do not follow the new method. Sometimes having a test project with volunteers to use the methodology first can prove the value to the others, sometimes, people are just deadwood and won't change no matter what.
People who, after some time and mulitple chances to change, still refuse to get on board with the new policy will need to be let go no matter how good they are as individual developers. If you can't follow the team rules, you need to move on or be moved on.  A culture change can't be accomplished without everyone eventually coming on board. 
Sometimes  you can use outside influences to get managers to change the way of doing business. We changed our whole development process here in order to get a certification required by one of our largest customers. HIPPA and Sarbanes-Oxley laws are also responsible for many companies formalizing a development process. If you can make a case to mamangement that formalizing the process to get some sort of certification or comply with the law is a benefit that will get them more business, then perhaps they will suddenly see the value.",https://stackoverflow.com/questions/1392546/development-philosophies-for-dev-teams/1392815,5,0
"What's the difference between unit, functional, acceptance, and integration tests? [closed]","Closed. This question needs to be more focused. It is not currently accepting answers.
                            
                        










Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                        
Closed 4 years ago.



What is the difference between unit, functional, acceptance, and integration testing (and any other types of tests that I failed to mention)?","Depending on where you look, you'll get slightly different answers.  I've read about the subject a lot, and here's my distillation; again, these are slightly wooly and others may disagree.
Unit Tests
Tests the smallest unit of functionality, typically a method/function (e.g. given a class with a particular state, calling x method on the class should cause y to happen).  Unit tests should be focussed on one particular feature (e.g., calling the pop method when the stack is empty should throw an InvalidOperationException).  Everything it touches should be done in memory; this means that the test code and the code under test shouldn't:

Call out into (non-trivial) collaborators
Access the network  
Hit a database
Use the file system
Spin up a thread
etc.

Any kind of dependency that is slow / hard to understand / initialise / manipulate should be stubbed/mocked/whatevered using the appropriate techniques so you can focus on what the unit of code is doing, not what its dependencies do. 
In short, unit tests are as simple as possible, easy to debug, reliable (due to reduced external factors), fast to execute and help to prove that the smallest building blocks of your program function as intended before they're put together.  The caveat is that, although you can prove they work perfectly in isolation, the units of code may blow up when combined which brings us to ...
Integration Tests
Integration tests build on unit tests by combining the units of code and testing that the resulting combination functions correctly.  This can be either the innards of one system, or combining multiple systems together to do something useful.  Also, another thing that differentiates integration tests from unit tests is the environment.  Integration tests can and will use threads, access the database or do whatever is required to ensure that all of the code and the different environment changes will work correctly.  
If you've built some serialization code and unit tested its innards without touching the disk, how do you know that it'll work when you are loading and saving to disk?  Maybe you forgot to flush and dispose filestreams.  Maybe your file permissions are incorrect and you've tested the innards using in memory streams.  The only way to find out for sure is to test it 'for real' using an environment that is closest to production.
The main advantage is that they will find bugs that unit tests can't such as wiring bugs (e.g. an instance of class A unexpectedly receives a null instance of B) and environment bugs (it runs fine on my single-CPU machine, but my colleague's 4 core machine can't pass the tests).  The main disadvantage is that integration tests touch more code, are less reliable, failures are harder to diagnose and the tests are harder to maintain.
Also, integration tests don't necessarily prove that a complete feature works.  The user may not care about the internal details of my programs, but I do!
Functional Tests
Functional tests check a particular feature for correctness by comparing the results for a given input against the specification.  Functional tests don't concern themselves with intermediate results or side-effects, just the result (they don't care that after doing x, object y has state z). They are written to test part of the specification such as, ""calling function Square(x) with the argument of 2 returns 4"". 
Acceptance Tests
Acceptance testing seems to be split into two types:
Standard acceptance testing involves performing tests on the full system (e.g. using your web page via a web browser) to see whether the application's functionality satisfies the specification. E.g. ""clicking a zoom icon should enlarge the document view by 25%."" There is no real continuum of results, just a pass or fail outcome. 
The advantage is that the tests are described in plain English and ensures the software, as a whole, is feature complete.  The disadvantage is that you've moved another level up the testing pyramid. Acceptance tests touch mountains of code, so tracking down a failure can be tricky.  
Also, in agile software development, user acceptance testing involves creating tests to mirror the user stories created by/for the software's customer during development.  If the tests pass, it means the software should meet the customer's requirements and the stories can be considered complete. An acceptance test suite is basically an executable specification written in a domain specific language that describes the tests in the language used by the users of the system.
Conclusion
They're all complementary.  Sometimes it's advantageous to focus on one type or to eschew them entirely.  The main difference for me is that some of the tests look at things from a programmer's perspective, whereas others use a customer/end user focus.",https://stackoverflow.com/questions/4904096/whats-the-difference-between-unit-functional-acceptance-and-integration-test/28651929,8,0
XCode error after linking SQLite3,"I get the following error after adding libsqlite3.dylib to my xcode frameworks folder.
ignoring file /Users/AlexStein/Desktop/iPhone Applications/Persistence copy/libsqlite3.dylib, missing required architecture i386 in file

The way I added sqlite3 was as follows: Right clicked on frameworks folder, selected ""Add files to..."" and searched for libsqlite3.dylib. There were two of those and two of libsqlite3.0.dylib. I knew the ""3.0"" wasn't right for the simulator, so I chose one of the ""3""s. When I try to chose the other one it simply says that I have already added it, so I assume they are the same.
I'm not sure why I'm getting the error as I'm pretty sure all my code is right. This is something to do with xCode. I'd appreciate any help!","have you checked project setting. architecture column .
EDITED
Apple__ Xcode 4 is different from its earlier version. We have to learn a new user interface and must workaround some birth defects. Try out these steps:

Navigate to Build Settings tab and check out the Architectures group
Look at the values, for example Architectures: Standard (armv6 armv7), Base SDK: Latest iOS (iOS 4.3), Build Active Architecture Only: No, Supported Platforms: iphonesimulator, iphoneos and Valid Architectures: armv6 armv7 i386.
If __alid Architectures_ is set only to armv6 and armv7, delete both. Restart Xcode. I do not know why but some of you reported that it helped. Set __alid Architectures_ to $(ARCHS_STANDARD_32_BIT). Valid Architectures will show armv6 and armv7 again but this time it may compile just fine. Strange though.
Besides standard armv6 and armv7, manually add i386 to __alid Architectures_. Note that i386 is a valid architecture since the Simulator is running on your Intel iMac (or MacBook Pro).

Notes:

Change your code to reduce the number of warnings like this:
1
2
3
4

replace:
if (self = [super init]) {
with:
if ((self = [super init])) {

armv6 notices: Apple initiated a major hardware cutoff with iOS 4.3. iOS 4.3 removed support for any of the armv6 based gadgets. Old iPod touch/iPhone (1G, 2G, 3G) devices are all running on arm v6 CPU. Armv6 compilers does not work with armv7 or above, but with Xcode 4 you can build apps for old devices running iOS 4.2 or earlier. In your code look for iOS and hardware versions to use the proper framework or other objects otherwise you get exceptions at runtime. It seems Apple prepares the road for armv8 devices and iOS 5.",https://stackoverflow.com/questions/6668171/xcode-error-after-linking-sqlite3/6668555,1,0
How to Establish a connection to ALM using REST API,"I have a Couple of Questions which need to be clarified below 
1)What is Meant by a REST client 
2)What is the difference between OTA API vs REST API in ALM during connectivity. 
I have already established a connection with OTA. Can anyone provide a sample code to establish a connection with ALM using REST API","1) REST client - it depends what you mean: or HPE ALM Web Client (that works using REST architecture) or ALM REST Api
HPE ALM Web Client was announced in ALM 12.01 and was decommissioned in ALM 12.50
It allowed to work with requirements and defects modules
ALM REST Api - RESTful API that allow create, read, update, and delete data on the ALM 
2) Using OTA you need to initialize connection to server using:    
set tdc = CreateObject(""TDApiOle80.TDConnection"")
tdc.InitConnectionEx ServerName

When using REST API yo need to:
POST on {ServerNasme}/qcbin/api/authentication/sign-in with header:
  Authorization: Basic {Login:Password encoded with Base64}
thats return you next cookies: 
   ALM_USER
   LWSSO_COOKIE_KEY
   QCSession
   XSRF-TOKEN",https://stackoverflow.com/questions/38473547/how-to-establish-a-connection-to-alm-using-rest-api/38851197,2,0
Are the result of bitwise operations on signed integral types well-defined?,"Consider this code:
using integer = int; // or any other fundamental integral type
using unsigned_integer = typename std::make_unsigned<integer>::type;
constexpr integer bits = std::numeric_limits<unsigned_integer>::digits;
integer value = -42; // or any value
integer mask = static_cast<integer>(1)<<static_cast<integer>(bits-1);
bool result_and = value & mask;
bool result_or = value | mask;
bool result_xor = value ^ mask;

I am wondering how well are these operations defined according to the standard. Do I have the guarantee to end up with the same results on all architectures? I am sure to operate on the sign bit on all architectures where this sign bit is 0 for positive numbers and 1 for negative numbers?","The results of bitwise and, bitwise or and bitwise xor are currently underspecified in the standard, in particular the term bitwise is never defined. We have defect report 1857: Additional questions about bits  that covers this issue and says:

The specification of the bitwise operations in 5.11 [expr.bit.and],
  5.12 [expr.xor], and 5.13 [expr.or] uses the undefined term __itwise_ in describing the operations, without specifying whether it is the
  value or object representation that is in view.
Part of the resolution of this might be to define __it_ (which is otherwise currently undefined in C++) as a value of a given power of 2.

and the resolution was:

CWG decided to reformulate the description of the operations
  themselves to avoid references to bits, splitting off the larger
  questions of defining __it_ and the like to issue 1943 for further
  consideration.

Which resulted in a consolidated defect report 1943: Unspecified meaning of __it_.
The result of left shifting a signed type is going to depend on the underlying representation. We can see this from the defect report 1457: Undefined behavior in left-shift which made it well defined to left shift into the sign bit and says:

The current wording of 5.8 [expr.shift] paragraph 2 makes it undefined
  behavior to create the most-negative integer of a given type by
  left-shifting a (signed) 1 into the sign bit, even though this is not
  uncommonly done and works correctly on the majority of
  (twos-complement) architectures:

...if E1 has a signed type and non-negative value, and E1 猕 2E2 is representable in the result type, then that is the resulting value;
    otherwise, the behavior is undefined.

As a result, this technique cannot be used in a constant expression,
  which will break a significant amount of code.

Noting the emphasis on the statement works correctly on the majority of
(twos-complement) architectures. So it is dependent on the underlying representation for example twos-complement.",https://stackoverflow.com/questions/33251441/are-the-result-of-bitwise-operations-on-signed-integral-types-well-defined/33255763,3,0
Attaching Files to QC defect by Java code from local machine,"When we open a defect in QC generally we need to attach the logs of the server
i want to Make an application such that It will take files(logs from all the relevant server) and stores in our local machine and then we can rar those files and directly attach it to QC defect mentioned by person opening defect
Now my question is how can i access the defect and attach the files to that particular defect
Please give your suggestions if any","QC has a published COM API called Open Test Architecture, (OTA) you can use that to load the relevant defect and attach the file to it. You can find relevant examples in QC Help for OTA.
Note that this is a COM API, if you insist using Java client to communicate with it, then you will have to use a Java to COM bridge like JIntegra (there is a free one as well, but I don't remember it's name).
But if you can write your client in VB6, C++ or .NET it will be much simpler (and cheaper) than writing it in Java.
For more Java to COM bridges, check out this question",https://stackoverflow.com/questions/2632565/attaching-files-to-qc-defect-by-java-code-from-local-machine/2663646,1,0
Is it possible to call a COM API from Java? [closed],"Closed. This question needs to be more focused. It is not currently accepting answers.
                            
                        










Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                        
Closed last month.



Is it possible to call a COM API from Java (specifically the HP/Mercury Quality Center OTA API)? If so, what's the best way?
Is something like JACOB appropriate?
Code fragments would be helpful for the basics :-)","jacob : yes, http://sourceforge.net/projects/jacob-project/ is an active project that will suite your purpose pretty well.
you can see multiple examples : http://jacob-project.wiki.sourceforge.net/Event+Callbacks
but if you want something that is more tested and are willing to pay money, then go for http://www.nevaobject.com/_docs/_java2com/java2com.htm.",https://stackoverflow.com/questions/138028/is-it-possible-to-call-a-com-api-from-java,5,0
Variation in performance when using GetDirectories,"I have a production performance issue that I'm stumped on.  I'm hoping that someone has seen something similar or at least has a few troubleshooting techniques.
I support an inherited asp.net web application that retrieves files from a shared network drive.  One particular folder [we'll call it FOLDERA] I've chosen to troubleshoot against contains about 300mb of files and multiple subfolders.  FOLDERA is considered large for this application.  Starting recently, the users have been having slow performance when retrieving files from FOLDERA on a production network share.  I narrowed the code down to the GetDirectories method within the asp.net application.
RootDir.GetDirectories(""*"", SearchOption.AllDirectories);
In production, the FOLDERA read takes about 8-10 seconds.  Prior to the recent performance degradation it was about 1 second.  In the test environment it takes 1-2 seconds with the same amount of data.
My theory is a network issue because the same weekend that the users noticed problems was the same weekend network and hardware upgrades occurred.  However, I don't know how to determine or prove this to the network engineers.
I would appreciate ideas on what might be going on.","I have used the same exact method on a large shared directory to extract and index several thousands of pdf files (~80000, size didn't matter) with no performance issues (1-2 seconds)
You could try to benchmark IO from file share between development and production environments with a external program to prove your theory, eliminating any software defect and passing issue to the infrastructure staff.
On the other side you could easily cache this information with CacheDependency on server side, depending on application architecture. This would drastically optimize the performance if this is a core functionality.",https://stackoverflow.com/questions/3620359/variation-in-performance-when-using-getdirectories/3719856,1,0
"For reliable code, NModel, Spec Explorer, F# or other?","I've got a business app in C#, with unit tests.   Can I increase the reliability and cut down on my testing time and expense by using NModel or Spec Explorer?  Alternately, if I were to rewrite it in F# (or even Haskell), what kinds (if any) of reliability increase might I see?
Code Contracts?  ASML?
I realize this is subjective, and possibly argumentative, so please back up your answers with data, if possible. :)  Or maybe an worked example, such as Eric Evans Cargo Shipping System?
If we consider 

Unit tests to be specific and strong theorems, checked
  quasi-statically on particular __nteresting instances_ and  Types to be general but weak theorems (usually checked statically), and contracts to be general and strong theorems, checked dynamically for particular instances that occur during regular program operation.
  (from B. Pierce's Types Considered Harmful),  

where do these other tools fit?
We could pose the analogous question for Java, using Java PathFinder, Scala, etc.","Reliability is a function of several variables, including the general architecture of the software, the capability of the programmers, the quality of the requirements and the maturity of your configuration management and general QA processes.  All these will affect the reliability of a rewrite.
Having said that, language certainly has a significant impact.  All other things being equal:

Defects are roughly proportional to SLOC count.  Languages that are terser see fewer coding errors.  Haskell seems to require about 10% of the SLOC required by C++, Erlang about 14%, Java around 50%.  I guess C# probably fits alongside Java on this scale.
Type systems are not borne equal.  Languages with type inference (e.g. Haskell and to a lesser extent O'Caml) will have fewer defects.  Haskell in particular will allow you to encode invariants in the type system so that a program will only compile if they can be proven true.  Doing so requires extra work, so consider the trade-off on a case-by-case basis.
Managing state is a source of many defects.  Functional languages, and especially pure functional languages, avoid this problem.
QuickCheck and its relatives allow you to write unit and system tests that verify general properties rather than individual test cases.  This can greatly reduce the work required to test the code, especially if you are aiming for high test coverage metrics.  A set of QuickCheck properties resembles a formal specification, and this concept fits nicely with Test Driven Development (write your tests first, and when the code passes them you are done).

Put all of these things together and you should have a powerful toolkit for driving quality through the development lifecycle.  Unfortunately I'm not aware of any robust studies that actually prove this.  All the factors I listed at the start would confound any real study, and you would need a lot of data before an unambiguous pattern showed itself.",https://stackoverflow.com/questions/2767295/for-reliable-code-nmodel-spec-explorer-f-or-other/2768725,2,0
Managing the maintenance burden of unit tests [closed],"Closed. This question needs to be more focused. It is not currently accepting answers.
                            
                        










Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                        
Closed 2 years ago.



Coding test-first, I find that perhaps 3/4 of my code is unit tests; if I were truly extreme, and didn't write a line of code except to fix a failing unit test, this ratio would be even higher.  Maintaining all these unit tests adds a huge amount of inertia to code changes.  Early on, I suck it up and fix them.  As soon as there's pressure, I end up with a broken_unit_tests directory to revisit 'when there's time'.  It feels like TDD is putting in high coverage too soon, before the design has had time to crystallize. 
How do I find my way out of this dilemma, and start welcoming changing requirements like I'm supposed to?","Keeping the aspect of Programmer Discipline aside... (it's a personal thing if you're okay with checking-in without doing a buddy build or without fixing all the tests. Agile assumes high discipline.. and Courage&Support to stay on the right path under Pressure :),
If you find that making a single change fails multiple tests, its a smell that something is wrong with your tests. Fragile Tests are common when you start out with TDD... if you spend more time fixing your tests than fixing your code... stop, breathe and reflect. Fix the disease rather than the symptom.
If you have some code snippets, we could discuss. As it stands I dont think I can help you out much...
Guideline: A test should fail for only one reason.. Conversely every failing test should point out the exact unique location of the defect. Two tests should not fail due to the same change. 
Unless you're making architecture level sweeping changes, this should be rare.",https://stackoverflow.com/questions/208302/managing-the-maintenance-burden-of-unit-tests/208344,5,0
What third-party ASP.NET controls or libraries can you not live without? [closed],"As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,   or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question   can be improved and possibly reopened, visit the help center for guidance.
                            
                        


Closed 6 years ago.


I am working on a project right now, and I have a few must-have controls that I include in most sites I build. Often times it strikes me that the best controls are not always that well publicized so I thought I would ask the Stack Overflow community about their favorite third-party ASP.NET controls - the ones you just have to have on a project because they are so valuable.
EDIT:
Some people have asked for clarification and the controls that I include.
First, I am not talking about just user interface controls but libraries as well, a few people have mentioned log4net which falls into this category.
For me, there is one third-party control package which I find to be masterfully done and an incredible value for the money - it is Peter Blum's Data Entry Suite. As Scott Hanselman says, it is a ""complete re-imagining of the ASP.NET Validation framework"" and it is loaded with controls for rich interaction with data entry forms.
Outside of that, I am really digging jQuery lately but like most of the answers so far also use the ASP.NET Ajax Control Toolkit in many places.","Generally, I prefer to avoid dependencies whenever possible.  However, there are a couple of 3rd party libraries which usually end up being used on nearly all projects:

ASP.NET AJAX Toolkit
Log4net

On many projects, I'm also using some internal libraries (compiled separately and referenced by the project).  These are for data access, configuration management, and utility functionality.
The Telerik controls are really good as well for use on ASP.NET projects, though again, only added into the project if they are actually going to be used.",https://stackoverflow.com/questions/405500/what-third-party-asp-net-controls-or-libraries-can-you-not-live-without/405853,13,0
"Does the Roles Manager cache data for a custom roles provider, and can I clear this cache?","I'm using a custom role provider, which to over simplify gets a person object from the database using EF on .net 4 MVC project, and allocates user roles based on some rules around that (and other queries).
The data changes regularly, though changes are mode through code elsewhere in the system, not the roles provider.  The roles provider is one way, and simply gets the roles a user is in.
When I change the database values, the role manager does not pick up on the change of roles until I do a recompile (by adding a space in web config for example), or the application otherwise restarts.
I've ensured the roles do not cache in a cookie by setting cacheRolesInCookie=false, which is what most help seems to point to, and presume there is a session cache built into the role manager.  
I've modified the EF query which returns the person object to include a time stamp as part of the query.  I can see through profiler the query is actually being called, and the time stamp changes each time, but my debug session shows stale data from the previous state for the 'person' item.  There are other parts of the site that display data from the Person table, which show the up to date state.  
I don't really understand how the debugger should behave on cached data.  I don't see why the EF query would fire at all if it's a cache issue, but the person data is definitely showing state as per the first run, not as per the current state of the table row.
I feel I'm missing something obvious.  Does the Role Manager cache data in session?","The answer really depends on the architecture of your application. I had this problem recently and was blaming the Role Manager cache as well. Turns out it was the management of the Entity context in my Data Access Layer altogether. I was managing my Entity Context and storing the context per-request as is typically recommended. However the problem was that the context was being set twice due to an unrelated defect, and so the Role Provider's context was always different than the rest of the application, and was only set once (since the Role Provider is instantiated on Application Start, not per-request).
I'd recommend looking how you are storing data contexts and trace through to see how that is being stored in relation to your Role Manager vs the rest of the application. Ensure you are truly only using one context per request.",https://stackoverflow.com/questions/7763185/does-the-roles-manager-cache-data-for-a-custom-roles-provider-and-can-i-clear-t/11779074,2,0
Is reading an indeterminate value undefined behavior?,"The question arose in the comments of an answer to the question Is C/C++ bool type always guaranteed to be 0 or 1 when typecast'ed to int?
The code in question allocates a (local) array of bool without initializing their value. 
const int n = 100;
bool b[n];

Clearly the values in b are indeterminate. 
Some of the commenters opined that reading e.g. b[0] was undefined behavior. Is this stated anywhere in the C++ standard? I am still convinced of the opposite:

There is clearly storage allocated and initialization of the fundamental bool type is complete, since it doesn't have a constructor. It is thus certainly not the same as dereferencing an uninitialized pointer, or calling methods/cast operators on uninitialized non-trivial objects. These specific cases seem to be covered by the standard. 
The behavior is indeed undefined in C: What happens to a declared, uninitialized variable in C? Does it have a value? and some respondents seem to confuse the two. 
In the latest C++0x draft I can find no definition of indeterminate value especially no definition that would allow accessing such a value to trigger a processor trap. Indeed, Bjarne Stroustrup is not sure what an inderminate value may be: http://zamanbakshifirst.blogspot.com/2007/02/c-indeterminate-value.html","yes, formally an rvalue conversion of indeterminate value is UB (except for unsigned char, originally i wrote ""and variants"" but as i recall the formal caters to 1's complement signed char where possibly minus 0 could be used as trap value)
i'm too lazy to do the standard paragraph lookup for you, and also to lazy to care about downvotes for that
however, in practice only a problem on (1) archaic architectures, and perhaps (2) 64-bit systems.
EDIT: oops, i now seem to recall a blog posting and associated Defect Report about formal UB for accessing indeterminate char. so perhaps i'll have to actually check the standard, + search DRs. argh, it will have to be later then, now coffee!
EDIT2: Johannes Schaub was kind enough to provide this link to SO question where that UB for accessing char was discussed. So, that's where I remembered it from! Thanks, Johannes.
cheers & hth.,",https://stackoverflow.com/questions/4279264/is-reading-an-indeterminate-value-undefined-behavior/4279313,4,0
Is C/C++ bool type always guaranteed to be 0 or 1 when typecast'ed to int?,"Many compilers seem to be keeping only 0 or 1 in bool values, but I'm not sure this will always work:
int a = 2;
bool b = a;
int c = 3 + b; // 4 or 5?","Yes:
In C++ (搂4.5/4):

An rvalue of type bool can be
  converted to an rvalue of type int,
  with false becoming zero and true
  becoming one.

In C, when a value is converted to _Bool, it becomes 0 or 1 (搂6.3.1.2/1):

When any scalar value is converted to
  _Bool, the result is 0 if the value compares equal to 0; otherwise, the
  result is 1.

When converting to int, it's pretty straight-forward.  int can hold 0 and 1, so there's no change in value (搂6.3.1.3).",https://stackoverflow.com/questions/4276207/is-c-c-bool-type-always-guaranteed-to-be-0-or-1-when-typecasted-to-int/4276293,6,0
"What happens to a declared, uninitialized variable in C? Does it have a value?","If in C I write:
int num;

Before I assign anything to num, is the value of num indeterminate?","Static variables (file scope and function static) are initialized to zero:
int x; // zero
int y = 0; // also zero

void foo() {
    static int x; // also zero
}

Non-static variables (local variables) are indeterminate. Reading them prior to assigning a value results in undefined behavior.
void foo() {
    int x;
    printf(""%d"", x); // the compiler is free to crash here
}

In practice, they tend to just have some nonsensical value in there initially - some compilers may even put in specific, fixed values to make it obvious when looking in a debugger - but strictly speaking, the compiler is free to do anything from crashing to summoning demons through your nasal passages.
As for why it's undefined behavior instead of simply ""undefined/arbitrary value"", there are a number of CPU architectures that have additional flag bits in their representation for various types. A modern example would be the Itanium, which has a ""Not a Thing"" bit in its registers; of course, the C standard drafters were considering some older architectures.
Attempting to work with a value with these flag bits set can result in a CPU exception in an operation that really shouldn't fail (eg, integer addition, or assigning to another variable). And if you go and leave a variable uninitialized, the compiler might pick up some random garbage with these flag bits set - meaning touching that uninitialized variable may be deadly.",https://stackoverflow.com/questions/1597405/what-happens-to-a-declared-uninitialized-variable-in-c-does-it-have-a-value,10,0
Has C++ standard changed with respect to the use of indeterminate values and undefined behavior in C++14?,"As covered in Does initialization entail lvalue-to-rvalue conversion? Is int x = x; UB? the C++ standard has a surprising example in section 3.3.2 Point of declaration in which an int is initialized with it's own indeterminate value:

int x = 12;
{ int x = x; }

Here the second x is initialized with its own (indeterminate) value.
  _ end example ]

Which Johannes answer to this question indicates is undefined behavior since it requires an lvalue-to-rvalue conversion.
In the latest C++14 draft standard N3936 which can be found here this example has changed to:

unsigned char x = 12;
{ unsigned char x = x; }

Here the second x is initialized with its own (indeterminate) value.
  _ end example ]

Has something changed in C++14 with respect to indeterminate values and undefined behavior that has driven this change in the example?","Yes, this change was driven by changes in the language which makes it undefined behavior if an indeterminate value is produced by an evaluation but with some exceptions for unsigned narrow characters.
Defect report 1787 whose proposed text can be found in N39141 was recently accepted in 2014 and is incorporated in the latest working draft N3936:
The most interesting change with respect to indeterminate values would be to section 8.5 paragraph 12 which goes from:

If no initializer is specified for an object, the object is default-initialized; if no initialization is performed, an object with automatic or dynamic storage duration has indeterminate value. [ Note: Objects with static or thread storage duration are zero-initialized, see 3.6.2. _ end note ]

to (emphasis mine):

If no initializer is specified for an object, the object is
  default-initialized. When storage for an object with automatic or
  dynamic storage duration is obtained, the object has an indeterminate
  value, and if no initialization is performed for the object, that
  object retains an indeterminate value until that value is replaced
  (5.17 [expr.ass]). [Note: Objects with static or thread storage
  duration are zero-initialized, see 3.6.2 [basic.start.init]. __nd
  note] If an indeterminate value is produced by an evaluation, the
  behavior is undefined except in the following cases:

If an indeterminate value of unsigned narrow character type (3.9.1 [basic.fundamental]) is produced by the evaluation of:

the second or third operand of a conditional expression (5.16 [expr.cond]),
the right operand of a comma (5.18 [expr.comma]),
the operand of a cast or conversion to an unsigned narrow character type (4.7 [conv.integral], 5.2.3 [expr.type.conv], 5.2.9
  [expr.static.cast], 5.4 [expr.cast]), or
a discarded-value expression (Clause 5 [expr]),

then the result of the operation is an indeterminate value.
If an indeterminate value of unsigned narrow character type (3.9.1 [basic.fundamental]) is produced by the evaluation of the right
  operand of a simple assignment operator (5.17 [expr.ass]) whose first
  operand is an lvalue of unsigned narrow character type, an
  indeterminate value replaces the value of the object referred to by
  the left operand.
If an indeterminate value of unsigned narrow character type (3.9.1 [basic.fundamental]) is produced by the evaluation of the
  initialization expression when initializing an object of unsigned
  narrow character type, that object is initialized to an indeterminate
  value.


and included the following example:

[ Example:
int f(bool b) {
  unsigned char c;
  unsigned char d = c; // OK, d has an indeterminate value
  int e = d;           // undefined behavior
  return b ? d : 0;    // undefined behavior if b is true
}

_ end example ]

We can find this text in N3936 which is the current working draft and N3937 is the C++14 DIS.
Prior to C++1y
It is interesting to note that prior to this draft unlike C which has always had a well specified notion of what uses of indeterminate values were undefined C++ used the term indeterminate value without even defining it (assuming we can not borrow definition from C99) and also see defect report 616. We had to rely on the underspecified lvalue-to-rvalue conversion which in draft C++11 standard is covered in section 4.1 Lvalue-to-rvalue conversion paragraph 1 which says:

[...]if the object is uninitialized, a program that necessitates this conversion has undefined behavior.[...]


Footnotes:

1787 is a revision of defect report 616, we can find that information in N3903",https://stackoverflow.com/questions/23415661/has-c1y-changed-with-respect-to-the-use-of-indeterminate-values-and-undefined,1,0
What is the value category of the operands of C++ operators when unspecified?,"PREMISE:
The C++11 Standard classifies expressions into three disjoint value categories: lvalues, xvalues, and prvalues (搂 3.10/1). An explanation of what value categories are is available for instance here.
I am struggling to figure out what are the requirements of the different operators on the value category of their operands. Paragraph 3.10/1 specifies:

[...] Every expression belongs to exactly one of the fundamental classifications in this taxonomy: lvalue, xvalue, or prvalue. This property of an expression is called its value category. [ Note: The discussion of each built-in operator in Clause 5 indicates the category of the value it yields and the value categories of the operands it expects. For example, the built-in assignment operators expect that the left operand is an lvalue and that the right operand is a prvalue and yield an lvalue as the result. User-defined operators are functions, and the categories of values they expect and yield are determined by their parameter and return types. __nd note ]

In spite of what the note above claims, Clause 5 is not always very clear about the value category of operators' operands. This is, for instance, all that is said about the value category of the operands of the assignment operator (Paragraph 5.17/1):

The assignment operator (=) and the compound assignment operators all group right-to-left. All require a modifiable lvalue as their left operand and return an lvalue referring to the left operand. The result in all cases is a bit-field if the left operand is a bit-field. In all cases, the assignment is sequenced after the value computation of the right and left operands, and before the value computation of the assignment expression. With respect to an indeterminately-sequenced function call, the operation of a compound assignment is a single evaluation. [ Note: Therefore, a function call shall not intervene between the lvalue-to-rvalue conversion and the side effect associated with any single compound assignment operator. __nd note ]

How about the right operands?
The words ""rvalue"" and ""lvalue"" no more occur in the whole Section 5.17. While the note in Paragraph 3.10/1 makes it explicit that the built-in assignment operators expect a prvalue as a right operand, this is not explicitly mentioned in Section 5.17.  Even the  final note of 5.17/1, which mentions lvalue-to-rvalue conversions, seems to imply that rvalues are expected somehow (what's the need for a conversion otherwise?), but notes are non-normative after all.
Sections concerning other operators, including multiplicative and additive operators, are generally silent on the value category of their operands. I couldn't find any ""default statement"" in the Standard stating that, when not specified otherwise, the operands of built-in operators are rvalues. Hence, the question.
QUESTION:

What is the value category of the right operand of the assignment operator; and, more generally
How to figure out the value category of an operator's operand when this is is not specified? Is it unconstrained (meaning that any value category is accepted)? If so, why should lvalue-to-rvalue conversions ever apply in an assignment expression?

References to the C++11 Standard are highly appreciated.","Yes, it's ill-specified and has been covered before. Basically, every time an lvalue expression is required is enumerated, so we assume that every other operand must be a prvalue expression.
So to answer your questions:

A prvalue.
If it's not specified, it's a prvalue.

The note that is quoted in the linked answer seems to have changed a few times. The quote from 搂3.10 of the C++11 standard is as follows (and at the current time is identical in the latest draft):

[ Note: The discussion of each built-in operator in Clause 5 indicates the category of the value it yields and the value categories of the operands it expects. For example, the built-in assignment operators expect that the left operand is an lvalue and that the right operand is a prvalue and yield an lvalue as the result. User-defined operators are functions, and the categories of values they expect and yield are determined by their parameter and return types. _ end note ]

Here it even says explicitly that the assignment operators expect the right operand to be a prvalue. Of course, this is a note and is therefore non-normative.",https://stackoverflow.com/questions/14991219/what-is-the-value-category-of-the-operands-of-c-operators-when-unspecified,1,0
Light weight Java API for LDAP,"I would like to validate username and password for my new open source Java project using LDAP(Iplanet or Open DS), despite of validating against database. 
Primary activities i would like to do are:
 1) Validate userName and password 
 2) Add a user or Group to directoy server
 3) Assigning a user to a Group.
Any light weight Java API, that provides quick learning curve.","I have a good experience with UnboundID LDAP SDK . It is just a single jar , has a good documentation ,  very user-friendly API , and a lot of advantages  when compared to the JNDI .
I especially like its ORM framework which can easily map a LDAP record to a Java object .",https://stackoverflow.com/questions/11886676/light-weight-java-api-for-ldap/11887546,3,0
"Why does CUDA 8.0 (sometimes) have a bad memory access, while 7.5 doesn't?","I was upgrading to CUDA 8.0 when some of the code started giving different results. I managed to locate roughly replicate the issue with a MCVE and solve my issue.
#include <cub/cub.cuh> // Tested with cub 1.5.5

#include <stdio.h>

static inline void f(cudaError_t err, const char *file, int line)
{
    if (err != cudaSuccess) {
        fprintf(stderr, ""ERROR in file %s, line %d: %s (%d)\n"", file, line, cudaGetErrorString(err), err);
        fprintf(stdout, ""ERROR in file %s, line %d: %s (%d)\n"", file, line, cudaGetErrorString(err), err);
    }
}

#define CHKERR(expr) do {f(expr, __FILE__, __LINE__);} while(0)

template<int dimSize>
__device__ __inline__ void UsedToWork(double *s_arr)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    typedef cub::BlockReduce<double, dimSize> BlockReduce;
    __shared__ typename BlockReduce::TempStorage temp_storage;

    // This following line was the issue
    double r = BlockReduce(temp_storage).Sum(s_arr[idx], dimSize);
    __syncthreads();
    if (idx == 0)
        printf(""t0 here %f\n\n"", r);
}

template<int size>
__global__ void ShouldWork(double *input)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    __shared__ double s_arr[size];
    if (idx < size)
        s_arr[idx] = input[idx];
    __syncthreads();

    UsedToWork<size>(s_arr);
}

int main()
{
    const int arraySize = 32;
    double h[arraySize] = { 
         1,  2,  3,  4,  5,  6,  7,  8,  9, 10,
        11, 12, 13, 14, 15, 16, 17, 18, 19, 20,
        21, 22, 23, 24, 25, 26, 27, 28, 29, 30,
        31, 32
    };

    double *d = 0;
    cudaError_t cudaStatus;

    CHKERR(cudaMalloc((void**)&d, arraySize * sizeof(double)));
    CHKERR(cudaMemcpy(d, h, arraySize * sizeof(double), cudaMemcpyHostToDevice));

    ShouldWork<32><<<1, arraySize * 2 >>>(d);

    CHKERR(cudaGetLastError());
    CHKERR(cudaDeviceSynchronize());
    CHKERR(cudaFree(d));

    return 0;
}

I replaced the line of interest with
double r = BlockReduce(temp_storage).Sum((idx < dimSize ? s_arr[idx] : 0.), dimSize);

ensuring that if idx was greater than dimSize (the size of the array) it wouldn't accessed (an illegal memory access was encountered (77)). While this was obviously an error, why did CUDA 7.5 allow the memory access without issue in the first place? Just to make things more interesting, if in the kernel I replace the
UsedToWork<size>(s_arr);

call with its (should be inlined anyway) definition
typedef cub::BlockReduce<double, size> BlockReduce;
__shared__ typename BlockReduce::TempStorage temp_storage;
double r = BlockReduce(temp_storage).Sum(s_arr[idx], size);
__syncthreads();

CUDA 8.0 doesn't give me the an illegal memory access was encountered (77) error. Now I'm just confused. Shouldn't the behavior at least be consistent?
Compiled on Windows 7, VS2013. Run on a Titan with 369.30.","The GPU has a runtime memory checker the details of which are unpublished.  This memory checking facility is not perfectly precise, but if an egregious enough error occurs (out-of-bounds access by a sufficient margin, say) then the runtime memory checking will flag a fault, halt the kernel, and declare that the context is corrupted.
The specific conditions which this will occur in are unpublished, and may vary from GPU architecture to GPU architecture, and also from CUDA version to CUDA version, as well as other possible factors.
As was conjectured in the comments, a possible mechanism for runtime error checking could be if the GPU code touches memory not associated with its context.  A given array out-of-bounds index then may have a dependency on where that array happens to be located in the context memory map, to determine if a particular out-of-bounds extent will actually go outside the context.
Such a memory map is quite possibly variable from CUDA version to CUDA verison, GPU architecture to architecture, and even probably depending on specific compile switches.
For best (strictest) memory access validity checking, its recommended to use the cuda-memcheck tool.  It's quite possible, for example, that a code that passes all CUDA runtime error checking will fail (and in fact have an actual coding defect) if run under cuda-memcheck.
There is no stated guarantee that the GPU will in normal operation detect invalid memory accesses.  It certainly has some capacity to do so, but it's not perfect.  I believe similar statements can be made about host code for the operating environments I am familiar with.",https://stackoverflow.com/questions/40402053/why-does-cuda-8-0-sometimes-have-a-bad-memory-access-while-7-5-doesnt/40434212,1,0
Portal maintenance documents [closed],"Closed. This question is off-topic. It is not currently accepting answers.
                            
                        










Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                        
Closed 5 months ago.



We outsourced a web based portal and now we're not interested to work with them because the site is becoming more buggy day by day and increase of customers complain. 
We've chosen a different team based on their local reputation and their portfolio are good to satisfy our urgent needs. We discussed this project with them and during a week they suggested some good ideas which help us to control. We are agreed to award this project to them. What I am thinking during the transition:
What documents do you think which can help new developers to understand the application? How many kinds of document I can request to them? If a new developers start working on it those documents help them to understand easily of all sides (application, database, configuration etc)
The application is on ASP.NET and SQL SERVER 2005 and the scariest part of all no source control tool is there. They do direct deployments without even push the publish button. Touch Luck :(
Thanks.","That's kind of hard to answer without knowing what kind of portal it is, but what comes to mind:

Owner's manual: Description of functionality, technologies used, full overview of all machines and services involved (don't forget data bases)
Backup: How and where is data backed up, where to restore it from in case of a crash
Description of all Databases used, relations between tables, at least quick rundown on what data is stored where
Links to any and all URLs to administration interfaces, tools, and scripts
Day-to-day operation: What cron jobs need to run frequently, are there caches, file lists or other things that need to be taken care of frequently
Make sure all domains used belong to you and are under your control
a description of the project's file structure (which part is where; where is the API; where are the visual elements; where are the front controllers)
How-To's on how to change the visual elements of the site (Style sheets, forms, templates...) 
A description of any and all URL rewriting operations that take place in various parts of the systems, and where they point to
Which Google Analytics / Google Webmaster account is used and how to get hold of it
Ideally, an API documentation and full phpDoc style source code documentation",https://stackoverflow.com/questions/2108907/portal-maintenance-documents/2108979,2,0
WinForms - Trying to make a background maximized form which stays in background and doesn't respond,"In short, I want to make a background form which takes up the whole screen and always stays at the back - without setting any of the other application forms which pop up to be set to its child when they are created.
I've got a WinForms app I'm occasionally maintaining which needs a lot of architectural improvements.  It's a touchscreen kiosk app.  It's going to get some serious attention in the future, but right now, I need to put a band-aid on it.
Problem is that the app was designed with windows coming and going and there is no main windows which the other windows are children of.  And everything is multi-threaded because it's responding to a lot of events from different sources.  Plus everything is confusingly Castle-Windsor-ized and this makes all changes problematic and defect-multiplying.
So some of the ""dialogs"" which come up aren't in dialog mode and don't have parents and don't take up the full screen like they should.  And I guess it wasn't picked up in test because sometimes prior windows would stick around and sometimes they wouldn't due to some bugs in the event handling.  There are a lot of latent bugs and design flaws in the system, so things break when I disrupt it too much.
This means that sometimes the desktop gets visible (like around the edge of a dialog which isn't set to full-screen, when a previously lingering form is properly disposed) and this means that operators could get on the net and surf porn or whatever.
I made a form which works and I set an event handler on most of the relevant events (which just calls SendToBack()) if they happen - like a focus or mouse-click or whatever.
I'm concerned this won't catch everything and the users will see this blank grey form and do nothing (if they clicked on it, it would go to the back).
Is there a guaranteed way to force this form to stay in the background?","public partial class BackgroundForm : Form
{

    const int WM_MOUSEACTIVATE = 0x21;
    const int MA_NOACTIVATEANDEAT = 0x0004;

    public BackgroundForm()
    {
        InitializeComponent();
        ShowInTaskbar = false;
    }

    protected override void DefWndProc(ref Message m)
    {
        if (m.Msg == WM_MOUSEACTIVATE)
        {
            m.Result = (IntPtr) MA_NOACTIVATEANDEAT;
            return;
        }

        base.DefWndProc(ref m);
    }

    protected override bool ShowWithoutActivation
    {
        get { return true; }
    }

}",https://stackoverflow.com/questions/7045524/winforms-trying-to-make-a-background-maximized-form-which-stays-in-background,1,0
__oader Constraint Violation_ when converting Eclipse dynamic web project to Maven,"I have created a simple dynamic web application in eclipse , Java web. And I have used Spring mvc architecture . The program is working fine it has a simple form handling function. Now i want to convert my project in to a Maven project but when I go Configuration->convert to maven, It gives me a ""Loader Constraint Violation"" error. I'm using Eclipse Luna.",maybe there is a problem with the maven plug-in you installed. try re install the plug-in and see,https://stackoverflow.com/questions/34986500/loader-constraint-violation-when-converting-eclipse-dynamic-web-project-to-mav,1,0
Flash AS3 Security Sandbox Violation / s.ytimg.com,"I have been reading about this trouble for days now and i'm going mad. Please Help!
I'm trying to load in vain a YouTube video into my portfolio. I'v uploaded the video on Youtube and it's Public. 
I've tried the code with a bunch of other Youtube videos and it works perfect with them all except with mine...
Like i said i'm going MAD!!! please help!!
* Security Sandbox Violation *
SecurityDomain 'http://s.ytimg.com/crossdomain.xml' tried to access incompatible context 'file:///M|/Architecture/PORTFOLIO/WEB/PORTFOLIO.swf'
Here's the code of the part in context:
import flash.display.Loader;
import flash.net.URLRequest;
import fl.transitions.Tween;
import fl.transitions.easing.*;
import flash.events.MouseEvent;
import flash.display.MovieClip;
import fl.controls.ProgressBar;
import fl.controls.ProgressBar;
import fl.transitions.TweenEvent;
import flash.events.Event;
import fl.controls.ProgressBar;
import flash.ui.MouseCursor;
import flash.system.Security;

pages.visible = false;
pages.fullPage.alpha = 0;
var my_images:XMLList;
var my_total:Number;

var page_no:Number = 1;
pages.pb.alpha = 0;

pages.btnBallon.visible = false;
pages.btnBallon.removeEventListener(MouseEvent.CLICK, lVid);
pages.btnBallon.buttonMode = false;

//Youtube loader

Security.allowDomain(""*"");
Security.allowDomain(""www.youtube.com"");
Security.allowDomain(""youtube.com"");
Security.allowDomain(""s.ytimg.com"");
Security.allowDomain(""i1.ytimg.com"");
Security.allowDomain(""i.ytimg.com"");
Security.allowDomain(""http://s.ytimg.com/crossdomain.xml"");
Security.allowDomain(""popslinger.org"");
Security.allowDomain(""http://www.box.net/api/crossdomain.xml"");

Security.loadPolicyFile(""http://i.ytimg.com/crossdomain.xml"");
Security.loadPolicyFile(""http://www.youtube.com/crossdomain.xml"");
Security.loadPolicyFile(""http://s.ytimg.com/crossdomain.xml"");
Security.loadPolicyFile('http://youtube.com/crossdomain.xml');
Security.loadPolicyFile(""http://www.box.net/api/crossdomain.xml"");



function video():void{
pages.btnBallon.visible = true;
pages.btnBallon.addEventListener(MouseEvent.CLICK, lVid);
pages.btnBallon.buttonMode = true;
}

function lVid(e:Event = null):void{

loader.load(new URLRequest(""http://www.youtube.com/apiplayer?version=3""));

loader.contentLoaderInfo.addEventListener(Event.INIT, onInit);
}

function onInit(e:Event):void{
addChild(loader);
player = loader.content;
player.addEventListener(""onReady"",onPlayerReady);
}

function onPlayerReady(e:Event):void{
var pageTween:Tween = new Tween(pages.fullPage, ""alpha"",     Strong.easeOut,pages.fullPage.alpha, 0, 1, true);
player.setSize = (560,315);
player.x = pages.x + 120;
player.y = pages.y;
pages.visible = false;
pages.btnBallon.visible = false;
pages.btnBallon.removeEventListener(MouseEvent.CLICK, lVid);
pages.btnBallon.buttonMode = false;
/*pages.btnNext.removeEventListener(MouseEvent.CLICK, nextPage);
pages.btnNext.visible = false;
pages.btnPrev.removeEventListener(MouseEvent.CLICK, nextPage);
pages.btnPrev.visible = false;*/
player.loadVideoByUrl(""www.youtube.com/embed/omcl93EYTrM"",0);
//player.loadVideoById(""omcl93EYTrM"",0);    

}","Try change this value:
File -> Publish settings",https://stackoverflow.com/questions/23437313/flash-as3-security-sandbox-violation-s-ytimg-com,1,0
SOAP vs REST (differences),"I have read articles about the differences between SOAP and REST as a web service communication protocol, but I think that the biggest advantages for REST over SOAP are: 

REST is more dynamic, no need to create and update UDDI(Universal Description, Discovery, and Integration).
REST is not restricted to only XML format. RESTful web services can send plain text/JSON/XML.

But SOAP is more standardized (E.g.: security).
So, am I correct in these points?","Unfortunately, there are a lot of misinformation and misconceptions around REST. Not only your question and the answer by @cmd reflect those, but most of the questions and answers related to the subject on Stack Overflow.
SOAP and REST can't be compared directly, since the first is a protocol (or at least tries to be) and the second is an architectural style. This is probably one of the sources of confusion around it, since people tend to call REST any HTTP API that isn't SOAP.
Pushing things a little and trying to establish a comparison, the main difference between SOAP and REST is the degree of coupling between client and server implementations. A SOAP client works like a custom desktop application, tightly coupled to the server. There's a rigid contract between client and server, and everything is expected to break if either side changes anything. You need constant updates following any change, but it's easier to ascertain if the contract is being followed.
A REST client is more like a browser. It's a generic client that knows how to use a protocol and standardized methods, and an application has to fit inside that. You don't violate the protocol standards by creating extra methods, you leverage on the standard methods and create the actions with them on your media type. If done right, there's less coupling, and changes can be dealt with more gracefully. A client is supposed to enter a REST service with zero knowledge of the API, except for the entry point and the media type. In SOAP, the client needs previous knowledge on everything it will be using, or it won't even begin the interaction. Additionally, a REST client can be extended by code-on-demand supplied by the server itself, the classical example being JavaScript code used to drive the interaction with another service on the client-side.
I think these are the crucial points to understand what REST is about, and how it differs from SOAP:

REST is protocol independent. It's not coupled to HTTP. Pretty much like you can follow an ftp link on a website, a REST application can use any protocol for which there is a standardized URI scheme.
REST is not a mapping of CRUD to HTTP methods. Read this answer for a detailed explanation on that.
REST is as standardized as the parts you're using. Security and authentication in HTTP are standardized, so that's what you use when doing REST over HTTP.
REST is not REST without hypermedia and HATEOAS. This means that a client only knows the entry point URI and the resources are supposed to return links the client should follow. Those fancy documentation generators that give URI patterns for everything you can do in a REST API miss the point completely. They are not only documenting something that's supposed to be following the standard, but when you do that, you're coupling the client to one particular moment in the evolution of the API, and any changes on the API have to be documented and applied, or it will break.
REST is the architectural style of the web itself. When you enter Stack Overflow, you know what a User, a Question and an Answer are, you know the media types, and the website provides you with the links to them. A REST API has to do the same. If we designed the web the way people think REST should be done, instead of having a home page with links to Questions and Answers, we'd have a static documentation explaining that in order to view a question, you have to take the URI stackoverflow.com/questions/<id>, replace id with the Question.id and paste that on your browser. That's nonsense, but that's what many people think REST is.

This last point can't be emphasized enough. If your clients are building URIs from templates in documentation and not getting links in the resource representations, that's not REST. Roy Fielding, the author of REST, made it clear on this blog post: REST APIs must be hypertext-driven. 
With the above in mind, you'll realize that while REST might not be restricted to XML, to do it correctly with any other format you'll have to design and standardize some format for your links. Hyperlinks are standard in XML, but not in JSON. There are draft standards for JSON, like HAL.
Finally, REST isn't for everyone, and a proof of that is how most people solve their problems very well with the HTTP APIs they mistakenly called REST and never venture beyond that. REST is hard to do sometimes, especially in the beginning, but it pays over time with easier evolution on the server side, and client's resilience to changes. If you need something done quickly and easily, don't bother about getting REST right. It's probably not what you're looking for. If you need something that will have to stay online for years or even decades, then REST is for you.",https://stackoverflow.com/questions/19884295/soap-vs-rest-differences/19884975,10,0
S3 REST API and POST method,"I'm using AWS S3 REST API, and after solving some annoying problems with signing it seems to work. However, when I use correct REST verb for creating resource, namely POST, I get 405 method not allowed. Same request works fine with method PUT and creates resource. 
Am I doing something wrong, or is AWS S3 REST API not fully REST-compliant?","Yes, you are wrong in mapping CRUD to HTTP methods.
Despite the popular usage and widespread misconception, including high-rated answers here on Stack Overflow, POST is not the ""correct method for creating resource"". The semantics of other methods are determined by the HTTP protocol, but the semantics of POST are determined by the target media type itself. POST is the method used for any operation that isn't standardized by HTTP, so it can be used for creation, but also can be used for updates, or anything else that isn't already done by some other method.  For instance, it's wrong to use POST for retrieval, since you have GET standardized for that, but it's fine to use POST for creating a resource when the client can't use PUT for some reason. 
In the same way, PUT is not the ""correct method for updating resource"". PUT is the method used to replace a resource completely, ignoring its current state. You can use PUT for creation if you have the whole representation the server expects, and you can use PUT for update if you provide a full representation, including the parts that you won't change, but it's not correct to use PUT for partial updates, because you're asking for the server to consider the current state of the resource. PATCH is the method to do that.
In informal language, what each method says to the server is:

POST: take this data and apply it to the resource identified by the given URI, following the rules you documented for the resource media type.
PUT: replace whatever is identified by the given URI with this data, ignoring whatever is in there already, if anything.
PATCH: if the resource identified by the given URI still has the same state it had the last time I looked, apply this diff to it.

Notice that create or update isn't mentioned and isn't part of the semantics of those methods. You can create with POST and PUT, but not PATCH, since it depends on a current state. You can update with any of them, but with PATCH you have an update conditional to the state you want to update from, with PUT you update by replacing the whole entity, so it's an idempotent operation, and with POST you ask the server to do it according to predefined rules.
By the way, I don't know if it makes sense to say that an API is or isn't REST-compliant, since REST is an architectural style, not a spec or a standard, but even considering that, very few APIs who claim to be REST are really RESTful, in most cases because they are not hypertext driven. AWS S3 is definitely not RESTful, although where it bears on your question, their usage of HTTP methods follows the HTTP standard most of the time.",https://stackoverflow.com/questions/19843480/s3-rest-api-and-post-method/19844272,4,0
What is the purpose of the __AUSE_ instruction in x86?,"I am trying to create a dumb version of a spin lock. Browsing the web, I came across a assembly instruction called ""PAUSE"" in x86 which is used to give hint to a processor that a spin-lock is currently running on this CPU. The intel manual and other information available state that 

The processor uses this hint to avoid the memory order violation in
  most situations, which greatly improves processor performance. For
  this reason, it is recommended that a PAUSE instruction be placed in
  all spin-wait loops. The documentation also mentions that  ""wait(some
  delay)"" is the pseudo implementation of the instruction.

The last line of the above paragraph is intuitive. If I am unsuccessful in grabbing the lock, I must wait for some time before grabbing the lock again.
However, what do we mean by memory order violation in case of a spin lock? 
Does ""memory order violation"" mean the incorrect speculative load/store of the instructions after spin-lock?
The spin-lock question has been asked on Stack overflow before but the memory order violation question remains unanswered (at-least for my understanding).","Just imagine, how the processor would execute a typical spin-wait loop:
1 Spin_Lock:
2    CMP lockvar, 0   ; Check if lock is free
3    JE Get_Lock
4    JMP Spin_Lock
5 Get_Lock:

After a few iterations the branch predictor will predict that the conditional branch (3) will never be taken and the pipeline will fill with CMP instructions (2). This goes on until finally another processor writes a zero to lockvar. At this point we have the pipeline full of speculative (i.e. not yet committed) CMP instructions some of which already read lockvar and reported an (incorrect) nonzero result to the following conditional branch (3) (also speculative). This is when the memory order violation happens. Whenever the processor ""sees"" an external write (a write from another processor), it searches in its pipeline for instructions which speculatively accessed the same memory location and did not yet commit. If any such instructions are found then the speculative state of the processor is invalid and is erased with a pipeline flush.
Unfortunately this scenario will (very likely) repeat each time a processor is waiting on a spin-lock and make these locks much slower than they ought to be.
Enter the PAUSE instruction:
1 Spin_Lock:
2    CMP lockvar, 0   ; Check if lock is free
3    JE Get_Lock
4    PAUSE            ; Wait for memory pipeline to become empty
5    JMP Spin_Lock
6 Get_Lock:

The PAUSE instruction will ""de-pipeline"" the memory reads, so that the pipeline is not filled with speculative CMP (2) instructions like in the first example. (I.e. it could block the pipeline until all older memory instructions are committed.) Because the CMP instructions (2) execute sequentially it is unlikely (i.e. the time window is much shorter) that an external write occurs after the CMP instruction (2) read lockvar but before the CMP is committed.
Of course ""de-pipelining"" will also waste less energy in the spin-lock and in case of hyperthreading it will not waste resources the other thread could use better. On the other hand there is still a branch mis-prediction waiting to occur before each loop exit. Intel's documentation does not suggest that PAUSE eliminates that pipeline flush, but who knows...",https://stackoverflow.com/questions/13903692/intels-pause-instruction-and-possible-memory-order-violation,2,0
"In the flux architecture, who is responsible for sending updates to the server?","So in the flux architecture, data flows as follows:
View -> Action -> Dispatcher -> Store
 ^ <-----------------------------|

So let's say the view is a comment box. When the user submits a comment, an addComment action is triggered, but where should that comment be sent to the server? Should it happen in the action function, before dispatching it, or should the store doing it when receiving the action from the dispatcher?
Both cases seam like a violation of the single responsibility pattern. Or should there be two CommentStores, one LocalCommentStore and a ServerCommentStore that both handle the addComment action?","In your case Action is responsible for both sending a pending action or optimistic update to the store and sending a call to the WebAPI:
View -> Action -> Pending Action or optimistic update  -> Dispatcher -> Store -> emitEvent -> View 
               -> WebUtils.callAPI()

onWebAPISuccess -> Dispatcher -> Store -> emitEvent -> View
onWebAPIFail -> Dispatcher -> Store -> emitEvent -> View",https://stackoverflow.com/questions/26816906/in-the-flux-architecture-who-is-responsible-for-sending-updates-to-the-server,2,0
How to design a database for subcategories with different field sets?,"For a school project, I am designing a system to document different types of architectural violations for disabled people (Sorry for my language if I am being rude, I am not English speaker). 
There are many different categories and subcategories of violations, and each subcategory of violation has different set of fields. The form for adding violations will be dynamic, according to the subcategory, user will face different set of fields. A field may be binary or integer. Some of the violations may have multiple fields.
My question is how should I design such a system's database?
The idea that I had so far is something like this
What do you think about this design? This is the first time I implement such a system, so I have no idea about how will it work.","Take a look at how Wordpress is designed, with many fewer tables than other cmses - the posts table only has basic fields, while the variations on data needed for many different post-types are handled in wp-postmeta - which has four fields - post id, meta name and meta value (plus order, which is seldom needed).",https://stackoverflow.com/questions/33725515/how-to-design-a-database-for-subcategories-with-different-field-sets,1,0
"ReactiveX, about mobile app architecture and layers","I'm learning Rx (RxSwift in particular) and I have a question about architecture, layers and boundaries.
I'm used to layered architectures (data, domain, presentation), usually on MVP or VIPER. For this project I'm using MVVM, the recommended architecture for a Reactive app. These are my current collaborators:
**** Presentation ******************************************

                 ________________________
                |                        |
                |   GameViewController   |
                |                        |
                |      ____________      |
                |     |            |     |
                |     | BoardView  |     |
                |     |____________|     |
                |                        |
                |________________________|
                             |
                             |
                            \|/
                 ________________________
                |                        |
                |     GameViewModel      |
                |________________________|
                             |
**** Domain **************** | *****************************
                            \|/
                 ________________________
                |                        |
                |     GameController     |
                |________________________|

When the user taps (makes a move) the BoardView emits an event, which is being observed by the GameViewController, which calls a method in GameViewModel that communicates with GameController to check if the move is correct and then emits another event being observed by each one in the chain, and finally BoardView draws its stuff according to the correctness of the move.
My question is, is this flow correct? Do I have to stick to this way of doing things or there is a Reactive way that suits better? For example, maybe BoardView can talk directly to the view model without the view controller being involved, and there is not boundary breaking nor ""violation of rules"".
I'm a bit lost in terms of Rx better architectures, MVVM is simple but to make it SOLID you have to create more collaborators, and then the chain of observables could be a bit over-engineered.
Any help will be really appreciated! Thanks :)","In a well written reactive app your logic will tend to be encapsulated in a number of stateless (static) functions rather than objects. Thus making your code more declarative.
As an example, your GameViewModel should accept a number of input observables that it will observe and produce a number of output observables for the views to subscribe to. 
struct MyViewModel {
    let output: Observable<OutState> 
    init(input: Observable<InState>) {
        output = input.map {
            // transform input state into output state
        }
    }
}

Note in the above, that the transform is a pure function. Also note that the object itself is superfluous. It could as easily be a function:
func myOutput(input: Observable<InState>) -> Observable<OutState> {
    return input.map {
        // transform input state into output state
    }
}

That transform block could, of course, be a function itself.
func transform(in: InState) -> OutState {
}

Which is very easy to test and naturally encapsulates a particular use-case of your application.",https://stackoverflow.com/questions/47094535/reactivex-about-mobile-app-architecture-and-layers,2,0
opencv 3.0 findContours function not working in window,"I am using visual studio 15 and working in opencv 3.0 ,i am  getting access violation error in my code and even this function is not working with sample code given in opencv.
#include""stdafx.h""
#include ""opencv2/imgproc/imgproc.hpp""
#include ""opencv2/highgui/highgui.hpp""
#include <math.h>
#include <iostream>

using namespace cv;
using namespace std;

static void help()
{
    cout
        << ""\nThis program illustrates the use of findContours and drawContours\n""
        << ""The original image is put up along with the image of drawn contours\n""
        << ""Usage:\n""
        << ""./contours2\n""
        << ""\nA trackbar is put up which controls the contour level from -3 to 3\n""
        << endl;
}

const int w = 500;
int levels = 3;

vector<vector<Point> > contours;
vector<Vec4i> hierarchy;

static void on_trackbar(int, void*)
{
    Mat cnt_img = Mat::zeros(w, w, CV_8UC3);
    int _levels = levels - 3;
    drawContours(cnt_img, contours, _levels <= 0 ? 3 : -1, Scalar(128, 255, 255),
        3, LINE_AA, hierarchy, std::abs(_levels));

    imshow(""contours"", cnt_img);
}

int main(int argc, char**)
{
    Mat img = Mat::zeros(w, w, CV_8UC1);
    if (argc > 1)
    {
        help();
        return -1;
    }
    //Draw 6 faces
    for (int i = 0; i < 6; i++)
    {
        int dx = (i % 2) * 250 - 30;
        int dy = (i / 2) * 150;
        const Scalar white = Scalar(255);
        const Scalar black = Scalar(0);

        if (i == 0)
        {
            for (int j = 0; j <= 10; j++)
            {
                double angle = (j + 5)*CV_PI / 21;
                line(img, Point(cvRound(dx + 100 + j * 10 - 80 * cos(angle)),
                    cvRound(dy + 100 - 90 * sin(angle))),
                    Point(cvRound(dx + 100 + j * 10 - 30 * cos(angle)),
                        cvRound(dy + 100 - 30 * sin(angle))), white, 1, 8, 0);
            }
        }

        ellipse(img, Point(dx + 150, dy + 100), Size(100, 70), 0, 0, 360, white, -1, 8, 0);
        ellipse(img, Point(dx + 115, dy + 70), Size(30, 20), 0, 0, 360, black, -1, 8, 0);
        ellipse(img, Point(dx + 185, dy + 70), Size(30, 20), 0, 0, 360, black, -1, 8, 0);
        ellipse(img, Point(dx + 115, dy + 70), Size(15, 15), 0, 0, 360, white, -1, 8, 0);
        ellipse(img, Point(dx + 185, dy + 70), Size(15, 15), 0, 0, 360, white, -1, 8, 0);
        ellipse(img, Point(dx + 115, dy + 70), Size(5, 5), 0, 0, 360, black, -1, 8, 0);
        ellipse(img, Point(dx + 185, dy + 70), Size(5, 5), 0, 0, 360, black, -1, 8, 0);
        ellipse(img, Point(dx + 150, dy + 100), Size(10, 5), 0, 0, 360, black, -1, 8, 0);
        ellipse(img, Point(dx + 150, dy + 150), Size(40, 10), 0, 0, 360, black, -1, 8, 0);
        ellipse(img, Point(dx + 27, dy + 100), Size(20, 35), 0, 0, 360, white, -1, 8, 0);
        ellipse(img, Point(dx + 273, dy + 100), Size(20, 35), 0, 0, 360, white, -1, 8, 0);
    }
    //show the faces
    namedWindow(""image"", 1);
    imshow(""image"", img);
    //Extract the contours so that
    //vector<vector<Point> > contours0;
    vector<cv::Mat> coutours;
    findContours(img, contours, hierarchy, RETR_TREE, CHAIN_APPROX_SIMPLE);

    contours.resize(contours.size());
    for (size_t k = 0; k < contours.size(); k++)
        approxPolyDP(Mat(contours[k]), contours[k], 3, true);

    namedWindow(""contours"", 1);
    createTrackbar(""levels+3"", ""contours"", &levels, 7, on_trackbar);

    on_trackbar(0, 0);
    waitKey();

    return 0;
}

I am using x64 architecture and linked all the library .lib along with d.lib(debug library).","I think the problem comes from your ""contours"" variable. You're declaring it as a vector<cv::Mat>, but the contours are not represented as a matrix, but rather as a series of points.
Look at this example : http://docs.opencv.org/2.4/doc/tutorials/imgproc/shapedescriptors/find_contours/find_contours.html
They declare the contours as vector<vector<Point> > contours;
Look also at the declaration of the function (http://docs.opencv.org/2.4/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html?highlight=findcontours#findcontours), the paramater contour is defined as : contours _ Detected contours. Each contour is stored as a vector of points.",https://stackoverflow.com/questions/34616778/opencv-3-0-findcontours-function-not-working-in-window,1,0
"Error: name_:__nvariant Violation_,__ramesToPop"":1","I'm seeing this strange error.
I'm writing an app which uses the graph api to retrieve event details from facebook.
The event has a couple of attributes from which:
 - owner which is an object containing owner id, owner name, and other attributes
 - cover which is an object representing the event cover image details.
I save events in a mongo database, here is what my event model looks like:
const EventSchema = new Schema({
  title: String,
  name: String,
  _id: {
    type: String,
    unique: true,
    default: shortid.generate,
  },
  start_time: Date,
  end_time: Date,
  description: String,
  owner: {},
  cover: {},
  venue: {},
  privacy: String,
  timezone: String,
  location: String,
  createdAt: { type: Date, default: Date.now },
  event_type: {},
});

I have an express route which sends back a given event by id:
router.get('/:id', (req, res) => {
  Event.findById(req.params.id).exec((error, events) => {
    if (error){
      res.json(error);
    }
    res.json(events);
  })
});

My component architecture goes like this:
-EventPage component which contains an EventDetails component.
import React, { PropTypes } from 'react'
import { connect } from 'react-redux'
import axios from 'axios';
import EventDetails from './eventDetails';

class EventPage extends React.Component {
  constructor(props) {
    super(props);

    this.state = {
      event: {},
    };
  }
  componentWillMount() {
    axios.get(`/api/events/${this.props.params.id}`)
      .then((eventResponse) => {
        this.setState({
          event: eventResponse.data
        })
      }).catch((err) => {
        console.log(JSON.stringify(err));
      })
  }
  render() {
    return (
      <div className=""row"">
        <EventDetails event={this.state.event} />
      </div>
    )
  }
}

EventPage.propTypes = {
};

export default EventPage;




import React, { PropTypes } from 'react'
import { connect } from 'react-redux'
import _ from 'lodash';

class EventDetails extends React.Component {
  constructor(props) {
    super(props);
  }
  render() {
    const { name, description, start_time, end_time, owner } = this.props.event;
    return(
      <div className='row'>
        <h1>{name}</h1>
        <p>{description}</p>
          <p>{JSON.stringify(this.props.event)}</p>
          <p>{this.props.event.owner}</p>
        <pre>

        </pre>


      </div>
    )
  }
}

EventDetails.propTypes = {
};

export default EventDetails;

Trying to display the event owner's name results in this error:
{""name"":""Invariant Violation"",""framesToPop"":1}
The error comes from the axios error handler in the EventPage component. 
Anyone sees what I've done wrong here?
Thanks for your help","I had probably the same problem with {""name"":""Invariant Violation"",""framesToPop"":1}.
I've passed a javascript object instead of an array and it worked for me.
    Message.find({}).sort({'date': -1}).limit(50).exec().then( (doc, err) => {
        console.log('found');
        const messages = [];
        doc.map( (item) => {
            messages.push({data: item});
        });
        callback(err, {items: messages}); 
    });",https://stackoverflow.com/questions/40251052/error-nameinvariant-violation-framestopop1,1,0
x86 Assembly error with printf and exit form C,"i got some problems with assembly code. I'm trying to call C function print. 
I compile file with:    
gcc helloC.s -o hello

And i got error like:
 /tmp/cc0SwfB8.o: In function `_start':
(.text+0x0): multiple definition of `_start'
/usr/lib/gcc/x86_64-linux-gnu/4.4.3/../../../../lib/crt1.o:(.text+0x0): first defined here
/usr/lib/gcc/x86_64-linux-gnu/4.4.3/../../../../lib/crt1.o: In function `_start':
(.text+0x20): undefined reference to `main'
collect2: ld returned 1 exit status

This is a program given by our professor. I'm compiling by putty on univesity computer. Linux function: lscpu says it is x86_64 architecture.
Code given by our professor:
#PURPOSE: This program writes the message ""hello world"" and
# exits
#
.section .data
helloworld:
.ascii ""hello world\n\0""
.section .text
.globl _start
_start:
main:
pushq $helloworld
call printf
pushq $0

call exit

The only thing i changed in this code is push -> pushq because of 64bit architecture. 
Edit:
as fuz said i changed .globl _start to .globl main. Olso i changed label _start: to main:
I compiles without error. 
When i lunch program with ./hello it says something like :""Memory protection violation"" (translated form my native language. (Naruszenie ochrony pami_ci))
#PURPOSE: This program writes the message ""hello world"" and
# exits
#
.section .data
helloworld:
.ascii ""hello world\n\0""
.section .text
.globl main
main:
pushq $helloworld
call printf
pushq $0

call exit","You cannot convert x86 assembly code to x64 assembly code mechanically like that.
Choose:

Compile for x86 and run.
Learn calling conventions and rewrite
rewrite in C

The reason you got memory protection violation is you called printf with arbitrary arguments. The first argument to printf goes in the rdi register in x64, not on the stack.",https://stackoverflow.com/questions/49819589/x86-assembly-error-with-printf-and-exit-form-c,2,0
mex code crash on new matlab version / architecture,"I have a routine that computes the shift-invariant discrete wavelet transform, as specified in the Rice Wavelet Toolbox, in the frequency domain. The code is in MEX (c-syntax) and uses some extra routines to do imaginary polynomials etc. in a separate file.
The source files fsidwt.c, fisidwt.c and the helper routines polyphase.[chm] are zipped together in a file here: http://ubuntuone.com/6zXIIuA3J4OTTlSquycMlz
This code worked without problems in older matlab versions, and other matlab functions depend on it.
Now however, when I compile it now, there is first a warning message
>> mex fisidwt.c polyphase.c % message about different gcc version
>> mex fsidwt.c polyphase.c % message about different gcc version
    fsidwt.c: In function __ultiMRFWD1D_:
    fsidwt.c:187: warning: cast to pointer from integer of different size
    fsidwt.c:188: warning: cast to pointer from integer of different size
    fsidwt.c:189: warning: cast to pointer from integer of different size
    fsidwt.c:190: warning: cast to pointer from integer of different size

which is odd because no integers are cast to pointers. But it's only a warning, so let's carry on.
Now the mex files fsidwt and fisidwt compute the forward and inverse frequency-domain shift-ivariant wavelet transform.
My test program is very simple:
>> clear all; len_sig=256; wlevels=3; numsig=1; numtest=1; 
    % 256 points, 3 wavelet decomposition levels
>> st=4; ts=(1:(len_sig/st))'*(1:st); ts=ts(:); ts=ts*ones(1,numsig); Ts=fft(ts); 
    % sawtooth with 4 'teeth' of increasing height and its FFT
>> h=[1;1]/sqrt(2); g=qmf(h); h=[h(:) g(:)]; H=fft(h,len_sig); 
    % Haar wavelet filters and 256-point FFTs
>> [ffs ffd] = fsidwt(Ts, H, wlevels); 
    % forward wavelet transform
>> ffr=fisidwt(ffs,ffd,cH,levels); 
    % recontruction

Unfortunately it exits with a segmentation violation whose cause I cannot trace from the core dump output...
The lines in the code [in the zipfile at http://ubuntuone.com/6zXIIuA3J4OTTlSquycMlz], line numbers 187-190 of fsidwt.c, read:
Hfilter2d = (dComplexMat) dComplexMake2D ( hcomp[0],  NQ, Q);
Gfilter2d = (dComplexMat) dComplexMake2D ( hcomp[1],  NQ, Q);
Detail2d  = (dComplexMat) dComplexMake2D ( workspaced, Q, NQ);
Approx2d  = (dComplexMat) dComplexMake2D ( workspacec, Q, NQ);

All the LHS are of type
dComplexMat

which is
typedef struct {double r,i;} dComplex;
typedef dComplex *dComplexVec;
typedef dComplexVec *dComplexMat;

All the RHS are of type
(dComplexVec, long, long)

and the code of dComplexMake2D() is:
dComplexMat dComplexMake2D(
            dComplexVec array1D,
            int width, int height) {
    register int i;
    dComplexMat theMatrix = (dComplexMat) mxCalloc ( width, sizeof(dComplexVec) );
    theMatrix[0]=(dComplexVec)array1D;
    for(i=1;i<width;i++)
        theMatrix[i] = theMatrix[i-1] + height;
    return theMatrix;
}

How would I start finding the cause of this segmentation violation? Does it have to do with the warning? Is it the new Matlab version? Or the 64-bit architecture?","Oh dear. The MathWorks helpdesk found, as they were investigating this, that the function header for dComplexMake2D had an extra d that I must have overlooked at least 42 times.
So embarrassing. But anyway, the code works like a dream (will put it online soon) and a lesson learned:
MEX warned about the return type of dComplexMake2D as it could not find a matching header, carried on (as it appears to do) but assumed return type int, linked to the compiled file where dComplexMake2D returns a pointer (a long int sized type) and that's where things went wrong...
Thanks to the tech support at MathWorks. They did point out that debugging MEX files is not in any way their business.",https://stackoverflow.com/questions/15024059/mex-code-crash-on-new-matlab-version-architecture,1,0
Clean Architecture: Sequence Flow Among Frameworks,"I have been trying to learn more about Uncle Bob's Clean Architecture from blogs, article, and videos.
If I were to use a database in this architecture, then what should the UI (as a framework such as web or form) know about the database? Or more generally, how should data flow between two or more pieces/parts that are in the same layer?
For example, The UI would talk to my adapter(s)/gateway(s) to interact with the business entities. To Read/Write, I can see that the UI could call whatever class/classes that can access the database and pass in the adapter(s)/gateway(s), so that it can interact with the business entities. 
    public class SomeUI
    {
        public static void Main(string[] args)
        {
            SomeAdapter adapter = new SomeAdapter();
            SomeDataAccess db = new SomeDataAccess();
            db.Save(adapter);
        }
    }

    public class SomeDataAccess
    {
        public void Save(SomeAdapter adapter)
        {
            //Interact with database
        }
    }

    public class SomeAdapter
    {
        //properties
    }

Many of the articles barely vary off of this one (https://subvisual.co/blog/posts/20-clean-architecture). I have not found a good article that covers how pieces that are in the same layer should work with each other. So, articles referring to that would be an acceptable answer.
This does not appear to violate the Dependency Rule, but it feels like I am not doing something right, since I am making a dependency between my UI and database. I believe that I may be over-thinking the concept, and I believe it may be from hammering away at learning the 3-tier architecture (UI -> BLL -> DAL).","You ask:

If I were to use a database in this architecture, then what should the UI (as a framework such as web or form) know about the database? Or more generally, how should data flow between two or more pieces/parts that are in the same layer?

There is no such term as UI component in Clean Architecture. In Clean Architecture terms, the UI would be the presentation layer or the delivery mechanism, decomposed in the following components:

The view model generator (or the presenter to use Uncle Bob's terms), which is in charge of encapsulating the business rules for the UI. This should access the business model in order to generate the view model from it. The business model is passed to the presenter's method inside the interactor response object by its caller, the interactor.
The view model which holds data for the view and passed to the view indirectly via for example an event.
The dumb view which is now decoupled from the domain model and all layers, displays data of the view model.

Decomposing that this way insures better testability, better SRP and more decoupling with application, domain and infrastructure layers.
So your presentation layer should know absolutely nothing about the infrastructure layer.

Maybe you got confused by examples using some kind of Web Form component/library? This kind of component propose interconnected functionalities each in relation with several architecture layers: domain, application and presentation_ So Web Form components are particularly delicate to adapt satisfactory in a Clean Architecture. Due to this inflexibility, I'm still struggling figuring out what is the best way to integrate a Web Form component in my Clean Architecture implementations...

Finally, to make it clear, you said:

For example, The UI would talk to my adapter(s)/gateway(s) to interact with the business entities. To Read/Write, I can see that the UI could call whatever class/classes that can access the database and pass in the adapter(s)/gateway(s), so that it can interact with the business entities.

It's not UI's responsibility to interact with your entities but as its name suggest it's interactor's responsibility (interactor = use case). Interactor are meant to encapsulate application business rules, they represent the application layer. They can CRUD your entities via the Entity Gateway which is your adapter to the infrastructure layer which could be an ORM, a REST API or whatever...

Edit #1:
Since a picture worth a thousand words here is Uncle Bob's UML class diagram representing the structure (and the data flow between the concerned components) of the Clean Architecture:


Edit #2:
It seems to me that your representation of the flow of control in a Clean Architecture is kind of reversed. Taking as reference the above diagram and Uncle Bob's analogy:

If you don't want your code to be dependent of a thing, make this thing a plugin.

(Said otherwise, make that thing the client of your code that you want independent from it.)
In a Clean Architecture, you want the presentation layer, or more contextually, the delivery mechanism (Controller + Presenter + ViewModel + View) to be a plugin of your business layer (which is composed of components on the right side of the communication channel boundary).",https://stackoverflow.com/questions/41553601/clean-architecture-sequence-flow-among-frameworks,2,0
"app.exe has triggered a break point, followed by Access violation writing location","I have a Qt c++ application with a huge code base. I'm kinda familiar with the architecture but I don't know much of the code. Every time the application is closed, it shows the message ""app.exe has triggered a break point"" and if I hit continue, I get 

""Unhandled exception at 0x6701c95d (QtCored4.dll) in app.exe:
  0xC0000005: Access violation writing location 0x0c3e9fd8.""

with a stack trace looking like this

I have App Verifier hooked up and it dumps this in the output window
===========================================================
VERIFIER STOP 0000000000000013: pid 0x2654: first chance access violation for current stack trace 

    000000000C3E9FD8 : Invalid address being accessed
    000000006701C95D : Code performing invalid access
    000000000008EB40 : Exception record. Use .exr to display it.
    000000000008E650 : Context record. Use .cxr to display it.
===========================================================
This verifier stop is continuable. 
After debugging it use `go' to continue.
===========================================================

My question is, how would one go about trying to debug to find out what's causing this issue? I don't have much experience in the unmanaged programming world and I'm completely lost. Any help would be much appreciated.","Chances are you've either got some memory corruption caused by something like a buffer overflow, or a double deletion of a dangling pointer.  This sort of thing can happen when calling Qt code if you don't realise that the QObject's parent takes ownership of it, as Bart notes.  You are already using App Verifier, but I would also try running gflags from the Debugging Tools for Windows and enable Page Heap debugging as that may help localise the problem.",https://stackoverflow.com/questions/9610135/app-exe-has-triggered-a-break-point-followed-by-access-violation-writing-locati,2,0
Why does CQRS seem to prevent unique constraints on write side?,"Everywhere I see a post about ensuring uniqueness in a CQRS architecture the most obvious solution for me which is to add a unique index on the write side is never mentioned, without any explanation.
Instead I read the read model must be queried for that, and concurrency issues should be tackled by a saga compensating the action. Seems overly complex when you can just reject the command on unique index violation, so why is that?","Why does CQRS seem to prevent unique constraints on write side?

It doesn't
What it does do is recognize that that maintaining an invariant on a distributed set is a nightmare.

the most obvious solution for me which is to add a unique index on the write side is never mentioned, without any explanation

That's right.  If you don't have a distributed set -- if all of the elements of the set are stored together -- then maintaining the invariant is straight forward.
But what does it mean to have a unique index constraint that spans two databases?
To express the idea in more modern terms, the guiding assumption is that the business logic should be scale agnostic.  If two write models are really independent of one another, then we ought to be able to store them separately.
If there is a constraint that needs to be satisfied that depends on data from two different write models, then those write models aren't really independent.
Greg Young raised a really good question

What is the business impact of having a failure?

That's the sort of thing we are supposed to be thinking about in domain driven design, after all.

Why would event sourcing prevent me to put an index on unique fields??

Same answer really: it doesn't, so long as your unique constraint and your events are stored together.  
If you have an RDMBS with a table that represents the elements of your set, and tables that store your events, you can update the two tables together within a single transaction, and roll back the whole mess if your constraint is violated.
But take that same idea, and put the set in a different database than the events?  Now you have two distinct transactions to coordinate.  Good luck with that.",https://stackoverflow.com/questions/52072602/why-does-cqrs-seem-to-prevent-unique-constraints-on-write-side,1,0
Linking object files with struct accesses compiled on architectures with different alignment restrictions,"I was asked the following question and was stumped:
What happens when you try to link object files with struct accesses where the code has been compiled for the same architecture and instruction set, but with different alignment restrictions? The struct definition is the same.
I feel like the code couldn't be linked for several reasons:

Possible alignment restriction violation
One object files struct access would be at a different offset than the others","There are a few possibilities:

The harmless one:
The different alignment-restrictions are benign in this case, they lead to the same final layout. All is good that ends good Depending on your types, not very unlikely.
The compiler saves such info and the mismatch is detected on linking.
Unlikely, not sure of any compiler doing so.
The likely one.
The code will be linked without warning or error, but will misbehave more or less spectacularly, due to different parts of the program having different ideas about how types are layed out.",https://stackoverflow.com/questions/26539484/linking-object-files-with-struct-accesses-compiled-on-architectures-with-differe,2,0
Cross-thread violation on rb_gc(),"The codebase I'm working on was recently upgraded from Ruby 1.9.2 to Ruby 1.9.3 and from Rails 3.1 to Rails 3.2.2.  Since I'm using RVM I simply did rvm install 1.9.3 which I would have expected to be all that was necessary.
When I run
rails s

I get the error

[BUG] cross-thread violation on rb_gc()

I've found a number of links relating to this problem.  There is one on StackOverflow, but it doesn't really give an answer.  The most promising answer is on the RVM site:

In every case of this I have seen thus far it has always ended up
  being that a ruby gem/library with C extensions was compiled against a
  different ruby and/or architecture than the one that is trying to load
  it. Try uninstalling & reinstalling gems with C extensions that your
  application uses to hunt this buggar down.

That's fairly helpful, but my Ruby-fu is not strong enough to know which gems have C extensions and which ones I should try to re-install.  Quite a few of the other links on the topic seem to suggest that the json gem is at fault, so I tried following the suggested solution.
gem uninstall json 
gem install --platform=ruby

This didn't really change anything for me__ still get the exact same error when trying to start the Rails environment.
How do I track down this problem?
If it helps, here is the output from gem list:
actionmailer (3.2.2)
actionpack (3.2.2)
activemodel (3.2.2)
activerecord (3.2.2)
activeresource (3.2.2)
activesupport (3.2.2)
addressable (2.2.7)
akami (1.0.0)
arel (3.0.2)
bcrypt-ruby (3.0.1)
bson (1.6.1)
bson_ext (1.6.1)
builder (3.0.0)
bundler (1.1.3, 1.0.21)
capybara (1.1.2)
carmen (0.2.13)
childprocess (0.3.1)
ci_reporter (1.7.0)
coderay (1.0.5)
coffee-rails (3.2.2)
coffee-script (2.2.0)
coffee-script-source (1.2.0)
commonjs (0.2.5)
cucumber (1.1.9)
cucumber-rails (1.3.0)
database_cleaner (0.7.2)
devise (2.0.4)
diff-lcs (1.1.3)
ejs (1.0.0)
email_spec (1.2.1)
engineyard (1.4.28)
engineyard-serverside-adapter (1.6.3)
erubis (2.7.0)
escape (0.0.4)
execjs (1.3.0)
factory_girl (3.0.0)
factory_girl_rails (3.0.0)
faker (1.0.1)
fakeweb (1.3.0)
ffi (1.0.11)
gherkin (2.9.3)
gyoku (0.4.4)
haml (3.1.4)
haml-rails (0.3.4)
hash-deep-merge (0.1.1)
highline (1.6.11)
hike (1.2.1)
httpi (0.9.6)
i18n (0.6.0)
jasmine (1.1.2)
jasmine-core (1.1.0)
jasminerice (0.0.8)
journey (1.0.3)
jquery-rails (2.0.1)
json (1.6.6)
json_pure (1.6.6)
kaminari (0.13.0)
kgio (2.7.4)
launchy (2.0.5)
less (2.1.0)
less-rails (2.2.0)
libv8 (3.3.10.4 x86_64-darwin-11)
log4r (1.1.10)
mail (2.4.4)
metaclass (0.0.1)
method_source (0.7.1)
mime-types (1.18)
mocha (0.10.5)
mongo (1.6.1)
mongoid (2.4.7)
mongoid-rspec (1.4.4)
multi_json (1.2.0)
net-ssh (2.2.2)
newrelic_rpm (3.3.3)
nokogiri (1.5.2)
nori (1.1.0)
open4 (1.3.0)
orm_adapter (0.0.7)
polyglot (0.3.3)
pr_geohash (1.0.0)
pry (0.9.8.4)
pry-highlight (0.0.1)
pry_debug (0.0.1)
rack (1.4.1)
rack-cache (1.2)
rack-ssl (1.3.2)
rack-test (0.6.1)
rails (3.2.2)
rails-footnotes (3.7.6)
railties (3.2.2)
raindrops (0.8.0)
rake (0.9.2.2)
rdoc (3.12)
recursive-open-struct (0.2.1)
rest-client (1.6.7)
rpm_contrib (2.1.8)
rsolr (1.0.7)
rspec (2.9.0)
rspec-core (2.9.0)
rspec-expectations (2.9.0)
rspec-mocks (2.9.0)
rspec-rails (2.9.0)
rubyzip (0.9.6.1)
sass (3.1.15)
sass-rails (3.2.5)
savon (0.9.9)
selenium-webdriver (2.20.0)
settings-tree (0.2.1)
simplecov (0.6.1)
simplecov-html (0.5.3)
simplecov-rcov (0.2.3)
slop (2.4.4)
spine-rails (0.1.0)
spork (1.0.0rc2)
sprockets (2.1.2)
sunspot (1.3.1)
sunspot_mongoid (0.4.1)
sunspot_rails (1.3.1)
sunspot_solr (1.3.1)
term-ansicolor (1.0.7)
therubyracer (0.10.1)
thor (0.14.6)
tilt (1.3.3)
treetop (1.4.10)
twitter-bootstrap-rails (2.0.6)
tzinfo (0.3.32)
uglifier (1.2.4)
unicorn (4.2.1)
warden (1.1.1)
wasabi (2.1.0)
xpath (0.1.4)","Here are various approaches you can try.
cleanup
To clean up old versions of your gems:
gem cleanup --dryrun

json
To temporarily see if the json gem is the issue, switch from json (native) to json (pure ruby) and change your Gemfile:
gem install json_pure 

native gems
Your gem list has a few that pop out for me as native: 

bcrypt
bson
erubis
ffi  (enables lots of native connections)
gherkin
kgio
less
nokogiri
raindrops
therubyracer (many unpredictable issues IMHO)
unicorn

ffi
Your gem ffi is especially interesting -- do you happen to know what you're doing with it? 
The ffi enables Ruby code to call native code, for example if some part of your Ruby app needs to connect to native libraries. 
When you're diagnosing your issue, I'd look at this gem first.
makefiles
To find any of your gems that have Makefile files, which is a good indicator that they have native code:
find / | grep ""/ruby/gems/"" | grep Makefile

To find all your gems so you can delete them:
find / | grep ""/ruby/gems/""

nuke RVM
To nuke RVM or its pieces, you can use rvm uninstall, rvm implode, or this script which nukes RVM and finds any lingering pieces:
https://raw.github.com/SixArm/sixarm_unix_shell_scripts/master/rvm-uninstall-danger

try rbenv + bundler
I changed from using rvm to using rbenv + bundler and it's working great for me.
The rbenv tool is a direct competitor to rvm for managing Ruby versions:
https://github.com/sstephenson/rbenv
Bundler is a great way to manage gemsets and gem dependencies:
http://gembundler.com/
brew
If you're on a Mac and using MacPorts, to change to Homebrew:
http://mxcl.github.com/homebrew/",https://stackoverflow.com/questions/10175872/cross-thread-violation-on-rb-gc,5,0
How to solve TFS error TF246021 and SQL Server error 500200 while checking in?,"I'm trying to check in a file, and one file (one and only one file) has a problem while being checked in. This file has been deleted many times and undeleted (due to a conflict in architecture and design) and now TFS throws this error:

TF246021: An error occurred while processing your request. Technical
  information (for administrator): SQL Server Error: 500200

As we saw the log file of the Event Viewer, the problem is:

Violation of PRIMARY KEY constraint 'PK_tbl_PendingDelta'. Cannot
  insert duplicate key in object 'dbo.tbl_PendingDelta'.

However, there is no pending changes in dbo.tbl_PendingDelta. In fact, this table is completely empty. What's wrong here? How can I solve this problem?","This seems to be related to the workspace. I had this problem on one workspace but was able to  perform the source control operation that triggered the error (rollback a previous rollback) in a different workspace without any problems.
Recreating your workspace might be a solution.",https://stackoverflow.com/questions/14746267/how-to-solve-tfs-error-tf246021-and-sql-server-error-500200-while-checking-in,5,0
WSO2 API Manager authentication Violation of UNIQUE KEY constraint error,"We have implemented WSO2 API Manager (v1.10.0) in a distributed architecture as outlined in the online documentation here.
This consists of the following (on 5 separate servers):

Gateway (x2)
Publisher & Store (on a single server)
Key Manager (x2)

These are wired-up to the 3 normal API Manager databases (Registry, User Manager & API Manager), which are on a SQL Server 2014 instance.
We are using the Key Managers for the authentication (login, forgotten password, etc.) of the website users, as well as for authenticating API calls.
However, when trying to log in to the site I'm seeing the following (Violation of UNIQUE KEY constraint) error on the Key Manager:

TID: [-1] [] [2016-10-06 00:36:47,842] ERROR
  {org.wso2.carbon.identity.oauth2.dao.TokenPersistenceTask} -  Error
  occurred while persisting access token
  :c5a0a11e63388dCHANGEDea34b0533445
  {org.wso2.carbon.identity.oauth2.dao.TokenPersistenceTask}
  org.wso2.carbon.identity.oauth2.IdentityOAuth2Exception: Error when
  storing the access token for consumer key :
  fpA6AhOfbVCHANGEDgH0WzBDOga   at
  org.wso2.carbon.identity.oauth2.dao.TokenMgtDAO.storeAccessToken(TokenMgtDAO.java:246)
    at
  org.wso2.carbon.identity.oauth2.dao.TokenMgtDAO.persistAccessToken(TokenMgtDAO.java:284)
    at
  org.wso2.carbon.identity.oauth2.dao.TokenPersistenceTask.run(TokenPersistenceTask.java:52)
    at
  java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at
  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745) Caused by:
  com.microsoft.sqlserver.jdbc.SQLServerException: Violation of UNIQUE
  KEY constraint 'CON_APP_KEY'. Cannot insert duplicate key in object
  'dbo.IDN_OAUTH2_ACCESS_TOKEN'. The duplicate key value is (15,
  williams.j2@CHANGED.org.uk, -1234, ,
  APPLICATION_USER, 369db21a386ae4CHANGED0ff34d35708d, ACTIVE, NONE).   at
  com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:216)
    at
  com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServerStatement.java:1515)
    at
  com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatement(SQLServerPreparedStatement.java:404)
    at
  com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtExecCmd.doExecute(SQLServerPreparedStatement.java:350)
    at
  com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:5696)
    at
  com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:1715)
    at
  com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:180)
    at
  com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:155)
    at
  com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.execute(SQLServerPreparedStatement.java:332)
    at
  org.wso2.carbon.identity.oauth2.dao.TokenMgtDAO.storeAccessToken(TokenMgtDAO.java:224)
    ... 5 more

This is resulting in the following .NET error on the website:

I've tried Googling this, but cannot find an up-to-date answer.
I have not configured the Key Managers to have master and worked nodes (as outlined here) as the documentation seems to suggest that this isn't needed.
Any help would be much appreciated please!","After some debugging found the issue! Before we put this config,
<JDBCPersistenceManager>
    <SessionDataPersist>
        <PoolSize>0</PoolSize>
    </SessionDataPersist>
</JDBCPersistenceManager>

APIM can save more than one ACTIVE OAUTH token to the IDN_OAUTH2_ACCESS_TOKEN table for single token obtaining call. 
When the token validation endpoint queries the tokens, only the last one is returned (Time based sorting and Limits are used). When that one is expired token validation mark it as inactive. But the previous one is kept as it is. 
When the refresh token happens, it check whether the latest token is inactive. Since it is inactive, it issues a new token. But when the token endpoints tries to persist the token, there is another ACTIVE token. That caused this exception.
To sort this out we can run a update query on IDN_OAUTH2_ACCESS_TOKEN table to mark all the ACTIVE tokens to INACTIVE. 
update IDN_OAUTH2_ACCESS_TOKEN set TOKEN_STATE=""INACTIVE"" where TOKEN_STATE=""ACTIVE""

Then the old faulty tokens will be removed and server will start working fine!",https://stackoverflow.com/questions/39885646/wso2-api-manager-authentication-violation-of-unique-key-constraint-error,3,0
PyOpenGL Access Violation Reading when calling glutWireSphere(),"I am building an application using PyOpenGL and PyQt5 on Python3.5.2
The purpose of the application is to setup a window with QGLWidget class and draw a sphere using glutWireSphere function. The code works fine on my Ubuntu Linux 16.04 LTS laptop but it crashes on my Windows 7 desktop with OSError: exception: access violation reading 0x00000000000000C1 in the line of glutWireSphere. If I comment out this line, the program executes normally. I tried a different code example (sample below) that demonstrates how to draw a sphere with glut and it did not crushed, but it manages the windowing with glut.
My Code:
from OpenGL.GLUT import *
from OpenGL.GLU import *
from OpenGL.GL import *
from PyQt5.QtOpenGL import *
from PyQt5 import QtGui
from PyQt5 import QtWidgets
from PyQt5 import QtCore


class MyWidget(QGLWidget):

    def __init__(self, parent = None):
        super(MyWidget, self).__init__(parent)

        self.cameraDistanceZ = 10

    def paintGL(self):
        glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)
        glColor3f(0.33, 0.33, 0.33)
        glutSolidSphere(1, 20, 20)


    def resizeGL(self, w, h):
        glViewport(0, 0, w, h)
        self.setOrthoProjection()
        self.screenCenterX = w/2
        self.screenCenterY = h/2

    def setOrthoProjection(self):
        glMatrixMode(GL_PROJECTION)
        glLoadIdentity()
        w = self.width()
        h = self.height()
        glOrtho(-10, 10, -10, 10, -5.0, 15.0)
        gluLookAt(0,0,self.cameraDistanceZ, 0,0, -10, 0,1,0)

    def initializeGL(self):
        print(""Start init process"")
        glutInit(sys.argv)
        glClearColor(0.0, 0.0, 0.1, 0.0)
        glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)
        glClearDepth(1.0)

        self.updateGL()


def main():
    app = QtWidgets.QApplication([""To parathyro""])
    widget = MyWidget()
    widget.show()
    sys.exit(app.exec_())
if __name__ == '__main__':
    main()

Working example:
from OpenGL.GLUT import *
from OpenGL.GLU import *
from OpenGL.GL import *
import sys


def main():
    glutInit(sys.argv)
    glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGB | GLUT_DEPTH)
    glutInitWindowSize(400,400)
    glutCreateWindow(b""myWindow"")

    glClearColor(0.,0.,0.,1.)
    glShadeModel(GL_SMOOTH)
    glEnable(GL_CULL_FACE)
    glEnable(GL_DEPTH_TEST)
    glEnable(GL_LIGHTING)
    lightZeroPosition = [10.,4.,10.,1.]
    lightZeroColor = [0.8,1.0,0.8,1.0] #green tinged
    glLightfv(GL_LIGHT0, GL_POSITION, lightZeroPosition)
    glLightfv(GL_LIGHT0, GL_DIFFUSE, lightZeroColor)
    glLightf(GL_LIGHT0, GL_CONSTANT_ATTENUATION, 0.1)
    glLightf(GL_LIGHT0, GL_LINEAR_ATTENUATION, 0.05)
    glEnable(GL_LIGHT0)
    glutDisplayFunc(display)
    glMatrixMode(GL_PROJECTION)
    gluPerspective(40.,1.,1.,40.)
    glMatrixMode(GL_MODELVIEW)
    gluLookAt(0,0,10,
              0,0,0,
              0,1,0)
    glPushMatrix()
    glutMainLoop()
    return

def display():
    glClear(GL_COLOR_BUFFER_BIT|GL_DEPTH_BUFFER_BIT)
    glPushMatrix()
    color = [1.0,0.,0.,1.]
    glMaterialfv(GL_FRONT,GL_DIFFUSE,color)
    glutWireSphere(1,20,20)
    glPopMatrix()
    glutSwapBuffers()
    return

if __name__ == '__main__': main()

If i try to draw something else, for example glutSolidSphere I get the same error but on a different address. All access violation errors I have found online are application specific and they are mostly related to wrong input type in some function. There is not much room for such a mistake in my application since functions in my code do not take many arguments, or there is something that I have not noticed yet.
For my Windows 7 desktop system I downloaded Python 3.5.2 from the official website, I installed PyQt5 using pip and PyOpenGL through the Unofficial Windows Binaries for Python Extension Packages according to these instructions
I thought it was something related to problems in the freeglut library but it works on a different example. I guess it has something to do with how I initialize freeglut in my application, or it is related to PyQT5
Question
How can I track the problem and solve it without moving to different architecture for my application?","It seems the crash occurs because glutWireSphere requires you to call first glutCreateWindow, check this little snippet for example:
from OpenGL.GLUT import *
from OpenGL.GLU import *
from OpenGL.GL import *

from PyQt5.Qt import *  # noqa


class MyWidget(QGLWidget):

    def __init__(self, parent=None):
        super().__init__(parent)

    def paintGL(self):
        glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)

        glPushMatrix()
        color = [1.0, 0., 0., 1.]
        glMaterialfv(GL_FRONT, GL_DIFFUSE, color)
        glutWireSphere(1, 20, 20)
        glPopMatrix()
        glutSwapBuffers()

    def glutInitialization(self):
        glutInit(sys.argv)
        glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGB | GLUT_DEPTH)
        glutInitWindowSize(400, 400)
        glutCreateWindow(b""Glut Window"")

    def initializeGL(self):
        glClearColor(0., 0., 0., 1.)
        glShadeModel(GL_SMOOTH)
        glEnable(GL_CULL_FACE)
        glEnable(GL_DEPTH_TEST)
        glEnable(GL_LIGHTING)
        lightZeroPosition = [10., 4., 10., 1.]
        lightZeroColor = [0.8, 1.0, 0.8, 1.0]  # green tinged
        glLightfv(GL_LIGHT0, GL_POSITION, lightZeroPosition)
        glLightfv(GL_LIGHT0, GL_DIFFUSE, lightZeroColor)
        glLightf(GL_LIGHT0, GL_CONSTANT_ATTENUATION, 0.1)
        glLightf(GL_LIGHT0, GL_LINEAR_ATTENUATION, 0.05)
        glEnable(GL_LIGHT0)
        glMatrixMode(GL_PROJECTION)
        gluPerspective(40., 1., 1., 40.)
        glMatrixMode(GL_MODELVIEW)
        gluLookAt(0, 0, 10,
                  0, 0, 0,
                  0, 1, 0)
        glPushMatrix()

        self.glutInitialization()


def main():
    app = QApplication([""To parathyro""])
    widget = MyWidget()
    widget.setWindowTitle(""Pyqt Widget"")
    widget.show()
    sys.exit(app.exec_())

if __name__ == '__main__':
    main()

You can see how the sphere will appear in the pyqt window (active gl context). Then, comment glutCreateWindow(b""Glut Window"") and run it again, so you'll see the seg fault crash will appear back.
My suggestion? I wouldn't rely on glut geometric function when not using Glut as the opengl window management system. 
Btw, generating a sphere isn't hard at all and is worth you give it a shot :)",https://stackoverflow.com/questions/48892375/pyopengl-access-violation-reading-when-calling-glutwiresphere,1,0
should the Swift Model object make calls to API?,"I am wondering if I am following the MVC architectural pattern, in my Model class can I make a call to the API? 
i.e.
class DataModel {
    var data = ""Some data that's a string""
    func getAPIData(){
       self.data = self.NetworkFunctionsClass.MakeAPICallFunction()
    }
}

I am not directly making a function in my Model class
Is this allowed? Or am I violating the Model rule by introducing some actions rather than storing data?","in my Model class can I make a call to the API?

There is nothing limits you from being able to do so, but usually that's not expected. From the traditional perspective for the standard MVC, the Controller layer is the responsible for such a task.
However, for the purpose of avoiding the ""Massive View Controller"" problem, you could apply the ""MVC-N"" approach. Actually, it nothing but a standard MVC + Networking client! Which leads to:

Avoiding duplicating the networking logic in each view controller.
Making the view controllers to be less ""massive"".

So, it goes as:
The Networking handles -obviously- the API requests, deserializing response (as models) and pass it to the view controller; Therefore the controller should not know about the models. The communication between the Networking and the Controllers layer is -usually- achieved by closures, and sometimes you might need to follow the approach of delegates.
Back to your case, you will be refine the code as (pseudo-code):
class DataModel {
    var data = ""Some data that's a string""
}

// separating the networking logic in another class:
class Networking {
    func getAPIData(url: URL, parameters: [String: Any], response: @escaping (_ dataModel: DataModel) -> Void) {
        // ...

        // here, you should pass the returned data model:
        response(DataModel())
    }
}

And, in the view controller:
class ViewController: UIViewController {
    override func viewDidLoad() {
        super.viewDidLoad()

        // somewhere in the view controller, you could pass it as:
        Networking().getAPIData(url: ..., parameters: ...) { data in
            // here, you can access `data`...
        }
    }
}

Certainly, the logic is also applicable for any specified tasks functionality -and not only networking-, such as fetching the device photos (accessing system resources) or writing and reading data from/into the disk.",https://stackoverflow.com/questions/51692258/should-the-swift-model-object-make-calls-to-api,2,0
DataGridView - Concurrency Issue When Updating or Deleting New Records,"I have a problem with a DataGridView based application.  After adding records to the grid, if I subsequently attempt to update or delete the new records I get the following respective errors.
Concurrency violation: the UpdateCommand affected 0 of the expected 1 records.
Concurrency violation: the DeleteCommand affected 0 of the expected 1 records.
A save of the dataset is enforced in the RowValidated event.
Private Sub uxGrid_RowValidated(sender As Object, e As DataGridViewCellEventArgs) Handles uxGrid.RowValidated
    If IsDGVRowDirty(sender, e.RowIndex) Then
        Save()
    End If
End Sub

Private Sub Save()
    Const procName As String = ""Save""
    Try
        _Logger.SendLog(Me.Name & ""."" & procName & "" - Saving alarm definition data."", NLog.LogLevel.Trace)
        _myAlarmDefinitionMngr.Save(_myDataSet)
        _Logger.SendLog(Me.Name & ""."" & procName & "" - Alarm definition data has been saved."", NLog.LogLevel.Trace)
    Catch ex As Exception
        MsgBox(""There was a problem whilst saving your changes.  Please reload the form and try again.  "" & vbCrLf & vbCrLf & ex.Message, MsgBoxStyle.Critical)
        _Logger.SendLog(ex.Message & "".  Thrown in module "" & Me.Name.ToString & ""."" & procName, NLog.LogLevel.Error, ex)
    Finally
    End Try
End Sub

The client app sits on top of a relatively simple 3-tier architectural design talking to an Oracle 11 back-end, in this case comprising the elements AlarmDefinitionManager.vb, AlarmDefinitionDB.vb, AlarmDefinition.vb.
Public Function Save(ByVal myDataSet As DataSet) As Integer
    Dim myOda As OracleDataAdapter
    Dim myConnection As New OracleConnection
    Dim myCommand As OracleCommand = Nothing
    Dim myDataAdapter As OracleDataAdapter
    Dim myBuilder As OracleCommandBuilder
    Dim sqlStatement As String

    myConnection = New OracleConnection
    myConnection.ConnectionString = _connectStr
    sqlStatement = ""SELECT ID, LEGENDID, STATUSID, STATUSTYPEID, DIGITALSET FROM P_TBL_ALARMDEF""
    If myDataSet.HasChanges Then
        Try
            myOda = New OracleDataAdapter(sqlStatement, myConnection)
            myBuilder = New OracleCommandBuilder(myOda)
            myOda.SelectCommand = New OracleCommand(sqlStatement, myConnection)
            myOda.Update(myDataSet, ""AlarmDefinition"")
            myDataSet.AcceptChanges()
            myConnection.Close()
        Catch ex As Exception
            Throw
        Finally
            myCommand = Nothing
            myDataAdapter = Nothing
            myBuilder = Nothing
           CType(myConnection, IDisposable).Dispose()
        End Try
    End If
End Function

My understanding of the situation is that I need to refresh the datasource in the grid, such that it reflects the changes made.  In order to achieve this I wish to refresh the grid by hooking the refresh process into an event in the datagridview.  However none of the events which I have tried seem to work.  With each event tried thus far, I have received errors to the effect that a refresh of the datasource is not permitted from within the event.  On reflection this seems logical.  Is there any event associated with the datagridview which I can use to force a refresh of the datasource?","You shouldn't try to save your dataset while validating the row.
 do your changes to your dataset then set your datasource to null and reattach your datasource back to the grid to force a refresh.
I personally don't like making changes to the datagrid. I prefer to edit the row outside the datagrid and send an UPDATE cmd to SQL then refresh my grid. I get more control on the changes made to the grid.",https://stackoverflow.com/questions/46281353/datagridview-concurrency-issue-when-updating-or-deleting-new-records,2,0
Entity Framework Navigation Property without Foreign Key Constraint,"I have a code first EF Model with a simple one(PartDetails)-to-many(Parts) relationship.
While the system is offline the lookup (PartDetails) table is purged and re-populated by external job, violating the constraint.
modelBuilder.Entity<Part>().HasOptional(e => e.PartDetails)
                           .WithMany()
                           .HasForeignKey(k => k.PartDetailsId);

I have tried .Map and other variants but all create the DB constraint.
How can I prevent EF from creating the DB FK constraint using the fluent API?
I understand this may not be the best architectural approach but I'm stuck with it.  If I can't find a solution to do this with the Fluent API I will manually drop the constraint in a migration script or drop and re-add the constraint during the purge job.","You could manually drop the constraint in a migration script using Sql(""DROP CONSTRAINT..."").
But a better architectural approach would be to drop and re-add the constraint during the purge job. 
It sounds like the purge job is where the problem really lies. Constraints are very important.",https://stackoverflow.com/questions/25885639/entity-framework-navigation-property-without-foreign-key-constraint,1,0
Cancelling a thread in pthread_cond_wait yields to access violation under MinGW,"My program dies with an access violation in Windows (Windows 7 - 32 bits). It is C code compiled with gcc 4.8.1 under MinGW. It uses pthreads-w32 2.9.1.
There are several threads working concurrently with no other apparent issues.
It can run well for days or fail in a couple of hours.
Code can also be compiled in several linux architectures, but I am not having problems there. It is also very difficult to run the program under a debugger.
Following is the function where the crash happens.
static void *timeout_lector(void *indice)
{
    int e;
    pthread_mutex_t fm;
    pthread_cond_t fc;
    struct timespec ts;
    struct timeval tv;
    struct equipo *eq;

    eq = &estacion.equipos[*((int *)indice)];

    if (pthread_mutex_init(&fm, NULL) || pthread_cond_init(&fc, NULL)) {
        LOG_PRINT(""Error creating mutex or cond in timeout_lector.\n"");
        exit(1);
    }
    pthread_mutex_lock(&fm);
    gettimeofday(&tv, NULL);
    ts.tv_sec  = tv.tv_sec;
    ts.tv_nsec = tv.tv_usec * 1000;
    siguiente_tmseg(&ts, 150);  /* Increments ts by 150 ms */
    pthread_cleanup_push(thread_cleanup_fc, (void *)(&fc));
    pthread_cleanup_push(thread_cleanup_fm, (void *)(&fm));
    if ((e = pthread_cond_timedwait(&fc, &fm, &ts)) != ETIMEDOUT) {
        LOG_PRINT(""Error waiting in timeout_lector: %d.\n"", e);
        exit(1);
    }
    pthread_mutex_lock(&eq->mutexto);
    eq->to = 1;
    pthread_mutex_unlock(&eq->mutexto);
    e = pthread_cond_wait(&fc, &fm);  /* wait until we are cancelled */
    LOG_PRINT(""Error in timeout_lector: %d.\n"", e);
    exit(1);
    /* Next lines are never executed and just for correct syntax */
    pthread_cleanup_pop(1);
    pthread_cleanup_pop(1);
    return(NULL);
}

These are the cleanup functions:
void thread_cleanup_fc(void *fc)
{
    pthread_cond_destroy(fc);
}

void thread_cleanup_fm(void *fm)
{
    pthread_mutex_unlock(fm);
    pthread_mutex_destroy(fm);
}

I have been running it with Dr.MinGW 0.7.3 and here is the report:
estacion.exe caused an Access Violation at location 62489D38 in module pthreadGC2.dll Writing to location 0101FCFC.

Registers:
eax=00000000 ebx=0444febc ecx=ffffffff edx=0101fcfc esi=0444fee0 edi=0444fcf0
eip=62489d38 esp=0444fcd0 ebp=0444fd18 iopl=0         nv up ei pl zr na po nc
cs=001b  ss=0023  ds=0023  es=0023  fs=003b  gs=0000             efl=00010246

AddrPC   Params
62489D38 0444FEE0 00000000 FFFFFFFF  pthreadGC2.dll!pthread_cond_destroy
004194E0 0444FEE0 0444FEBC 0444FD68  estacion.exe!thread_cleanup_fc  [C:/codigo/CeltaDAS/estacion/tiempo.c @ 223]
6248ABB5 00000684 FFFFFFFF 0444FE08  pthreadGC2.dll!ptw32_pop_cleanup.constprop.3
6248BAAF 00000000 00000000 03C63EE0  pthreadGC2.dll!sem_timedwait
6248512D 003E1138 0444FEB0 767DA53A  pthreadGC2.dll!pthread_getspecific
62485D22 003E1138 00000078 00000000  pthreadGC2.dll!ptw32_push_cleanup
62485D22 0444FEE0 0444FEE4 0444FED0  pthreadGC2.dll!ptw32_push_cleanup
0040C0D1 014908EC 014AAD70 00000000  estacion.exe!timeout_lector  [C:/codigo/CeltaDAS/estacion/equipos.c @ 214]
62485BD3 014B1190 0805AA4A 00000000  pthreadGC2.dll!ptw32_threadStart@4
767E1287 0444FF94 773DEE1C 03CF0048  msvcrt.dll!itow_s
767E1328 03CF0048 0444FFD4 777037EB  msvcrt.dll!endthreadex
773DEE1C 03CF0048 319EB544 00000000  kernel32.dll!BaseThreadInitThunk
777037EB 767E12E5 03CF0048 00000000  ntdll.dll!RtlInitializeExceptionChain
777037BE 767E12E5 03CF0048 00000000  ntdll.dll!RtlInitializeExceptionChain

Windows 6.1.7601
DrMingw 0.7.3

Dr.MinGW reports in the stack trace passing through line 214 in timeout_lector. It corresponds to the pthread_cond_wait line in the code.
This threads are created continuosly and several in parallel. They wait for some time while they can be cancelled, if time passes they change a variable, and then wait until they are cancelled, what happens almost inmediately by another thread. There are no many cancellable functions that I can use for waiting and that are portable in different systems, so I chose pthread_cond_timedwait for it. But the problem seems to be in the second and indefinite wait.
I have search for similar problems, but nothing appears to be the same problem.
More similar are:
pthread_cond_wait: random segmentation fault
Cancelling pthread_cond_wait() hangs with PRIO_INHERIT mutex
I would really appreciate it if someone could help me with this.","First destroy mutex and condition on which thread is waiting and then do thread cancel. If you destroy first thread then this kind of problem comes in windows.
Have look on this.",https://stackoverflow.com/questions/30898193/cancelling-a-thread-in-pthread-cond-wait-yields-to-access-violation-under-mingw,1,0
How to trap stack overflow in a Windows x64 C++ application,"I am trying to compile an application to x64 platform architecture in Windows. A couple of threads, handling the parsing of a scripting language, uses this code recommended by Microsoft to trap stack overflows and avoid access violation exceptions:
__try
{
    DoSomethingThatMightUseALotOfStackMemory();
}
__except(EXCEPTION_EXECUTE_HANDLER)
{
    LPBYTE lpPage;
    static SYSTEM_INFO si;
    static MEMORY_BASIC_INFORMATION mi;
    static DWORD dwOldProtect;

    // Get page size of system
    GetSystemInfo(&si);

    // Find SP address
    _asm mov lpPage, esp;

    // Get allocation base of stack
    VirtualQuery(lpPage, &mi, sizeof(mi));

    // Go to page beyond current page
    lpPage = (LPBYTE)(mi.BaseAddress)-si.dwPageSize;

    // Free portion of stack just abandoned
    if (!VirtualFree(mi.AllocationBase,
                    (LPBYTE)lpPage - (LPBYTE)mi.AllocationBase,
                     MEM_DECOMMIT))
    {
        exit(1);
    }

    // Reintroduce the guard page
    if (!VirtualProtect(lpPage, si.dwPageSize, 
                        PAGE_GUARD | PAGE_READWRITE, 
                        &dwOldProtect))
    {
        exit(1);
    }
    Sleep(2000);
}

Unfortunately it uses one line of inline assembler to get the stack pointer. Visual Studio does not support inline assembly for x64 mode and I can't find a compiler intrinsic for getting the stack pointer neither.
Is it possible to do this in a x64 friendly manner?","As pointed out in a comment to the question, the whole ""hack"" above can be replaced by the _resetstkoflw function. This works fine in both x86 and x64 mode.
The code snippet above then becomes:
// Filter for the stack overflow exception. This function traps
// the stack overflow exception, but passes all other exceptions through. 
int stack_overflow_exception_filter(int exception_code)
{
    if (exception_code == EXCEPTION_STACK_OVERFLOW)
    {
        // Do not call _resetstkoflw here, because at this point
        // the stack is not yet unwound. Instead, signal that the
        // handler (the __except block) is to be executed.
        return EXCEPTION_EXECUTE_HANDLER;
    }
    else
        return EXCEPTION_CONTINUE_SEARCH;
}

void example()
{
    int result = 0;
    __try
    {
        DoSomethingThatMightUseALotOfStackMemory();
    }
    __except(stack_overflow_exception_filter(GetExceptionCode()))
    {
        // Here, it is safe to reset the stack.
        result = _resetstkoflw();
    }

    // Terminate if _resetstkoflw failed (returned 0)
    if (!result)
        return 3;

    return 0;
}",https://stackoverflow.com/questions/29349307/how-to-trap-stack-overflow-in-a-windows-x64-c-application,1,0
The correct way to obtain a ViewModel instance outside of an Activity or a Fragment,"I'm building a location app where I display background locations from a Room database in my MainActivity. I can get a ViewModel by calling
locationViewModel = ViewModelProviders.of(this).get(LocationViewModel.class);
locationViewModel.getLocations().observe(this, this);

Periodic background locations should be saved to the Room database when I receive location updates via a BroadCastReceiver. They should be saved by calling locationViewModel.getLocations().setValue()
public class LocationUpdatesBroadcastReceiver extends BroadcastReceiver {

    static final String ACTION_PROCESS_UPDATES =
            ""com.google.android.gms.location.sample.backgroundlocationupdates.action"" +
                    "".PROCESS_UPDATES"";

    @Override
    public void onReceive(Context context, Intent intent) {
        if (intent != null) {
            final String action = intent.getAction();
            if (ACTION_PROCESS_UPDATES.equals(action)) {
                LocationResult result = LocationResult.extractResult(intent);
                if (result != null) {
                    List<Location> locations = result.getLocations();
                    List<SavedLocation> locationsToSave = covertToSavedLocations(locations)
                    //Need an instance of LocationViewModel to call locationViewModel.getLocations().setValue(locationsToSave)
                }
            }
        }
    }
}

Question is how should I get the LocationViewModel instance in a non-activity class like this BroadcastReceiver? Is it correct to call locationViewModel = ViewModelProviders.of(context).get(LocationViewModel.class) where context is the context that I receive from onReceive (Context context, Intent intent) of the BroadcastReceiver?
After getting the ViewModel, do I need to use LiveData.observeForever and LiveData.removeObserver since the BroadcastReceiver is not a LifecycleOwner?","Question is how should I get the LocationViewModel instance in a
  non-activity class like this BroadcastReceiver?

You shouldn't do that. Its bad design practice.

Is it correct to call locationViewModel =
  ViewModelProviders.of(context).get(LocationViewModel.class) where
  context is the context that I receive from onReceive (Context context,
  Intent intent) of the BroadcastReceiver?

No. It won't help
You can achieve your desired outcome as follows:
Separate your Room DB operation from ViewModel in a separate singleton class. Use it in ViewModel and any other place required. When Broadcast is received, write data to DB through this singleton class rather than ViewModel. 
If you are observing for the LiveData in your Fragment, then it will update your views too.",https://stackoverflow.com/questions/51007271/the-correct-way-to-obtain-a-viewmodel-instance-outside-of-an-activity-or-a-fragm/51007301,2,0
What class does this variable belong to?,"class Program
{
    class Mammal
    {

    }

    class Horse : Mammal
    {

    }

    static void Main()
    {
        Horse myHorse = new Horse();
        Mammal myMammal = myHorse;
        Horse myHorseAgain = myMammal as Horse;
    }
}

Will myHorseAgain have methods defined in Horse class?","Yes, it will. Although it is going through an upcast, you will find that for the upcasted (Mammal) instance, the following condition still holds:
(myMammal is Horse) == true

But actually doing this is an anti-pattern. Go for an architecture using interfaces instead.",https://stackoverflow.com/questions/43527013/what-class-does-this-variable-belong-to/43527214,2,0
"When using a singleton design pattern, which construction is preferred and why?","Aside from ""what is so bad about singletons"" :-), I have an ASP.NET web application that utilises singletons at the business logic layer, thus:
public class MyBusinessService
{
    private static MyBusinessService mInstance = null;

    public static MyBusinessService Instance
    {
        get { return mInstance; }
    }

    static MyBusinessService()
    {
        mInstance = new MyBusinessService();
    }
}

We use them primarily for a dependency in a Model View Presenter architecture.
They can also be used across business logic classes in one of two ways. Firstly in the following manner:
var myService = new MyBusinessService();
myService.DoSomething();
myService.DoSomethingElse();

Or, it can be used in the following manner:
MyBusinessService.Instance.DoSomething();
MyBusinessService.Instance.DoSomethingElse();

Which construct is preferred and why? I'm not interested in whether the singleton pattern itself is good or bad.
Update:
Ok, this question seems to be quite popular. I guess it is a quasi-singleton. Worst of both worlds! I'm not really interested in refactoring the pattern / anti-pattern / code hell. I'm more interested in understanding the effects of both usages described.
Our view (ASP.NET page) looks like this:
var presenter = new SomeViewPresenter(this, MyBusinessService.Instance);

but could alternatively be implemented as:
var presenter = new SomeViewPresenter(this, new MyBusinessService());

I prefer the former in this case. N.B. The use of the word singleton and the incorrect usage above is understood, but as the code stands, what is the outcome of the two original options?","The latter is preferred because the former isn't behaving like a singleton - you are instantiating a new instance without any guards for stopping more than one instance existing.
The latter is the code you'd likely end up with if you put these guards in place.  You don't need to use the property all the time too:
var service = MyBusinessService.Instance;
service.This();
service.That();

I also look at statics in ASP.NET with some skepticism and this is from a WinForms developer :-)",https://stackoverflow.com/questions/7904465/when-using-a-singleton-design-pattern-which-construction-is-preferred-and-why,6,0
What is so bad about singletons? [closed],"Closed. This question is opinion-based. It is not currently accepting answers.
                            
                        










Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.
                        
Closed 4 years ago.



The singleton pattern is a fully paid up member of the GoF's patterns book, but it lately seems rather orphaned by the developer world. I still use quite a lot of singletons, especially for factory classes, and while you have to be a bit careful about multithreading issues (like any class actually), I fail to see why they are so awful.
Stack Overflow especially seems to assume that everyone agrees that Singletons are evil. Why?
Please support your answers with ""facts, references, or specific expertise""","Paraphrased from Brian Button:

They are generally used as a global instance, why is that so bad? Because you hide the dependencies of your application in your code, instead of exposing them through the interfaces. Making something global to avoid passing it around is a code smell.
They violate the single responsibility principle: by virtue of the fact that they control their own creation and lifecycle.
They inherently cause code to be tightly coupled. This makes faking them out under test rather difficult in many cases.
They carry state around for the lifetime of the application. Another hit to testing since you can end up with a situation where tests need to be ordered which is a big no no for unit tests. Why? Because each unit test should be independent from the other.",https://stackoverflow.com/questions/137975/what-is-so-bad-about-singletons,36,0
Best practice for adding custom logic in Lumen/Laravel controller,"I'm using Lumen (Laravel) to create an API for an online activity/campaign application which handles things like on-site registrations and gift redemptions for various events. Occasionally, there are certain events with very specific functionality required which need their own custom logic. I'm wondering how to best handle this custom code from an architecture/best practices standpoint.
Here's what I have: I have a route which calls a CustomCampaignController like so:
$router->group([
    'prefix' => 'v1'
], function () use ($router) {
    // ..... other routes for standard activities
    $router->post('customCampaigns', 'CustomCampaignController@runController');
});

Under App\Http\Controllers I've opened a directory to store classes for all custom activities. The customCampaigns route takes an activityId parameter whose value matches one of the activity classes. For example if the client posts activityId=""MyCustomActivity"" to customCampaigns, I would instantiate the following class: App\Http\Controllers\Custom\MyExampleActivity.
// app/Http/Controllers/CampaignController.php
public function runController(Request $request) {
    $className = 'App\\Http\\Controllers\\Custom\\' . $request->input('activityId');
    $customController = new $className;
    return $customController->run();
}

The custom controller would then do its thing and return the response
// app/Http/Controllers/Custom/MyCustomActivity.php
namespace App\Http\Controllers\Custom;

class MyCustomActivity
{
    public function __construct()
    {
        //
    }

    public function run()
    {
        // Custom logic here
        return response('Response');
    }
}

Is this a good approach or could it be considered an anti-pattern? Please let me know if there is another pattern for this type of problem.","I would prefer put the custom activity as the part of url. So, you will have something like this
$router->group([
  'namespace' => 'App\Http\Controllers\Custom',
  'prefix' => 'v1/customCampaigns'
], function () use ($router) {
  $router->post('myCustomActivity', 'MyCustomActivityController@methodName');
});

Using this format, you can map the endpoint directly into a specific controller.",https://stackoverflow.com/questions/47547566/best-practice-for-adding-custom-logic-in-lumen-laravel-controller,1,0
What's the difference between design patterns and architectural patterns?,"When we read about design patterns on the internet we note that there are 3 categories:

Creational
Structural
Behavioral

But when we create the architecture of a software, then we think about MVP, MVC or MVVM.
For example, among creational patterns I found the singleton pattern, but I have also used singleton in my MPV. 
So my question is: Is a design pattern a over all structure of a product? 

If yes, then how singleton can be a design pattern? Because I can use it anywhere in my application. Basically, it is restricted only to create one instance at a time in memory, but doesn't this concept define how software is designed?
If not, then where are MVP, MVC and MVVM in the three categories of patterns? And what is the difference between design and architecture of software?","It requires a detailed explanation but I will try to sketch the differences to best of my knowledge.
Patterns are distilled commonality that you find in programs. It allows us to deconstruct a large complex structure and build using simple parts. It provides a general solution for a class of problems. 
A large complex software goes through a series of deconstruction at different levels. At large level, architectural patterns are the tools. At smaller level, design patterns are the tools and at implementation level, programming paradigms are the tools.
A pattern can occur at very different levels. See Fractals. Quick sort, Merge sort are all algorithmic patterns for organizing a group of elements in a order.
For a most simplistic view:
 Programming paradigms   Specific to programming language
 ......................
 Design patterns         Solves reoccurring problems in software construction
 ......................
 Architectural patterns  Fundamental structural organization for software systems
 ......................

Idioms are paradigm-specific and language-specific programming techniques that fill in low-level details.
Design patterns are usually associated with code level commonalities. It provides various schemes for refining and building smaller subsystems. It is usually influenced by programming language. Some patterns pale into insignificance due to language paradigms.
Design patterns are medium-scale tactics that flesh out some of the structure and behavior of entities and their relationships.
While architectural patterns are seen as commonality at higher level than design patterns. 
Architectural patterns are high-level strategies that concerns large-scale components, the global properties and mechanisms of a system.
How are patterns obtained?
Through :

re-use, 
classification 
and finally abstraction to distill the commonality. 

If you have followed the thoughts laid above. You will see that Singleton is a ""design pattern"" while MVC is one of the ""architectural"" pattern to deal with separation of concerns.
Try reading on:

http://en.wikipedia.org/wiki/Architectural_pattern_(computer_science)
http://en.wikipedia.org/wiki/Design_pattern
http://en.wikipedia.org/wiki/Anti-pattern",https://stackoverflow.com/questions/4243187/whats-the-difference-between-design-patterns-and-architectural-patterns/4243407,5,0
What is the opposite of domain driven design?,"I'm looking for a term that describes a design pattern (or anti-pattern) that describes when a system is organized by generalizations rather than the terms of the business.
For example consider the domain driven example of:
Customer.Load
Account.Load
Orders.Load

vs:
Load.LoadCustomers
Load.LoadAccounts
Load.LoadOrders

Another example could be seen here in mvc:
ASP.NET MVC - putting controller & associated views in the same folder?
While more about organization vs design, the comment from pettys IMO is correct:

Realize that there are two dimensions of the system at odds here, the
  architectural and the business. The original poster wants to keep all
  parts that deal with a certain business concern in one place. The
  default ASP.NET MVC layout spreads a single business concern out quite
  a bit so that architecturally-similar classes can stay together.
  Between these two choices I'd rather have things grouped by business
  concern than by architectural concern

There is only one way to physically organize the project.  The OP is asking to organize by domain and pettys describes the default organization as ""architecturally"" organized.  But it is this ""architectural"" organization that I'm better trying to classify.
It seems to me that this is aspect-oriented design, ie design favoring a classification with respect to a general process instead of specific terms of the business.  I do not intend to confuse this with aspect-oriented programming which can be complementary to DDD, but it IS confusing which is why I'm wondering if there is a better name.","There are numerous generalizations around which a system can be organized. As far as I know, there were a few in vogue circa 2003 when DDD was published.

Table-driven or data-driven design, which typically meant deriving your implementation from a specific set of relational data.
Model-driven design, which typically meant deriving your implementation from UML diagrams.
Configuration-driven design, which typically meant converting as much of your implementation as possible into XML files.

I'm sure there were more, and you could say that all of them focus more on the system's architecture than its domain. In retrospect you could consider some or all of them to be anti-patterns. In the words of Neal Ford,

Today's best practice is tomorrow's anti-pattern.

reference: https://visualstudiomagazine.com/articles/2015/07/01/domain-driven-design.aspx",https://stackoverflow.com/questions/43623015/what-is-the-opposite-of-domain-driven-design,2,0
Instantiable Angular Service - What Other Way Would I Rewrite This?,"For a project I am working on, one of the things I've written is an instantiable service for an Angular UI Bootstrap Calendar control.
You can see the plunker for it in action here.
The code question I have is more of an architectural and best-practices question.  Specifically, I think I've written an Angular anti-pattern.
Services - like the calendarSvc - are singletons, yet I am explicitly circumventing this by making the factory return a constructor function.
That being said, there is a concrete business need for 1-n calendars to exist on a single page.  This code is an effective refactor of the code needed to manage a single calendar, so it definitely helps the code to be more DRY.
Question: What are some effective alternatives to this instantiable service that still let me specify instances of reusable Calendar objects as needed, but are done without circumventing the Angular way of managing the code?","What you did is not a antipattern, but indeed best practice! 
See this style guide page for example: http://github.com/mgechev/angularjs-style-guide#services. 
You did not circumvent the Angular way. The idea of the factory is the possibility to return constructor functions. The other alternative to creating services is the service-method of module which would automatically instantiate your service with the new keyword.
Take also a look at this other StackOverflow answer as reference: https://stackoverflow.com/a/20110304/669561",https://stackoverflow.com/questions/26509352/instantiable-angular-service-what-other-way-would-i-rewrite-this,1,0
Java: Persistence Class with toDTO method categorized as adapter pattern?,"If i have an Persistence class which implements a method ""toDTO"" and this method return a POJO representing the object, will this be categorized as an adapter pattern? 
Please note, that this is not a question of the implementation as such, but rather in which way it would be categorized.","If the DTO uses the live state of the Persistence object then this may be classified as an adapter; if it makes a defensive copy of the data, discarding the original, then it is not an adapter.",https://stackoverflow.com/questions/14280168/java-persistence-class-with-todto-method-categorized-as-adapter-pattern/14280541,2,0
"Difference between DTO, VO, POJO, JavaBeans?","Have seen some similar questions:

What is the difference between a JavaBean and a POJO?
What is the Difference Between POJO (Plain Old Java Object) and DTO (Data Transfer Object)?

Can you also please tell me the contexts in which they are used? Or the purpose of them?","JavaBeans
A JavaBean is a class that follows the JavaBeans conventions as defined by Sun. Wikipedia has a pretty good summary of what JavaBeans are:

JavaBeans are reusable software components for Java that can be manipulated visually in a builder tool. Practically, they are classes written in the Java programming language conforming to a particular convention. They are used to encapsulate many objects into a single object (the bean), so that they can be passed around as a single bean object instead of as multiple individual objects. A JavaBean is a Java Object that is serializable, has a nullary constructor, and allows access to properties using getter and setter methods.
In order to function as a JavaBean class, an object class must obey certain conventions about method naming, construction, and behavior. These conventions make it possible to have tools that can use, reuse, replace, and connect JavaBeans.
The required conventions are:

The class must have a public default constructor. This allows easy instantiation within editing and activation frameworks.
The class properties must be accessible using get, set, and other methods (so-called accessor methods and mutator methods), following a standard naming convention. This allows easy automated inspection and updating of bean state within frameworks, many of which include custom editors for various types of properties.
The class should be serializable. This allows applications and frameworks to reliably save, store, and restore the bean's state in a fashion that is independent of the VM and platform.

Because these requirements are largely expressed as conventions rather than by implementing interfaces, some developers view JavaBeans as Plain Old Java Objects that follow specific naming conventions.

POJO
A Plain Old Java Object or POJO is a term initially introduced to designate a simple lightweight Java object, not implementing any javax.ejb interface, as opposed to heavyweight EJB 2.x (especially Entity Beans, Stateless Session Beans are not that bad IMO). Today, the term is used for any simple object with no extra stuff. Again, Wikipedia does a good job at defining POJO:

POJO is an acronym for Plain Old Java
  Object. The name is used to emphasize
  that the object in question is an
  ordinary Java Object, not a special
  object, and in particular not an
  Enterprise JavaBean (especially before
  EJB 3). The term was coined by Martin
  Fowler, Rebecca Parsons and Josh
  MacKenzie in September 2000:

""We wondered why people were so against using regular objects in their
    systems and concluded that it was
    because simple objects lacked a fancy
    name. So we gave them one, and it's
    caught on very nicely.""

The term continues the pattern of
  older terms for technologies that do
  not use fancy new features, such as
  POTS (Plain Old Telephone Service) in
  telephony, and PODS (Plain Old Data
  Structures) that are defined in C++
  but use only C language features, and
  POD (Plain Old Documentation) in Perl.
The term has most likely gained
  widespread acceptance because of the
  need for a common and easily
  understood term that contrasts with
  complicated object frameworks. A
  JavaBean is a POJO that is
  serializable, has a no-argument
  constructor, and allows access to
  properties using getter and setter
  methods. An Enterprise JavaBean is not
  a single class but an entire component
  model (again, EJB 3 reduces the
  complexity of Enterprise JavaBeans).
As designs using POJOs have become
  more commonly-used, systems have
  arisen that give POJOs some of the
  functionality used in frameworks and
  more choice about which areas of
  functionality are actually needed.
  Hibernate and Spring are examples.

Value Object
A Value Object or VO is an object such as java.lang.Integer that hold values (hence value objects). For a more formal definition, I often refer to Martin Fowler's description of Value Object:

In Patterns of Enterprise Application Architecture I described Value Object as a small object such as a Money or date range object. Their key property is that they follow value semantics rather than reference semantics.
You can usually tell them because their notion of equality isn't based on identity, instead two value objects are equal if all their fields are equal. Although all fields are equal, you don't need to compare all fields if a subset is unique - for example currency codes for currency objects are enough to test equality.
A general heuristic is that value objects should be entirely immutable. If you want to change a value object you should replace the object with a new one and not be allowed to update the values of the value object itself - updatable value objects lead to aliasing problems.
Early J2EE literature used the term value object to describe a different notion, what I call a Data Transfer Object. They have since changed their usage and use the term Transfer Object instead.
You can find some more good material on value objects on the wiki  and by Dirk Riehle.

Data Transfer Object
Data Transfer Object or DTO is a (anti) pattern introduced with EJB. Instead of performing many remote calls on EJBs, the idea was to encapsulate data in a value object that could be transfered over the network: a Data Transfer Object. Wikipedia has a decent definition of Data Transfer Object:

Data transfer object (DTO), formerly known as value objects or VO, is a design pattern used to transfer data between software application subsystems. DTOs are often used in conjunction with data access objects to retrieve data from a database.
The difference between data transfer objects and business objects or data access objects is that a DTO does not have any behaviour except for storage and retrieval of its own data (accessors and mutators).
In a traditional EJB architecture, DTOs serve dual purposes: first, they work around the problem that entity beans are not serializable; second, they implicitly define an assembly phase where all data to be used by the view is fetched and marshalled into the DTOs before returning control to the presentation tier.


So, for many people, DTOs and VOs are the same thing (but Fowler uses VOs to mean something else as we saw). Most of time, they follow the JavaBeans conventions and are thus JavaBeans too. And all are POJOs.",https://stackoverflow.com/questions/1612334/difference-between-dto-vo-pojo-javabeans/1612671,7,0
What is the difference between a JavaBean and a POJO?,"I'm not sure about the difference. I'm using Hibernate and, in some books, they use JavaBean and POJO as an interchangeable term. I want to know if there is a difference, not just in the Hibernate context, but as general concepts.","A JavaBean follows certain conventions. Getter/setter naming, having a public default constructor, being serialisable etc. See JavaBeans Conventions for more details.
A POJO (plain-old-Java-object) isn't rigorously defined. It's a Java object that doesn't have a requirement to implement a particular interface or derive from a particular base class, or make use of particular annotations in order to be compatible with a given framework, and can be any arbitrary (often relatively simple) Java object.",https://stackoverflow.com/questions/1394265/what-is-the-difference-between-a-javabean-and-a-pojo,9,0
What is the difference between POJO (Plain Old Java Object) and DTO (Data Transfer Object)?,I cannot find difference between them. Does anyone know how to differentiate them?,"POJO or ""Plain Old Java Object"" is a name used to describe ""ordinary"" Java objects, as opposed to EJBs (originally) or anything considered ""heavy"" with dependencies on other technologies.
DTO or ""Data Transfer Object"" is an object for... well... transferring data, usually between your ""business"" classes and persistence layer. It typically is a behavior-less class much like a C-style struct. They are an outdated concept.",https://stackoverflow.com/questions/1425302/what-is-the-difference-between-pojo-and-dto,8,0
Difference between the design pattern and anti-pattern,"I'm reading the theory about designing software architecture and I see there are some theories such as SOLID principles, Design Pattern and Anti-Pattern that we should consider when we design our software.
My Question: What are the differences between the Design Pattern and Anti-Pattern and what problem they solve?","Simple Difference would be:
Design Patterns:
Solutions which are productive and efficient and are developed by Software Engineers over the years of practice and solving problems.  
Anti Patterns:
Known solutions which are actually bad or defective to certain kind of problems.
From Wikipedia:

An anti-pattern is a common response to a recurring problem that is
  usually ineffective and risks being highly counterproductive.[1][2]
  The term, coined in 1995 by Andrew Koenig,[3] was inspired by a book,
  Design Patterns, which highlights a number of design patterns in
  software development that its authors considered to be highly reliable
  and effective.
The term was popularized three years later by the book AntiPatterns,
  which extended its use beyond the field of software design to refer
  informally to any commonly reinvented but bad solution to a problem.
  Examples include analysis paralysis, cargo cult programming, death
  march, groupthink and vendor lock-in.

Example of some anti patterns:

https://javaantipatterns.wordpress.com/
https://sourcemaking.com/antipatterns

Hope this makes sense.",https://stackoverflow.com/questions/45808254/difference-between-the-design-pattern-and-anti-pattern,1,0
Name of anti pattern for complexity,"Is there a standard anti-pattern or something that could be referenced, to argue that when a system reaches a given complexity it will become unmaintainable and will collapse?
Something like systems are never finished they are just abandoned, just a more serious version.","Here is a suggestion: the Lava Flow description. It isn't exactly what you're talking about, but it might, with a longer description of the project.
I've personally encountered this monster on several big scale projects.",https://stackoverflow.com/questions/5964276/name-of-anti-pattern-for-complexity,2,0
Enterprise Architecture Anti-patterns [closed],"Closed. This question is off-topic. It is not currently accepting answers.
                            
                        










Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                        
Closed 9 months ago.



What are the key anti-patterns to avoid when architecting applications for the enterprise? We are using C# and SQL Server and Silverlight, btw - but I imagine some of the anti-patterns will be language neutral.","Wikipedia is quite comprehensive on the subject of anti-patterns and here's the book on the subject: AntiPatterns. 
Top five in my experience:

Premature optimization
Anemic Domain Model
God object
Golden Hammer (cf. regular expressions)
Chain Gang",https://stackoverflow.com/questions/2039789/enterprise-architecture-anti-patterns,2,0
HBase Anti-Patterns,"What tasks should one not use HBase for?
My understanding is that HBase and HDFS should be treated as transient data stores, holding data only for the duration of time that a map/reduce job needs them for.
Is it inappropriate to use HBase as a canonical data store? Its random-access latency spikes make this pretty impractical anyway, but this could be mitigated with caching and other badn-aids.","HBase is supposed to be used as a canonical data store (like Big Table, which is used heavily in many Google services). HDFS is made for MapReduce, but HBase is built on HDFS for allowing more than just MapReduce. HBase is really a data base.
The main reason why you should choose HBase over traditional relational database systems is scalability. If you don't have Big Data, do not bother using HBase. If your application requires many joins, needs a query language like SQL, and doesn't have data scaling around the globe, prefer a RDBMS.
On the other hand, if your relational database has a huge number of NULL entries, HBase is a nice alternative since it is sparse: it doesn't store NULLs.",https://stackoverflow.com/questions/9940360/hbase-anti-patterns,1,0
"is it an 'anti-pattern' to grab mobile configuration from a remote, 'configuration' server?","I'm new to mobile development and was wondering if it is an anti-pattern to have a remote, configuration server in order to 'configure' a mobile client.  The idea is to avoid configuration details [except the URL to the configuration server] with the mobile distribution and rather connect to the configuration server to grab other details such as third party keys, service endpoints, etc.  Any thoughts. Thanks in advance!!","So long as you are able to secure connections to the the configuration server, using a username/password and SSL encryption it's not an anti-pattern. It would save you from publishing new applications versions just to update some configuration.
What does however sound like a bad idea to me is keeping third-party API keys inside a mobile application.
Can you guarantee that each an every user using you application will not use the keys placed inside the mobile application to use the third party service to their own ends? I don't think anyone can make that guarantee.",https://stackoverflow.com/questions/44401503/is-it-an-anti-pattern-to-grab-mobile-configuration-from-a-remote-configurati,1,0
Exposed domain model in Java microservice architecture,"I'm aware that copying entity classes and properties into DTOs is considered anti-pattern, so by Exposed domain model pattern the same @Entity can be used as both database entity class, and DTO for service and MVC layer. (see here https://codereview.stackexchange.com/questions/93511/data-transfer-objects-vs-entities-in-java-rest-server-application)
But suppose we have microservice architecture where the same set of properties is used as entity in one project with persistence, and as DTO in another project which uses the first one as a service. What's the proposed pattern in such a situation?
Because the second project doesn't need @Entity related functionality, and if we put that class in shared library, it will be tied unnecessary to JPA specific APIs and libraries. And the alternative is to again use separate DTO classes anti-pattern.","When your requirements for a DTO model exactly match your entity model you are either in a very early stage of the project or very lucky that you just have a simple model. If your model is very simple, then DTOs won't give you many immediate benefits.
At some point, the requirements for the DTO model and the entity model will diverge though. Imagine you add some audit aspects, statistics or denormalization to your entity/persistence model. That kind of data is usually never exposed via DTOs directly, so you will need to split the models. It is also often the case that the main driver for DTOs is the fact that you don't need all the data all the time. If you display objects in e.g. a dropdown you only need a label and the object id, so why would you load the whole entity state for such a use case?
The fact that you have annotations on your DTO models shouldn't bother you that much, what is the alternative? An XML-like mapping? Manual object wiring?
If your model is used by third parties directly, you could use a subclassing i.e. keep the main model free of annotations and have annotated subclasses in your project that extend the main model.
Since implementing a DTO approach correctly, I created Blaze-Persistence Entity Views which will not only simplify the way you define DTOs, but it will also improve the performance of your queries.
If you are interested, I even have an example for an external model that uses entity view subclasses to keep the main model clean.",https://stackoverflow.com/questions/46425554/exposed-domain-model-in-java-microservice-architecture,2,0
React and Redux architecture review,"I麓m new to react world and I麓m trying to understaand what麓s the best way to architecture my code. 
Actually, it麓s a react-native using react-native-router-flux, but I don麓t believe these make any difference to the question.
I麓ve tried to setup a splash screen like this:
/* @flow */
'use strict';

import { connect } from 'react-redux';

import React, { Component } from 'react';

import { View, Text, Image } from 'react-native';

import { init } from ""../core/auth/actions""

import { Actions } from 'react-native-router-flux';

const mapStateToProps = (state) => {
    return {
        isAuthenticated: state.authState.authenticated,
        showLock: state.authState.showLock
    }
}

class Splash extends Component {

    constructor(props) {
        super(props)
    }

    componentWillMount() {
        const {dispatch} = this.props;
        dispatch(init());
    }

    navigateToHome(){
        Actions.home();
    }

    navigateToLogin(){
        Actions.login();
    }

    render() {

        const { isAuthenticated, showLock } = this.props

        return (
            <View>
                {isAuthenticated &&
                    this.navigateToHome()
                }
                {showLock && this.navigateToLogin()}
            </View>

        );
    }

}

const connectedSplash = connect(mapStateToProps)(Splash)

export default connectedSplash;

When the application starts, this scene gets rendered, and a init action is dispatched.
After passing through a reducer it changes the state to either authenticated or showlock, and then I麓m redirecting to the next scene.
It all works perfectly actually. However I麓m getting the following warning:

Warning: setState(...): Cannot update during an existing state
  transition (such as within render or another component's
  constructor). Render methods should be a pure function of props and
  state; constructor side-effects are an anti-pattern, but can be moved
  to componentWillMount.

What would be the recommended way to accomplish something similar?
I.e upon a store state change call a method?","Not sure if I got the best possible way, but this approach seemed already better to me:
    componentDidUpdate(){
    const { isAuthenticated, showLock } = this.props
    if (isAuthenticated){
        return this.navigateToHome();
    }

    if (showLock){
        this.navigateToLogin();
    }

}

render() {


    return (
        <View>

        </View>

    );
}",https://stackoverflow.com/questions/41656095/react-and-redux-architecture-review,1,0
MVC - Sharing ViewModel instances across multiple Views,"Just wondering what peoples thoughts are about sharing the one view model instances across multiple Views? 
Is this an anti pattern or is it accepted practice, what are the pros and cons?","Just from my experience
The main drawback can be refactoring effort in future if the views diverge in what they display. You need to think about how likely this is when you make the decision to share the view.
Also be careful with the naming of the view models. For example if you have the views Book/Edit.aspx and Book/Add.aspx you would not want AddBookViewModel.cs
used in the Book/Edit.aspx. This could create some confusing looking code in the your tests for example.
In the above example, I would typically go with CreateEditBookViewModel.cs
If you look at the likes of SharpArchitecture (www.sharparchitecture.net/) I believe they use this approach in their scaffolding and they also share the elements of the views using the shared viewmodels via partials.
In terms of it being an anti-pattern, I'd say no; just healthy code reuse.",https://stackoverflow.com/questions/5018632/mvc-sharing-viewmodel-instances-across-multiple-views,1,0
How to be with huge Domain classes in DDD?,"When you are developing an architecture in OO/DDD style and modeling some domain entity e.g. Order entity you are putting whole logic related to order into Order entity. 
But when the application becomes more complicated, Order entity collects more and more logic and this class becomes really huge. 
Comparing with anemic model, yes its obviously an anti-pattern, but all that huge logic is separated in different services.
Is it ok to deal with huge domain entities or i understand something wrong?","It's fine to use services in DDD.  You will commonly see services at the Domain, Application or Infrastructure layers.
Eric uses these guidelines in his book for spotting when to use services:

The operation relates to a domain concept that is not a natural part of an ENTITY or VALUE OBJECT.
The interface is defined in terms of other elements in the domain model
The operation is stateless",https://stackoverflow.com/questions/12665393/how-to-be-with-huge-domain-classes-in-ddd,3,0
Layered architecture in Java EE,"I have a question that I have never been able to solve:
Considering these two architecture
1st
UI layer
    |
Application layer
    |
Domain Layer
    |
Infrastructure Layer

2nd
Client Tiers
    |
Presentation Tiers
    |
Business Tiers
    |
Integration Tiers
    |
Resources Tiers

What are the difference between them.
Where do entity beans lie in these architecture. If I have a business layer with object that implements business logic, why would I have to add behavior in entity beans. I read somewhere that it is an anti pattern to have domain model objects without behavior.
Thanks
Update
This is actually a project(training) that I need to do to get my msc in distributed systems.
These are actually the technologies I'm using
Struts 2
JPA
HSQLDB
So if I understand well
My application consist of
A client layer (web browser)
A presentation Layer (struts 2 )
A business Layer (POJOs + JPA)
An integration layer (with hibernate DAOs)
A resource layer (HSQLDB)
But as the presentation, business and integration layer are implemented on the same server (tomact) I only have a three tiers architecture. Am I right ?
As far as including behavior in my JPA objects, usually this is what I used to do :
Have a dao for each JPA entity.
Have a bean (like an EJB) that would manage the business logic required. So I never put beahvior on JPA objects.
Say for example I wanted to make a purchase request. I would have a CatalogueManager that would help me interact with items, suppliers. I would have also an EmployeeManager that would help me interact with employees. And finally a PurchaseRequestManager that would use the two previous business objects to make a PurchaseRequest.
Now what you are telling me is to put the methods that I have in the PurchaseManager in the PurchaseRequest JPA entity, and do the same for methods in EmployeeManager => to put them in the Employee JPA entity.
But what would happen if my employee object would also be used for the human ressources department, I would also need to put other methods there. For a big application I would have a lot of methods in the employee JPA entity. Wouldn't that be counter productive?
thanks","In my mind, ""Layers"" are a logical separation, with no implication of a deployment topology.
""Tiers"" are about physical deployment. For example the UI Layer logic could be implemented completely in a thick client, deployed to a Client Tier on the desktop and no separate Presentation Tier, or could be a Web 2.0 Browser based app, with the UI Layer spread between a Javascript UI Client in the Browser and Presentation Tier in a Server.
Now onto Entitiy Beans. First, Entity Beans are in EJB 3 replaced with JPA - we annotate objects to control their persistence.
I think that you've got two kinds of business logic, that which is concerned with the behaviour of individual persistent Classes such as Customers, Orders, Employees, Shipments, Students, Courses or whatever, and then their is logic which is at some higher level than that, dealing with combinations of these classes.
It seems reasonable to me that logic concerned with behaviour of, say, a Customer should be in the Customer class. Such behaviour might be quite trivial, for example some kinds of validation and summarisation (e.g. total Order Value) but it's Domain logic and can reasonably be in those Domain objects. So our JPA objects have two roles, implementing domain logic and also managing persistence by virtue of their annotation. The architectural status of those annotations is interesting, they are effectively the ""glue"" between Domain and Infrastructure.",https://stackoverflow.com/questions/4122079/layered-architecture-in-java-ee,2,0
"From an architectural standpoint, should models in a rails application be able to set their own user association in an after_initialize function?","Let us assume the following structure:
class Question < ActiveRecord::Base
  after_initialize :set_defaults

  belongs_to :user

  ...

  private
    def :set_defaults
      self.user = SomeAuth.current_user
    end
end

class User < ActiveRecord::Base
  has_many :questions

  ...
end

Further, let us assume that SomeAuth is some black-box magic authentication scheme.
We can think of that as Devise or Authlogic (or insert your favorite ruby authentication solution here).
Under those assumptions, is it an anti-pattern to have a Question set its own user attribute after initialization (for all intents and purposes, after Question.new is called, i.e. in an after_initialize macro)?","I wouldn't want to classify it as a good or bad pattern in absolute terms.
If your system is only one RoR application, then it seems like a good pattern. You use a mechanism provided by a framework thus not creating extra code and keeping it pretty simple. Your domain object is tied to your auth scheme, but in this simple context it may be  beneficial.
If your system is collection of many distinct applications, and you want to reuse a domain model between them, then you might want to divorce your domain and auth concepts. Some subsystems may not care about auth concerns (like batch processing of a certain kind). In this context an explicit relationship between domain and auth (and the way it gets loaded) might be an anti-pattern.",https://stackoverflow.com/questions/7853975/from-an-architectural-standpoint-should-models-in-a-rails-application-be-able-t,1,0
Managing many windows in GUI architecture,"I've got a question about an architecture af a complex GUI app.
I'm creating such app and have many, many windows which often interacts with each other, and I'm facing problem of having all needed references to other windows within them.
What is the best approach to manage those windows?
First think I came up with was to create global singleton sharing all windows references (and encapsulate them in getters/setters) and each windows would have only reference to that, let's say 'window manager' - but generally I don't like that type of programming (I'm anti-global :P), I consider it's bad and unsafe.
Are there any paradigms/design patterns am I missing?
Appreciate any useful tips,
Maciek","You need an application wide ""controller"". Think of it like a central DNS. It doesn't have to be a special ""global singleton"". Your application will already have one of those (the root object of your app). Just have it manage a list of windows.",https://stackoverflow.com/questions/8277491/managing-many-windows-in-gui-architecture,2,0
Java - Is this a bad design pattern?,"In our application, I have seen code written like this:
User.java (User entity)
public class User
{
  protected String firstName;
  protected String lastName;

 ...
   getters/setters (regular POJO)
}

UserSearchCommand
{
   protected List<User> users;
   protected int currentPage;
   protected int sortColumnIndex;
   protected SortOder sortOrder;

   // the current user we're editing, if at all
   protected User user;

   public String getFirstName()
   {return(user.getFirstName());}

   public String getLastName()
   {return(user.getLastName());}

}

Now, from my experience, this pattern or anti-pattern looks bad to me.  For one, we're mixing several concerns together.  While they're all user-related, it deviates from typical POJO design.  If we're going to go this route, then shouldn't we do this instead?
UserSearchCommand
{
   protected List<User> users;
   protected int currentPage;
   protected int sortColumnIndex;
   protected SortOder sortOrder;

   // the current user we're editing, if at all
   protected User user;

   public User getUser()
   {return(user);}

}

Simply return the user object, and then we can call whatever methods on it as we wish?  
Since this is quite different from typical bean development, JSR 303, bean validation doesn't work for this model and we have to write validators for every bean.
Does anyone else see anything wrong with this design pattern or am I just being picky as a developer?
Walter","While Sjoerd and JB make valid points, depending on your use of the SearchCommand I would make the following argument for the second example. If the operations you are defining in the first example do not effect the behavior of the UserSearchCommand, then by defining getFirstName(), etc you are really just duplicating code, which can lead to maintainability issues, for example what if later you add a middle name to the user class? Then you not only need to add it to user but also an accessor in UserSearchCommand. If doing something to the user would modify the behavior of the search then that could be a valid argument to have the caller access the user through the search command, but this could also be achieved via a mechanism such as PropertyListeners.
As you pointed out the first example is mixing information together and, to me, seems counter-intuitive from and OOP perspective. If it is a matter of restricting access to some properties to the User then it may be a matter of changing the access modifiers, or creating an interface that exposes what you deem safe and an implementation class that exposes appropriate package/protected properties. Unless your UserSearchCommand is a misnomer I would expect it to perform operations involving searching for users and then make those user's available as a unit (not individual properties of a user). That is a search command should perform operations related to a search and a user should contain information about the user.
Of course this is a stylistic question, but I would give my vote to your second example.",https://stackoverflow.com/questions/2560972/java-is-this-a-bad-design-pattern,5,0
What is Inversion of Control?,"Inversion of Control (or IoC) can be quite confusing when it is first encountered.

What is it?
Which problem does it solve?
When is it appropriate to use and when not?","The Inversion of Control (IoC) and Dependency Injection (DI) patterns are all about removing dependencies from your code.
For example, say your application has a text editor component and you want to provide spell checking. Your standard code would look something like this:
public class TextEditor {

    private SpellChecker checker;

    public TextEditor() {
        this.checker = new SpellChecker();
    }
}

What we've done here creates a dependency between the TextEditor and the SpellChecker.
In an IoC scenario we would instead do something like this:
public class TextEditor {

    private IocSpellChecker checker;

    public TextEditor(IocSpellChecker checker) {
        this.checker = checker;
    }
}

In the first code example we are instantiating SpellChecker (this.checker = new SpellChecker();), which means the TextEditor class directly depends on the SpellChecker class.
In the second code example we are creating an abstraction by having the SpellChecker dependency class in TextEditor constructor signature (not initializing dependency in class). This allows us to call the dependency then pass it to the TextEditor class like so:
SpellChecker sc = new SpellChecker; // dependency
TextEditor textEditor = new TextEditor(sc);

Now the client creating the TextEditor class has the control over which SpellChecker implementation to use because we're injecting the dependency to the TextEditor signature.",https://stackoverflow.com/questions/3058/what-is-inversion-of-control,36,0
Are try/catch for every single statement that throws an exception considered an anti-pattern?,"I am currently reviewing a colleagues Java code, and I see a lot of cases where every single statement that may throw an exception being encapsulated in its own try/catch. Where the catch block all perform the same operation (which operation is not relevant for my question).
To me this seems like a code smell, and I do remember reading about it being a common anti-pattern. However I cannot find any references on this.
So are try/catch for every single statement that throws and exception considered an anti-pattern, and what are the argument supporting this?

Constructed example:
(Does not relate to the original problem, so please don't mind other problems with this example, as it is just constructed to illustrate what I mean.)
public int foo()
{
    int x, y = 7;

    try
    {
        x = bar(y);
    }
    catch(SomeException e)
    {
        return 0;
    }

    try
    {
        y = baz(x,y);
    }
    catch(SomeOtherException e)
    {
        return 0;
    }

    /* etc. */

    return y;
}

(Assume that it is appropriate to catch both exceptions here, i.e. we know what do with them, and the appropriate thing is to return 0 in both cases.)","I can't serve you with an authoritative source, but these are fundamental tenets of exception handling:

exceptions should be caught at the point where one can properly handle them, and
you must not swallow exceptions - there should always be (at least) a trace of an exception thrown, so the minimum is to log it, so that at least the developers get a chance of noticing that something bad happened. Which leads to the third point:
exceptions should only be used to signal exceptional bad events, not to control program flow.

There are lots of earlier posts on SO dealing with this, for example Best practices for exception management in JAVA or C#.",https://stackoverflow.com/questions/7093193/are-try-catch-for-every-single-statement-that-throws-an-exception-considered-an/7093257,6,0
Best practices for exception management in Java or C# [closed],"Closed. This question is opinion-based. It is not currently accepting answers.
                            
                        










Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.
                        
Closed 3 years ago.



I'm stuck deciding how to handle exceptions in my application. 
Much if my issues with exceptions comes from 1) accessing data via a remote service or 2) deserializing a JSON object. Unfortunately I can't guarantee success for either of these tasks (cut network connection, malformed JSON object that is out of my control). 
As a result, if I do encounter an exception I simply catch it within the function and return FALSE to the caller. My logic is that all the caller really cares about is if the task was successful, not why it is wasn't successful.
Here's some sample code (in JAVA) of a typical method)
public boolean doSomething(Object p_somthingToDoOn)
{
    boolean result = false;

    try{
        // if dirty object then clean
        doactualStuffOnObject(p_jsonObject);

        //assume success (no exception thrown)
        result = true;
    }
    catch(Exception Ex)
    {
        //don't care about exceptions
        Ex.printStackTrace();
    }
    return result;
}

I think this approach is fine, but I'm really curious to know what the best practices are for managing exceptions (should I really bubble an exception all the way up a call stack?). 
In summary of key questions:

Is it okay to just catch exceptions but not bubble them up or formally notifying the system (either via a log or a notification to the user)?
What best practices are there for exceptions that don't result in everything requiring a try/catch block?

Follow Up/Edit
Thanks for all the feedback, found some excellent sources on exception management online:

Best Practices for Exception Handling | O'Reilly Media
Exception Handling Best Practices in .NET
Best Practices: Exception Management (Article now points to archive.org copy)
Exception-Handling Antipatterns

It seems that exception management is one of those things that vary based on context. But most importantly, one should be consistent in how they manage exceptions within a system. 
Additionally watch out for code-rot via excessive try/catches or not giving a exception its respect (an exception is warning the system, what else needs to be warned?).
Also, this is a pretty choice comment from m3rLinEz.

I tend to agree with Anders Hejlsberg and you that the most callers only
  care if operation is successful or not.

From this comment it brings up some questions to think about when dealing with exceptions:

What is the point this exception being thrown?
How does it make sense to handle it? 
Does the caller really care about the exception or do they just care if the call was successful?
Is forcing a caller to manage a potential exception graceful?
Are you being respectful to the idoms of the language?

Do you really need to return a success flag like boolean? Returning boolean (or an int) is more of a C mindset than a Java (in Java you would just handle the exception) one. 
Follow the error management constructs associated with the language :) !","It seems odd to me that you want to catch exceptions and turn them into error codes. Why do you think the caller would prefer error codes over exceptions when the latter is the default in both Java and C#?
As for your questions: 

You should only catch exceptions that you can actually handle. Just
catching exceptions is not the right thing to do in most cases.
There are a few exceptions (e.g. logging and marshalling exceptions
between threads) but even for those cases you should generally
rethrow the exceptions.
You should definitely not have a lot of try/catch statements in your
code. Again, the idea is to only catch exceptions you can handle.
You may include a topmost exception handler to turn any unhandled
exceptions into something somewhat useful for the end user but
otherwise you should not try to catch each and every exception in
every possible place.",https://stackoverflow.com/questions/409563/best-practices-for-exception-management-in-java-or-c,15,0
What topics fall under architecture?,"I am an experienced coder. During the time I learnt C#, I followed a set of topics eg serialisation, exception handling, graphics, etc.
However, what topics are there in architecture and software design?
My list contains the concept of roundtrips, tiers, layering, is there anything else?
Thanks","At the architectural level we are concerned about:

Services
Components
Security
Infrastructure
Backward/forward compatibility (migration, dual phase, etc)
Scalability
Reliability
Federation
Standards (not necessarily industry standards, just consistency)

Architecture is just as much about non-technical communication and business strategy as it is about anything technical. Architects are responsible for translating business goals into systems, which are implemented by developers and technicians.
As far as your existing list - ""tiers"" is a high-level development concept, while I would consider ""roundtrips"" relatively low-level. At an architectural level, and even a high-level software design level, ""roundtrips"" is an implementation detail of the communication between systems, and is not very interesting until it starts impacting one of the areas listed above. I'm not sure what you mean by ""layering"".",https://stackoverflow.com/questions/3182196/what-topics-fall-under-architecture/3183588,4,0
Can I dispatch an action in reducer?,"is it possible to dispatch an action in a reducer itself? I have a progressbar and an audio element. The goal is to update the progressbar when the time gets updated in the audio element. But I don't know where to place the ontimeupdate eventhandler, or how to dispatch an action in the callback of ontimeupdate, to update the progressbar. Here is my code:
//reducer

const initialState = {
    audioElement: new AudioElement('test.mp3'),
    progress: 0.0
}

initialState.audioElement.audio.ontimeupdate = () => {
    console.log('progress', initialState.audioElement.currentTime/initialState.audioElement.duration);
    //how to dispatch 'SET_PROGRESS_VALUE' now?
};


const audio = (state=initialState, action) => {
    switch(action.type){
        case 'SET_PROGRESS_VALUE':
            return Object.assign({}, state, {progress: action.progress});
        default: return state;
    }

}

export default audio;","Dispatching an action within a reducer is an anti-pattern. Your reducer should be without side effects, simply digesting the action payload and returning a new state object. Adding listeners and dispatching actions within the reducer can lead to chained actions and other side effects.
Sounds like your initialized AudioElement class and the event listener belong within a component rather than in state. Within the event listener you can dispatch an action, which will update progress in state.
You can either initialize the AudioElement class object in a new React component or  just convert that class to a React component.
class MyAudioPlayer extends React.Component {
  constructor(props) {
    super(props);

    this.player = new AudioElement('test.mp3');

    this.player.audio.ontimeupdate = this.updateProgress;
  }

  updateProgress () {
    // Dispatch action to reducer with updated progress.
    // You might want to actually send the current time and do the
    // calculation from within the reducer.
    this.props.updateProgressAction();
  }

  render () {
    // Render the audio player controls, progress bar, whatever else
    return <p>Progress: {this.props.progress}</p>;
  }
}

class MyContainer extends React.Component {
   render() {
     return <MyAudioPlayer updateProgress={this.props.updateProgress} />
   }
}

function mapStateToProps (state) { return {}; }

return connect(mapStateToProps, {
  updateProgressAction
})(MyContainer);

Note that the updateProgressAction is automatically wrapped with dispatch so you don't need to call dispatch directly.",https://stackoverflow.com/questions/36730793/can-i-dispatch-an-action-in-reducer/41260990,4,0
Is Domain Driven Design right for my project?,"I have been reading this ebook about DDD and it says that only highly complex systems are suited for DDD architecture.  This leads me to second guess my decision to move more towards DDD as my architecture.
I am converting a classic ASP application over to .NET section by section.  It includes a robust product categorization scheme and shopping cart which gets ~100-200 orders per day, and a video section similar to YouTube (videos and social features like rating, commenting, etc).  Since I have converting it in chunks, I want to treat each area of the site as separate project.
The site continuously gets new features and needs to be easy to maintain and update.
Right now I am just using a basic homemade ADO.NET DAL with BLL and DTOs that act as a common layer.
Would it be better to go with a different architecture than DDD for this project?  I am new to architecture and want to use some sort of pattern as a guide that I can follow throughout the conversion process to avoid the dreaded spaghetti anti-pattern.
If not DDD, then what?  Still trying to find a good direction.  It needs to be fast and easy for me to get started on without being a complete expert as I am still learning.","DDD is not an architecture. 
It's a philosophy of design, you cannot just rename all your FooDAO's to FooRepositiories, throw in an Anti-Corruption Layer and call it DDD.
It stands for Domain Driven Design.  It puts a focus on the models that you use to solve problems in your specific domain.  What Eric Evans is suggesting is that if your site is simply a form to join a mailing list there's no reason to spend days in front of whiteboard playing with models. It's my opinion if there's only a single context in your domain you don't need DDD.  More on that in a bit.
There's a famous quote:

__here are only two hard problems in Computer Science: cache invalidation and naming things._ _ Phil Karlton

And DDD does have patterns to address these. It offers ubitiquous language as pattern to tackle naming, and it offers the oft misunderstood collection of patterns: Repository, Aggregate, Entity, and Value Object to tackle model consistency (which I will lump cache invalidation into and won't cover further here).  
But I would say most importantly it adds a critical 3rd item (not off by 1 problems):

Where to put stuff

Where to put code and logic. For me, the most fundamental concept in DDD is that of context.  Not all problems are best served by the same model, and knowing where one context ends and another begins is a critical first step in DDD.
For instance, at my company we work with Jobs, Jobseekers and Recruiters.  The world of a jobseeker is very different from the world of a recruiter and they both look at a Job differently.  As an example,  In the world (context) of Recruiters they can post a job and say 

I want to make this job available in New York, Austin, and San Fran.

In OO speak: One Job has one or many Locations.  
In the world of the jobseeker a job in LA is not the same job as a job in Boston. Nevermind if they are the same position at the same company.  The difference in physical location is meaningful to the the jobseeker.  While the recruiter wants to manage all Widget Manager jobs from a single place even if they are spread out around the country, a Jobseeker in New York does not care if the same position is also available in Seattle.
So the question is?  How many Locations should a Job have?  One or Many?
The DDD answer is both. If you're in the context of jobseeker then a job has only one location, and if you're a recruiter in that context a job can have many locations.  
The Jobseeker context is wholly separate from the Recruiter Context and they should not necessarily share the same model.  Even if in the end of the day all the jobs end up in the same DB (another context itself perhaps), sharing models between contexts can make a model a jack of all trades and master of none.  
Now this example is very specific to the Domain of recruitment and jobseeking.  It has nothing to do with Entity Framework of ADO or MVC or ASP.  DDD is framework agnostic.  
And it is DDD heresy to claim that framework A or B makes your architecture DDD. The whole point of DDD is that a model should serve the needs of a specific Context within a particular Domain.  Frameworks can only support that and make it possible, they cannot do:
$ dddonrails new MyDDDApplication
$ dddonrails generate context ContextA
$ dddonrails generate context ContextB
$ dddonrails generate model Widget ContextA
$ dddonrails generate model Widget ContextB
$ dddonrails start

To specifically address the question, ""To DDD? Or not to DDD?""  The good news is you don't have to decide at the onset, ""This is going to be a DDD project!""  DDD requires no toolset other than the willingness to think hard about the problems you're trying to solve and ask is my code helping or hurting me?  
The bad news is DDD requires a serious commitment to look at and challenge your design, asking every day ""Is this model making the problems in this context as easy as possible?""
But separate the somewhat tactical and practical concerns of what presentation framework (MVC or no MVC) and persistence framework (Entity Framework?) from the task of modeling your business domain.  If you want to start now, think about what contexts are in your app.  Some questions to ask:

Are multiple areas of the application solving different problems with the same basic data? 
How many teams work on the app?  
How do they integrate with each other? 

Mapping these out is called Drawing a Context Map and it's an important first step to starting to do DDD.
I encourage you to checkout the ddd website for more.  There's some good eric evans videos on qcon too.  You may also want to read the Eric Evans' book Domain Driven Design, he has many more examples.",https://stackoverflow.com/questions/5409324/is-domain-driven-design-right-for-my-project,3,0
How do I resolve Dependency Injection in MVC Filter attributes,"I have a custom attribute class derived from AuthorizationAttribute, which performs custom security on controller actions. The OnAuthorizationCore method depends on various other components (e.g. DAL) in order to ajudicate whether a user can invoke an action.
I'm using Autofac for dependency injection. The ExtensibleActionInvoker claims to be able to perform property injection on action filters. Setting an attribute's properties at runtime (which seems like a bad idea) will work in a simple unit test, but in a busy, multi-threaded web server it's bound to go wrong, and so this idea seems like an anti-pattern. Hence this question:
If my AuthorizationAttribute depends on other components in order to work correctly, what it the right [architecture] pattern in order to achieve this?
i.e. AuthorizationAttribute depends on IUserRepository...  how should this relationship be resolved?","The ExtensibleActionInvoker claims to be able to perform property injection on action filters.

Correct - but don't confuse action filters with the attributes that might not implement them. The cleanest way to approach this in ASP.NET MVC is to split responsibilities, even though the MVC framework allows you to combine them.
E.g., use a pair of classes - an attribute class that holds data only:
// Just a regular old attribute with data values
class SomeAttribute : Attribute { ... }

And a filter that has dependencies injected:
// Gets dependencies injected
class SomeFilter : IActionFilter { ... }

SomeFilter just uses the typical approach of getting the SomeAttribute attribute from the controller or action method via GetCustomAttributes() to do whatever work is needed.
You can then use ExtensibleActionInvoker to wire up the filter:
builder.RegisterControllers(...).InjectActionInvoker();
builder.RegisterType<ExtensibleActionInvoker>().As<IActionInvoker>();
builder.RegisterType<SomeFilter>().As<IActionFilter>();

It might be a little more code than you'd write using the attribute-as-filter approach, but the quality of the code will be better in the long run (e.g. by avoiding the limitations of attributes and the awkwardness of the Service Locator solutions.)",https://stackoverflow.com/questions/4163217/how-do-i-resolve-dependency-injection-in-mvc-filter-attributes,4,0
Nodejs - It is wrong to mix Promise based architecture with events,"I'm working on a nodejs app that will become larger. I don't find a lot of ressources for advanced Nodejs project architecture and structure.
I would like to know if it's an antipattern to mix paradigm of promises with Events in NodeJS.
If someone have ressources about advanced project architecture or open source large Nodejs projects, it could really help me.
Best regards.","It is not wrong to mix Promises and callbacks. Sometimes it is necessary, especially if a library or existing code only has one or the other. Callbacks and Promises are just ways of handling the asynchronous nature of NodeJS, so just be aware of controlling the flow of your program.
Some other thoughts:

I find it easier to use Promises inside of callback functions, as you can use the Promise resolution to determine when and how to call the callback
If possible, promisify your callbacks (or even other callback libraries) if you can! That might be a personal preference of mine since I like promises more. Some libraries even allow you to go from Promises to callbacks, but I haven't done that.
The ES6 async/await stuff changes things again! So keep adapting as you are able, or as you need to.
There is nothing wrong with callbacks or Promises. No wrong choice and it's ok to mix, just keep control flow in mind.",https://stackoverflow.com/questions/43666815/nodejs-it-is-wrong-to-mix-promise-based-architecture-with-events,1,0
"Azure Functions [JavaScript / Node.js] - HTTP call, good practices","From my Azure Function (which runs in Node.js, triggered by EventHub message) I would like to make a post request to some external page.
Something like:
module.exports = function (context, eventHubMessages) {

var http = require(""http"");

context.log('JavaScript Function triggered by eventHub messages ');

http.request(post_options, function(res){
    ...
})

context.done();

The code above will probably work but I have a doubt if that is not an antipattern.
Imagine situation when there are thousands of functions triggered in short period of time - for each execution we would need to create an HTTP client and create a connection...
From short research I have found some solution proposal for C# Azure Functions: https://docs.microsoft.com/en-us/azure/architecture/antipatterns/improper-instantiation/
which uses static HttpClient class.
I have a question, is there any similar approach in Node.js Azure Function? Or any other way to avoid this problem, to share an object between Node.js Azure Function executions?","If thousands of functions triggered in short period of time you should limit the sockets by modifying the http.globalAgent or by passing an instance of a new Agent 

An Agent is responsible for managing connection persistence and reuse
  for HTTP clients. It maintains a queue of pending requests for a given
  host and port, reusing a single socket connection for each until the
  queue is empty, at which time the socket is either destroyed or put
  into a pool where it is kept to be used again for requests to the same
  host and port. Whether it is destroyed or pooled depends on the
  keepAlive option.

Source: https://nodejs.org/api/http.html#http_class_http_agent
http.globalAgent.maxSockets defaults to infinity so unless you limit this value your function will run out of sockets and you'll see your requests start to fail.
Additionally if you are planning on connecting to the same host you should enable keep-alive on the globalAgent/Agent to enable pooled connections.",https://stackoverflow.com/questions/47906463/azure-functions-javascript-node-js-http-call-good-practices,1,0
Authentication Handling using HTTPClient,"I have a class which connects to a Web API, therefore I am initialising a static HTTPClient at top of the class like this
private static readonly HttpClient httpClient = new HttpClient();

https://docs.microsoft.com/enus/azure/architecture/antipatterns/improper-instantiation/
This HTTPClient is used by all public methods within the class to contact the API, each method except login() requires a basic authentication header, this header should be in the format:
Authorization: Basic device_id:X-Secret-Key
Where the device_id is a constant for this instance of the class and the secret key a return from the login() method.
Therefore should every method contain:
request.Headers.Add(""Authorization"", ""Basic "" + Convert.ToBase64String(Encoding.ASCII.GetBytes(authInfo)));

Where request is the HTTPRequestMessage being created and authInfo is a string in the format device_id:X-Secret-Key.
Or should a every method call a seperate HTTPClient from the one used by the Login() function, declared like:
var handler = new HttpClientHandler();
handler.Credentials = new NetworkCredential (device_id, secret_key);
var client = new HttpClient (handler);

Thank you for any responses","The Authorization header, add it to the httpClient:
httpClient.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(""Basic"", Convert.ToBase64String(Encoding.ASCII.GetBytes(authInfo)));

After you add this header ONCE, every future calls to the WEB API service should be authorized.",https://stackoverflow.com/questions/45609530/authentication-handling-using-httpclient,1,0
CurrentConnections exceeds ConnectionLimit in ServicePoint,"I have a cluster/server with multiple nodes that handle requests from the application.
The application the user is running, opens 2  web clients with the following URLs:

https://myprocess.myapp.com/api/json/v1
https://myprocess.myapp.com/api/protobuf/v1

In order to support stickiness (I want each of the 2 web clients will keep the connection against the nodes - persistence),
the ConnectionLeaseTimeout stayed with the default value, which means ""don't close the connection""
and because the DefaultPersistentConnectionLimit is 2 by default, I set  the DefaultConnectionLimit to 1.
The problem:

servicePoint.CurrentConnections is 2, although servicePoint.ConnectionLimit is 1.
In the server, I see that the remote host port is changing, i.e. I see more than 2 ports (more than 1 port for each client).

What am I doing wrong?
My class output:
CommunicationWebClient https://myprocess.myapp.com/api/json/v1
CommunicationWebClient https://myprocess.myapp.com/api/protobuf/v1
SendAsync _uri=https://myprocess.myapp.com/api/json/v1 servicePoint.Address=https://myprocess.myapp.com/api/json/v1 servicePoint.ConnectionLimit=1 servicePoint.CurrentConnections=2
...
SendAsync _uri=https://myprocess.myapp.com/api/protobuf/v1 servicePoint.Address=https://myprocess.myapp.com/api/json/v1 servicePoint.ConnectionLimit=1 servicePoint.CurrentConnections=2
...
public sealed class CommunicationWebClient : IDisposable
{
    private HttpClient _httpClient;
    private Uri _uri;

    public CommunicationWebClient(Uri uri)
    {
        Logger.Debug($""{nameof(CommunicationWebClient)} {nameof(uri)}={uri}"");

        _uri = uri;

        ServicePointManager.DefaultConnectionLimit = 1;

        _httpClient = new HttpClient(new WebRequestHandler())
        {
            Timeout = 10.Minutes(),
        };
    }

    public void Dispose()
    {
        _httpClient.Dispose();
    }


    public async Task SendAsync(
        ByteArrayContent content)
    {
        var servicePoint = ServicePointManager.FindServicePoint(_uri);

        Logger.Debug($""{nameof(SendAsync)} "" +
                    $""{nameof(_uri)}={_uri} "" +
                    $""{nameof(servicePoint.Address)}={servicePoint.Address} "" +
                    $""{nameof(servicePoint.ConnectionLimit)}={servicePoint.ConnectionLimit} "" +
                    $""{nameof(servicePoint.CurrentConnections)}={servicePoint.CurrentConnections}"");

        using (var httpResponseMessage = await _httpClient.PostAsync(_uri, content))
        {
            ...
        }
    }
}","If you still have a problem, check that your CommunicationWebClient is not disposed too often. It disposes HttpClient but it behaves not as usually people expect.
Checkout this article: https://docs.microsoft.com/en-us/azure/architecture/antipatterns/improper-instantiation/
Shortly speaking when you dispose HttpClient in case of windows, you ask windows to close all opened sockets. But by default windows have 4 minute timeout to completely close the socket. So for all these 4 minutes there will be a connection between your HttpClient and web server.",https://stackoverflow.com/questions/46226287/currentconnections-exceeds-connectionlimit-in-servicepoint/50233560,1,0
New Design Patterns,is there any new design patterns available other than the patterns covered by GoF book and Head First Design Patterns? Have any one of you used your own design patterns in your projects? Please let me know. if possible give some UML Diagrams. Thanks in advance.,"Have you come across the concept of Anti-Patterns yet?  I came across these in the book ""AntiPatterns: Refactoring Software, Architectures, and Projects in Crisis"" - which I feel is a great read (note: this is a personal recommendation).  This book reviews commonly identified mistakes in software projects and then attempts to provide a resolution, quite often using a GoF pattern.  Interestingly many of the anti-patterns seem to originate in a design pattern being incorrectly applied.",https://stackoverflow.com/questions/3607939/new-design-patterns/3622542,4,0
Please suggest an e-book for anti design pattern?,"I would like to know ""anti-design"". Please suggest an e-book for anti design pattern.","You may like to check out:
The daily wtf & Anti patterns",https://stackoverflow.com/questions/6381744/please-suggest-an-e-book-for-anti-design-pattern/6381821,4,0
How do I properly implement DOM manipulation in Angular?,"I have recently been assigned to take over and clean up an Angular project that is already complete and in production. This is my first time using Angular.
Everything I've read so far on Angular...

https://www.airpair.com/angularjs/posts/top-10-mistakes-angularjs-developers-make
http://nathanleclaire.com/blog/2014/04/19/5-angularjs-antipatterns-and-pitfalls/
http://kirkbushell.me/when-to-use-directives-controllers-or-services-in-angular/

...Seem to all say the same thing about an Angular app's architecture. Most notably that:

Controllers should never do DOM manipulation or hold DOM selectors; that's where directives and using ng-model come in.

However, the project I've been assigned seems to completely ignore this. For example, an excerpt from the MenuController:
(function() {
  app.controller('MenuController', function($scope) {
    ...

    $scope.openMenu = function () {
        $('.off-canvas-wrap').addClass('offcanvas-overlap-right');
    };

    ...
  });
}());

Should I move this code (and a lot of other code) to directives? Or should I follow the pattern the application has already established and continue doing DOM manipulation in the controllers?","Yes. Code that fiddles with the DOM has no business in the angular controller and belongs in a directive.
One other advantage of using angular is that there are many good ready made directives available. So if you are going to refactor things anyways, you may want to see if you can speed things up by using other people's directives.
That said, it sounds like a lot of work. You'll want to inform whoever is paying about this and discuss the pros and cons of refactoring with them.",https://stackoverflow.com/questions/29720389/how-do-i-properly-implement-dom-manipulation-in-angular,2,0
Where to find resources on Refactoring? [closed],"Closed. This question is off-topic. It is not currently accepting answers.
                            
                        










Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                        
Closed last year.



Refactoring is the process of improving the existing system design without changing its behavior.
Besides Martin Fowler's seminal book ""Refactoring - Improving the design of existing code"" and Joshua Kerievsky's book ""Refactoring to Patterns"", are there any good resources on refactoring?","http://www.refactoring.com/ might help you. They have a long list of methods here:

http://www.refactoring.com/catalog/index.html

Joel's article Rub a dub dub shows you why you should refactor and not rewrite (but I guess you already knew that rewriting is a thing you should never do..)",https://stackoverflow.com/questions/48817/where-to-find-resources-on-refactoring/817330,9,0
Create relationship among the functions of buttons in Python,"I want to import 3 csv files using 3 buttons separately and merge them into one csv file and save it using a button click. 
This is my code:
    from Tkinter import *
from Tkinter import Tk
from tkFileDialog import askopenfilename
import pandas as pd
from tkFileDialog import asksaveasfilename
import time

class Window(Tk):
    def __init__(self, parent):
        Tk.__init__(self, parent)
        self.parent = parent
        self.initialize()

    def initialize(self):
        self.geometry(""600x400+30+30"")

        self.wButton = Button(self, text='Product Price List', command = self.OnButtonClick)
        self.wButton.pack()


    def OnButtonClick(self):
        self.top = Toplevel()
        self.top.title(""Product Price List"")
        self.top.geometry(""300x150+30+30"")
        self.top.transient(self)
        self.wButton.config(state='disabled')

        self.topButton = Button(self.top, text=""Import Price list CSV"", command = self.OnImport1)
        self.topButton.pack()

        self.topButton = Button(self.top, text=""Import Price Adjustment CSV"", command = self.OnImport2)
        self.topButton.pack()

        self.topButton = Button(self.top, text=""Import Price Adjustment CSV"", command = self.OnImport3)
        self.topButton.pack()

        self.topButton = Button(self.top, text=""Save As"", command = self.OnSaveAs)
        self.topButton.pack()

        self.topButton = Button(self.top, text=""CLOSE"", command = self.OnChildClose)
        self.topButton.pack()

        def OnImport1(self):
            a = askopenfilename()
        def OnImport2(self):
            b = askopenfilename()
            c = a.OnImport1.merge(b, how='left', left_on='Dynamic_spcMatrix', right_on='Dynamic_spcMatrix' )
        def OnImport3(self):
            d = askopenfilename()
            d = d.dropna(axis=0)
            g = d.groupby('Dynamic_spcMatrix')['Attribute_spcName'].apply(lambda x: ', '.join(x.astype(str))) #join attributes usin commas
            c['Attribute_spcName'] = c['Dynamic_spcMatrix'].map(g)
            c = c[['Type', 'Name', 'Currency_spcCode', 'Product_spcCfg_spcModel_spcId', 'Product_spcName', 'Attribute_spcName', 'Matrix_spcType', 'Start_spcDate', 'End_spcDate', 'Original_spcList_spcPrice', 'Max_spcSale_spcPrice', 'Min_spcSale_spcPrice', 'String_spcMatrix_spcColumn_spc1', 'String_spcMatrix_spcColumn_spc2', 'String_spcMatrix_spcColumn_spc3', 'String_spcMatrix_spcColumn_spc4','Number_spcMatrix_spcColumn_spc1']]
        def OnSaveAs(self):
            dlg = asksaveasfilename(confirmoverwrite=False)
            fname = dlg
            if fname != '':
                f = open(fname, ""a"")
                new_text = time.time()
                f.write(str(new_text)+'\n')
                f.close()     
            c.to_csv(fname, index=False)


    def OnChildClose(self):
        self.wButton.config(state='normal')
        self.top.destroy()

if __name__ == ""__main__"":
    window = Window(None)

    window.title(""Create Csv"")

    window.mainloop()

I want to import 3 csv files using 3 buttons separately and merge them into one csv file and save it using a button click.
When I run this following error occurs.
Exception in Tkinter callback
Traceback (most recent call last):
  File ""C:\Users\tt20172129\AppData\Local\Continuum\anaconda2\lib\lib-tk\Tkinter.py"", line 1541, in __call__
    return self.func(*args)
  File ""<ipython-input-15-64436dd12913>"", line 51, in OnImport2
    c = a.OnImport1.merge(b, how='left', left_on='Dynamic_spcMatrix', right_on='Dynamic_spcMatrix' )
NameError: global name 'a' is not defined

I am new to python and also to coding.  Hopefully there is someone that can help me so that I can learn. :)","Shripal Gandhi is fundamenatally correct, a does not have scope outside of the method on Import1.  That is why the code does not work.
His solution - a global - works - but is crude and a great antipattern of why to use a global.
You have to make a architectural decision here.
You have to store the results of a = askopenfilename(), instead of dropping into the global scope, simply keep it at the class scope.  Simply had self. in front of a, b, and c, and then you can reference them from any of your other class methods including when you want to write it out (the next time this bug would have showed up)
def OnImport1(self): 
     self.a = askopenfilename()
def OnImport2(self):
    self.b = askopenfilename()

def OnImport3(self):

    self.c = self.a.OnImport1.merge(self.b, how='left', left_on='Dynamic_spcMatrix', right_on='Dynamic_spcMatrix' )",https://stackoverflow.com/questions/49422041/create-relationship-among-the-functions-of-buttons-in-python/49429758,2,0
State management approach using react,"I have been using react for a while and at time I find it to be cumbersome to write action and reducers for simple tasks. The approach i use in such cases is as follows 

I find this approach to be effective and less cumbersome in implementing. Any thoughts and drawbacks of this approach.","React doesn't define how you should build your app and doesn't suggest a ""default"" architecture like some other frameworks (e. g. backbone) do. Also, redux is not a necessary part of react. It is used to prevent state from distributing across multiple components - but local state is not an antipattern and you indeed could/should have some. See this article from a redux co-author for details.
Regarding the architecture you you provided on the pic, well, this looks like classic MVC.

""Data provider component"" = model
""Smart component"" = controller
""Dumb component"" = view

Again, react doesn't define what is wrong and what is right. Your idea looks fine. With proper implementation it could be very clear to comprehend.",https://stackoverflow.com/questions/49107940/state-management-approach-using-react/49113016,1,0
why the argument of a function are register type,"sorry if my accent hurts
In C Programming, you can declare a function like this:
dennis ritche 2nd ed page 84 
 f(register unsigned m,register long n)

Most of the books say that the default storage type of function arguments are preferably register.
Why so? What is the need?
answer followed compiler keeps them with cpu registers but
I've not seen any difference in execution speed when using register type 
Can any one explain in detail why and where and when exactly the register is used in C Programming?
thanks","I'd be interested in seeing which books say this.  
To be honest, at this point the state of compiler technology, there is almost never any use to declaring something with a register modifier.  In fact, many compilers will simply throw this out, and do whatever they want.  The point is that this varies so much across architectures, compilers, and is also affected by what kinds of other optimization are done by the compiler.  So, the choice of whether or not to store something in a register is a choice better left to the compiler and the use of the register modifier is usually considered an archaic antipattern.
EDIT: I incorrectly remembered register was a qualifier (which doesn't line up with the use of a type qualifier anyway, so I should have known), fixed...",https://stackoverflow.com/questions/10991793/why-the-argument-of-a-function-are-register-type/10999218,1,0
Installing ruby gems in a npm project,"I have a node.js project that depends on a ruby gem. Is it possible somehow to create an installation process that installs the gem dependencies in an easy way? Ideally, when I do
npm install

to install the npm dependencies, it would be great if the required ruby gems were installed as well.
Is there some kind of bridge between the two? If not, has anyone investigated this and found a suggested best practice or work around in these situations?","Theoretically, npm-scripts provides the facilities to run scripts during npm install. You could for example add these lines to your package.json:
{ ""scripts"" :
  { ""preinstall"" : ""/usr/bin/env gem install some_gem_name"" }
}

Of course, you may want to add a more complex script that handles the case where Ruby and/or Rubygems are not installed, the Gem installation fails etc. etc. Installing dependencies can get arbitrarily complex, which is why many package developers (in any given language) often just assume that the required dependencies are already up and running on the target system. Finally, the npm-scripts documentation states that

INSTALL SCRIPTS ARE AN ANTIPATTERN

and

The only valid use of install or preinstall scripts is for compilation which must be done on the target architecture.

All in all, I suggest that you instead focus your energy on adding proper installation instructions to your Readme.",https://stackoverflow.com/questions/23741174/installing-ruby-gems-in-a-npm-project/23745832,2,0
Structure of a program with many global variables [closed],"Closed. This question is opinion-based. It is not currently accepting answers.
                            
                        










Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.
                        
Closed 4 years ago.



As the title suggests, I'm interested in the best (perhaps the most Pythonic way) to structure a program which uses many global variables.
First of all, by ""many"", I mean some 30 variables (which may be dictionaries, floats or strings) which every module of my program needs to access. Now, there seem to be two ways to do this:

define the ""global"" variables in seperate modules
use an object oriented approach

The advantage of using an object oriented approach is that I can have many instances of some main class initialized, and perhaps compare different values (results of some analysis, for example) later on.
I already have a program written, but basically it breaks down to one class with some 30 or so attributes. Although it works fine, I'm aware this is a pretty messy way to do this.
So, basically, is I use OOP approach, I would perhaps need to break my main class down to a few subclasses, every one of which stores specific logically related variables.
Any suggestions are welcome.
P.S. Just to be concrete about what I'm trying to do: I have a FEM-solver which needs to store structure info, element and node data, analysis result data, etc. So, I'm dealing with a lot of data types most of which are connected in some way.","Unfortunately, as was hinted at in the comments, there is no ""Pythonic"" way to do this. Having a large number of global constants is just fine - many programs and libraries do this. But in the comments, you've specified that all of your globals are being modified.
You need to take your program's architecture back to the drawing board. Rethink the relationships between your program's entities (functions, classes, modules, etc). There has to be a better way to organize it.
And by the way, it also sounds like you're getting close to using the God Object Antipattern. Use some of the advice in this SO question to refactor your massive class that has it's fingers all over your program.",https://stackoverflow.com/questions/32799503/structure-of-a-program-with-many-global-variables/32799802,1,0
How to use OOP efficiently? Can I get some reference [closed],"Closed. This question is off-topic. It is not currently accepting answers.
                            
                        










Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                        
Closed 4 years ago.



I dont know if this question is suitable here but I recently attended interview in which I answered all of the typical OOP FAQ. But at the end interviewer told me that I know OOP concepts but dont know how to apply them in real application. So I just want to know if there is any good book available for me to understand and apply OOP concepts in real application example. I have searched everywhere and I finally came here to ask. Thank you in advance.","Here are some good books that helped me:

Design Patterns - Elements of Reusable Object-Oriented Software
Object Design - Roles, Responsibilities, and Collaborations - Rebecca Wirfs-Brock
Analysis Patterns Reusable Object Models
AntiPatterns, Refactoring Software, Architectures, and Projects in Crisis
Clean Code - A Handbook of Agile Software Craftsmanship
Head First Object-Oriented Analysis and Design

I would also say that it's best to read these in conjunction with doing a real project. If you don't have a real project, actually code the ones in the books as they go along. It may seem unnecessary, but your fingers learning is just as important as your head learning! That's ""practical knowledge""; sitting down to code and not hesitating b/c your hands know what to do.
Hope it helps",https://stackoverflow.com/questions/33873367/how-to-use-oop-efficiently-can-i-get-some-reference/33874754,1,0
Hierarchical SQL Query,"I'm working on a simple CMS system for which I have a database with the following tables:
Items
Contents
Langs

The Items table has the following structure:
itemId
name (for semantic reasons)
type
parent (foreign key to itemId)

An item can be either a document or a section type. A section is a piece of content on a document which is linked to it via the parent collumn. But also a document can have a parent which makes it a subpage.
Now I get stuck on making a query to fetch all the items from the database hierarchically. So something like this:
documentId => name
              metaDescription => language => meta
              sections => sectionId => language => title
                                                   content
                                                   uri
              subPages => documentId => name
                                        metaDescription
                                        sections => etc...

Just to clarify, a website can have multiple languages which are in the Langs table and every language is linked to a piece of content in the Contents table which is also linked to an item in the Items table. The metaDescription is the linked content collumn linked to a item of type document.
Is there a way to do this with one query? This was my first attempt, but it doesnt work for subPages:
    SELECT
        documents.itemId        AS id,
        documents.name          AS documentName,
        documents.lastModified  AS lastModified,
        meta.content            AS metaDescription,
        meta.uri                AS documentUri,
        sections.itemId         AS sectionId,
        sections.name           AS sectionName,
        sections.lastModified   AS sectionLastModified,
        contents.name           AS sectionTitle,
        contents.content        AS sectionContent,
        contents.uri            AS contentUri,
        contents.lastModified   AS contentLastModified,
        langs.name              AS contentLang
    FROM 
        SITENAME_kw_items AS documents
            INNER JOIN
        SITENAME_kw_contents AS meta
        ON documents.itemId = meta.itemId
            INNER JOIN
        SITENAME_kw_items AS sections
        ON sections.parent = documents.itemId
            INNER JOIN
        SITENAME_kw_contents AS contents
        ON sections.itemId = contents.itemId
            INNER JOIN
        SITENAME_kw_langs AS langs
        ON langs.langId = contents.langId

Sorry for the long question. Hope you guys can help!","Below is how I do it in ""our"" DMS (recursive CTE), which is Adam Gent's suggestion expanded.
Note that I just see one could use COALESCE instead of nesting ISNULL.
The order by you would do according to the breadcrumbs (here Bez_Path or UID_Path).
A far better way would be to use a closure-table architecture.
See here: 
http://dirtsimple.org/2010/11/simplest-way-to-do-tree-based-queries.html
and here:
http://www.mysqlperformanceblog.com/2011/02/14/moving-subtrees-in-closure-table/
The closure table also has the advantage that it works on MySQL, where CTE & recursion are not supported.
Also note that closure tables are much better (and simpler and faster to query) than recursion.
Also think about symlinks in such a structure. 
The something_UID, something_parent_UID pattern (as shown below) is almost always an antipattern.
CREATE VIEW [dbo].[V_DMS_Navigation_Structure]
AS 
SELECT 
     NAV_UID 
    ,NAV_Typ 
    ,NAV_Parent_UID 
    ,NAV_Stufe 
    ,NAV_ApertureKey 
    ,NAV_Nr 
    --,NAV_Bemerkung 
    ,NAV_Status 
    ,NAV_Referenz 

    ,ISNULL(PJ_Bezeichnung, ISNULL(FO_Bezeichnung, DOC_Bezeichnung + '.' + DOC_Dateiendung)  ) AS NAV_Bezeichnung 
    ,NAV_PJ_UID 
    ,NAV_FO_UID 
    ,NAV_DOC_UID 
    ,ISNULL(NAV_PJ_UID, ISNULL(NAV_FO_UID,NAV_DOC_UID)) AS NAV_OBJ_UID 
FROM T_DMS_Navigation 

LEFT JOIN T_DMS_Projekt 
    ON T_DMS_Projekt.PJ_UID = T_DMS_Navigation.NAV_PJ_UID 

LEFT JOIN T_DMS_Folder 
    ON T_DMS_Folder.FO_UID = T_DMS_Navigation.NAV_FO_UID 

LEFT JOIN T_DMS_Dokument 
    ON T_DMS_Dokument.DOC_UID = T_DMS_Navigation.NAV_DOC_UID 








CREATE VIEW [dbo].[V_DMS_Navigation_Structure_Path]
AS 
WITH Tree 
(
     NAV_UID
    ,NAV_Bezeichnung
    ,NAV_Parent_UID
    ,Depth
    ,Sort
    ,Bez_Path
    ,UID_Path
    ,PJ_UID
    ,FO_UID
    ,DOC_UID
    ,OBJ_UID
) 
AS
(
    SELECT 
         NAV_UID 
        ,NAV_Bezeichnung 
        ,NAV_Parent_UID 
        ,0 AS Depth 
        ,CAST('0' AS varchar(10)) AS Sort 
        ,CAST(NAV_Bezeichnung AS varchar(4000)) AS Bez_Path 
        ,CAST(NAV_OBJ_UID AS varchar(4000)) AS UID_Path 
        ,NAV_PJ_UID AS PJ_UID 
        ,NAV_FO_UID AS FO_UID 
        ,NAV_DOC_UID AS DOC_UID 
        ,NAV_OBJ_UID AS OBJ_UID 
    FROM V_DMS_Navigation_Structure 

    WHERE NAV_Parent_UID IS NULL 

    UNION ALL 

    SELECT 
         CT.NAV_UID 
        ,CT.NAV_Bezeichnung 
        ,CT.NAV_Parent_UID 
        ,Parent.Depth + 1 AS Depth 
        ,CONVERT(varchar(10), Parent.Sort + '.' + CAST(Parent.Depth + 1 AS varchar(10))) AS Sort 
        ,CONVERT(varchar(4000), Parent.Bez_Path + '\' + CAST(CT.NAV_Bezeichnung AS varchar(1000))) AS Bez_Path 
        ,CONVERT(varchar(4000), Parent.UID_Path + '\' + CAST(CT.NAV_OBJ_UID AS varchar(1000))) AS UID_Path 
        ,NAV_PJ_UID AS PJ_UID 
        ,NAV_FO_UID AS FO_UID 
        ,NAV_DOC_UID AS DOC_UID 
        ,NAV_OBJ_UID AS OBJ_UID 
    FROM V_DMS_Navigation_Structure CT 

    INNER JOIN Tree AS Parent 
        ON Parent.NAV_UID = CT.NAV_Parent_UID
)

SELECT TOP 999999999999999 * FROM Tree
ORDER BY Depth",https://stackoverflow.com/questions/17213056/hierarchical-sql-query/17213537,2,0
Interface-oriented: Fragmentized interfaces to represent a data structure,"I'm inspired from encapsulation achieved by interfaces. Interface is a powerful tool when objects shares common properties and methods. Interface-oriented designs are designs that decouple entities and the manager that are managing them. They do not need to know any of the implementation  details and the exact type, and a typical good interface design is that object that passed through the method call could be hot-plugged at runtime.
So I managed to turn my objects to expose through interfaces.
Suppose I have a LoginModule, which contains the business logic to handle the submitted Form by Client. For example, the client may either login through enter Username and Password or with their MobileNo
public class UserLoginForm
{
  public string Username { get; set; }
  public string Password { get; set; }
}
public class MobileLoginForm
{
  public string MobileNo { get; set; }
  public string Password { get; set; }
}

Then the business logic layer is going to take a look at the request.
public class LoginModule
{
  public int LoginByUsername(UserLoginForm form)
  {
    // DoSomething, such as CheckDbIfExist(form.Username);
    // return the result, e.g. -1 stand for wrong password
  }
   public int LoginByMobileNo(MobileLoginForm form)
  {
    // DoSomething, such as CheckDbIfExist(form.Username);
    // return the result, e.g. -1 stand for wrong password
  }
}

In this example, the two logics are so similar that LoginByMobileNo could eventually direct to call LoginByUserName after getting the Username by looking up MobileNo in the database.
But you know, it just couldn't because we need a UserLoginForm to call it, or we just create a separated method Login(string username, string password) to let both login method to call it.
The actual consideration is not on how to reconstruct these two methods, as there are many other types of request, something like CokeRequest that request a coke delivery to int RoomNo and int Count... ...
public interface ILoginModule
{
   void LoginByUsername(XXX xxx);
}

First Question: What is that XXX?
This interface have no access to UserLoginForm, he don't know the existance of that form anyway. If I am doing the interface design correctly, XXX should be an interface that he could know its existance.
public interface ILoginModule
{
   void LoginByUsername(IUserLogin login);
   void LoginByMobile(IMobileLogin login);
}
// LoginModule now implement ILoginModule

public interface IUserLogin
{
   string UserName { get; set; }
   string Password { get; set; }
}
public interface IMobileLogin
{
   string MobileNo { get; set; }
   string Password { get; set; }
}
// The the two Forms now implement the respective interface

By then the nightmare begun. I find to needing a dedicated interface for every Request that the client made

An interesting observation:
Service interfaces are Method-only interfaces
Request interfaces are Property-only interfaces

I do not believe it is the right way to do deal with interfaces.
Neither if we fragmentize the interfaces.
public interface ILoginModule
{
   void LoginByUsername(IUserLogin login);
   void LoginByMobile(IMobileLogin login);
}
// LoginModule now implement ILoginModule

public interface IPassword
{
   string Password { get; set; }
}
public interface IUserLogin : IPassword
{
   string UserName { get; set; }
}
public interface IMobileLogin : IPassword
{
   string MobileNo { get; set; }
}
// The the two Forms now implement the respective interface

Because we are still needing hundreds of interface (which is of same variety as the requests to be)
Nor
public interface ILoginModule
{
   void LoginByUsername(IUser login1, IPassword login2);
   void LoginByMobile(IMobile login, IPassword login2);
}
// LoginModule now implement ILoginModule

public interface IPassword
{
   string Password { get; set; }
}
public interface IUser
{
   string UserName { get; set; }
}
public interface IMobile
{
   string MobileNo { get; set; }
}
// The the two Forms now implement the respective interface

It smells bad because we are no different from passing the string content one by one. If the request contains 8 param, then the method becomes DoWork(form,form,form,form,form,form,form,form)
I really can't figure out why I'm falling into this irony. If I have n types of requests, it is normal to give them to m modules to handle, it still n methods in total. If we use interface, it adds n interfaces to represent n request, and this didn't help to hot-plug data at anytime because each interface get only one single implementation.
In this situation the LoginModule can't even benefit from it: It could not call LoginByUsername after finding out the Username which corresponds to MobileNo at any circumstances...
Most interface tutorial is taking about car and bus and racecar in which they void StartEngine()...While seldom for architecture, or communication between layers, just for instance, the MVC model.
I'm not saying interfaces are bad, but I need a way to correctly implement it or should I implement it in this situation. Any misused patterns are antipattern with no single doubt.
I hope to hear any voices.","After hours of consideration, it was found the starting point was problematic, leading to problematic solutions.
The answer of using interfaces to represent a data structure is nope. It is useless to declare property-only interfaces.
The request submitted by the client is anemic when it comes to the business logic just take its properties and do the work.
That says, ILoginModule that defines void LoginWithMobile(string MobileNo, string Password) is already enough for handling the request without passing the Request itself as a parameter.

An interface is useful for abstraction of ""what it does"" instead of
  ""what it contains"". Therefore all objects that aims to store data
  without itself managing them should not implement any interface.",https://stackoverflow.com/questions/48488503/interface-oriented-fragmentized-interfaces-to-represent-a-data-structure,1,0
Web apps architecture: 1 or n API,"Background:
I'm thinking about web application organisation. I will separate front (web site for browser) from back (API): 2 apps, 2 repository, 2 hosting. Front will call API for almost everything.
So, if I have two separate domain services with my API (example: learning context and booking context) with no direct link between them, should I build 2 API (with 2 repository, 2 build process, etc.) ? Is it a good practice to build n APIs for n needs or one ""big"" API ? I'm speaking about a substantial web app with trafic.
(I hope this question will not be closed as not constructive... I think it's a real question for a concrete case, sorry if not. This question and some other about architecture were not closed so there is hope for mine)","It all depends on the application you are working on, its business needs, priorities you have and so on. Generally you have several options:

Stay with one monolithic application
Stay with one monolithic application but decouple domain model across separate modules/bundles/libraries
Create distributed architecture (like Service Oriented Architecture (SOA) or Event Driven Architecture (EDA))

One monolithic application
It's the easiest and the cheapest way to develop application on its beggining stage. You don't have to worry about complex architecture, complex deployment and development process. It also works better if there are no many developers around.
Once the application is growing up, this model begins to be problematic. You can't deploy modules separatly, the app is more exposed to antipatterns, spaghetti code/design (especially when a lot people working on it). QA process takes more and more time, which may make it unusable on CI basis. Introducing approaches like Continuous Integration/Delivery/Deployment is also much much harder.
Within this approach you have one repo/build process for all your APIs,
One monolithic application but decouple domain model
Within this approach you still have one big platform, but you connect logicaly separate modules on 3rd party basis. For example you may extract one module and create a library from it. 
Thanks to that you are able to introduce separate processes (QA, dev) for different libraries but you still have to deploy whole application at once. It also helps you avoid antipatterns, but it may be hard to keep backward compatibility across libraries within the application lifespan.
Regarding your question, in this way you have separate API, dev process and repository for each ""type of actions"" as long as you move its domain logic to separate library.
Distributed architecture (SOA / EDA)
SOA has a lot profits. You can introduce completely different processes for each service: dev, QA, deploying. You can deploy just one service at once. You also can use different technolgies for different purposes. QA process gets more reliable as it involves smaller projects. You can version communication (API) between services which makes them even more independent. Moreover you have better abilitly to scale horizontaly.
On the other hand complexity of the high level architecture grows. You have much more different components you have to take care: authentication / authorisation between services, security, service discovering, distributed transactions etc. If your application is data driven (separate frontend which use APIs for consuming data) and particular services don't need to communicate to each other - it may be not as much complicate (but such assumption is IMO quite risky, sooner or letter you will need to communicate them).
In that approach you have separate API, with separate repositories and separate processes for each ""type of actions"" (which I understand ss separate domain model / services).

As I wrote on the beggining the way you choose depends on the applicatoin and its needs. Anyway, back to your original question, my suggestion is to keep APIs as separate as you can. Even if you have one monolithic application you should be able to version APIs separatly and keep their domain logic separate. Separating repositories and/or processes depends on the approach you choose (eg. among these I mentioned before).
If I missed your point, please describe in more detailed way what answer do you expect.
Best!",https://stackoverflow.com/questions/25907009/web-apps-architecture-1-or-n-api/25948594,1,0
MVC EF Context Instances,"I apologise if my question seems dumb, I have tried Googling, and not found what I am looking for, so any advice would be appreciated.
I am fairly new to the Idea of MVC, I have been doing Web Forms development for a number of years, but I wanted to give something else a try.
I am using Entity Framework (6) for a database connection, and MVC4 for a web front end.
My question is, how should the DB context instance be handles.
My Controller Action code that is working is this
public ViewResult List(int buildingId)
{
    var model = new Data.Model();
    var query = from r in model.Rooms
                where r.Building.Id == buildingId
                select r;
    /*
    var q2 = model.Buildings.Where(b => b.Id == buildingId).SelectMany(b => b.Rooms);
    var q3 = model.Buildings.Where(b => b.Id == buildingId).First().Rooms;
    */

    return View(query);
}

The commented lines are just other ways I could get the query results I am looking for.
Data.Model is the EF Db context.
What I don't like about this is that the context is disposable, and I am not disposing it. In my mind this is lazy and bad practice.
I have tested with model begin disposed, the first change that I require is to return a list of the query, which I don't mind, but then because the context is disposed, on the view I cannot access properties like @item.Building.Description. So if disposing I would need to return exactly what I was to display on the view (there are multiple ways I could do that, so I am not very worried about how).
Another option would be to have a static/shared context somewhere in the project, so all database requests use the same context instance. This is nice from the side that it would then only be using one DB connection, but EF might already be handling that for me, so I wouldn't want to go against using the way EF was designed, if this is how it should be.
So, my question is, what would be considered best practices?

Keep working like I am, instantiating a new context, and not disposing.
Dispose the context each time, and make sure I return anything I need visible on the view.
Use a class that will return a static context instance if it is already instantiated.

Thank you","Generally I would recommend to use a more architectured approach and use a separate (data-providing) layer for fetching data from the database. In that case an IoC (inversion on control) container (such as Ninject, Unity, etc.) could handle object lifetime for you, but if you haven't used Dependency Injection patterns before, you have a lot to investigate first.
Having said that, the simple answer is that you should definitely dispose your context objects as soon as possible (in general they should be as shortlived as possible). And the common pattern is 
using (var model = new Data.Model())
{
    var buildings = model.Rooms.Where(r => r.Building.Id == buildingId).ToList();
}

You fetch the data right away by calling ToList() and dispose of the context. This avoids the problems you've encountered when returning a query and passing it to the view. As you've noticed, it gets really tricky to control when exactly your DB-query gets executed and when to dispose of the context.
Another option would be to have a static/shared context somewhere in the project, so all database requests use the same context instance - this is a horrible idea and a well-known bad practice/antipattern.
So to sum up, the best practice is working as you describe in Option 2 in your list: Dispose the context each time, and make sure I return anything I need visible on the view.
And as a sidenote, it is also a good practice to use a ViewModel per View (see ASP.NET MVC - How exactly to use View Models)",https://stackoverflow.com/questions/21524426/mvc-ef-context-instances/21525157,1,0
Java Web Application for 5000~ Users,"For the first time (hopefully not the last) in my life I will be developing an application that will have to handle a high number of users (around 5000) and manage lots of data. I have developed an application that manages lots of data (around 100~ GB of data, not so much by many of your standards), but the user count was pretty low (around 50).
Here is the list of tools / frameworks I think I will be using:

Vaadin UI framework
Hibernate
PostgreSQL
Apache Tomcat
Memcached (for session handling)

The application will mainly be run inside a company network. It might be run on a cluster of servers or not, depends on how much money the company wants to spend to make its life easier.
So what do you think of my choices and what should I take caution of?
Cheers","The answer, as with all performance/scaling related issues is: it depends.
There is nothing in your frameworks of choice that would lead me to think it wouldn't be able to handle a large amount of users.  But without knowing what exactly you want to do or what your budget is, it's impossible to pick a technology.
To ensure that your application will scale/perform, I would consider the following:

Keep the memory footprint of each session low.  For example, caching stuff in the HttpSession may work when you have 50, but not a good idea when you have 5000 sessions.
Do as much work as you can in the database to reduce the amount of data that is being moved around (e.g. when looking at tables with lots of rows, ensure that you've got paging that is done in the database (rather than getting 10,000 rows back to Tomcat and then picking the first 10...)
Try to minimise the state that has to be kept inside the HttpSession, this makes it easier to cluster.

Probably the most important recommendations:

Use load testing tools to simulate your peak load and beyond and test.  JMeter is the tool I use for performance/load-testing.

When load testing, ensure:

That you actually use 5000 users (so that 5000 HttpSessions are created) and use a wide range of data (to avoid always hitting the cache).

EDIT: 
I don't think 5000 users is not that much and you may find that (performance-wise) you only need a single server (depends on the size of the server and the results of the load testing, of course, and you may consider a clustered solution for failovers anyway...) for handling the load (i.e. not every one of your 5000 users will be hitting a button concurrently, you'll find the load going up in the morning (i.e. everyone logs in).",https://stackoverflow.com/questions/9683790/java-web-application-for-5000-users/9684322,6,0
JSF 2.0 - Possibilities of bean scopes,"I posted a couple of questions but haven't gotten any reply yet. Everything I state here concerns JSF 2.0.* mostly.
A typical bean contains information to be displayed on a page. Common business web-based application is a set of pages where each of them involves view-edit-save state which are represented by several xhtml pages. So we create a single bean to manage those states. But there are several problems I will describe shortly:
1) Each page is a different view thus forcing you to place bean into a session scope. It takes its toll in bloating the session storage.
2) Passing parameters between views. In order to edit a document one should know ID of the document or/and another set of objects. Placing them into a session is not a good decision (bloated session antipattern).
So far several attempts to rectify a situation have been tried. 
a) t:saveState. It has been doing its job for many years. But now we are getting rid of it.
b) Seam conversation. It has imposed so many problems concerning an exact moment the conversation dies. Time out is not an easy parameter to be set since we don't know how long a business user will, for example, be editing a document. Not a solution for us.
c) CODI(not tried) It seems to be a nice JSR 299 implementation and, potentilly can solve all out problems but it is so scarcely documented and, since being an extension, sticks to the WELD which is another framework and we just want to use all the power of JSF.
d) Spring web flow. Well, it is a very nice framework, documented abundantly, great IOC container, flow scope and all other nice things it provides can be a remedy. It solves multiply tab problem(it is my wording, so forgive me if it is unclear what I am getting at). Imagine we have an edit page and view scoped bean and we are filling the form. If users opens another page in a new tab, GET request is fired and the bean goes out of scope. Web flow can recognise such a problem and starts a new flow if a new tab has been opened.
(continuation on Web Flow)But it is monolithic and will force us to rewrite the whole project. Yes, I know it supports JSF and I have tested and fumbled with it for a little while to see whether it fits the bill or not. It doesn't because of its security. Unfortunately we don't have time nor resources to build a new project from scratch.
We have almost run out of solutions. JSF is a great framework, has been tested extensively and used in many projects. But the developers refuse to include CDI in it.
Can anyone recommend any solution to edit-view-save problem with a single bean? Any architectural advice will be of great help. Thank you so much in advance.","First of all: this is rather a discussion than a question, so there will never be a clear 'yes' or 'no'... There are always pros and cons beyond objective arguments (the developers dont like it) ;-)
Anyhow, let me start with the ascertainment that your situation is very common for all kind of web applications, and the problems you are describing are even more common for everyone who thinks about web application development from a architectural point of view. 
Being confronted with an almost identical scenario over a year ago, here is our architecture:
Java EE 6 with JSF 2.0, CDI ( + EJB 3 & JPA, but this is beyond the scope of this answer). 

ViewScoped Beans, one per View (JSF ViewScope connected to CDI with Seam 3 Faces)
ConversationScoped SFSB as facade per usecase for the business-logic, transaction / security boundary (facade being references by 1 - n view controllers)
RequestScoped Services (stateless to be reusable for other clients (through different facades)

All this works like a charm, with almost no glue code between the layers.

1) Each page is a different view thus
  forcing you to place bean into a
  session scope. It takes its toll in
  bloating the session storage.
2) Passing parameters between views.
  In order to edit a document one should
  know ID of the document or/and another
  set of objects. Placing them into a
  session is not a good decision
  (bloated session antipattern).

I'm absolutely with you. That's why we use conversations.

b) Seam conversation. It has imposed
  so many problems concerning an exact
  moment the conversation dies. Time out
  is not an easy parameter to be set
  since we don't know how long a
  business user will, for example, be
  editing a document. Not a solution for
  us.

With 3 years experience of Seam 2 / 3 in production, I assure you that this is absolutely manageable. A conversation fits a usecase like a glove, and after a while you don't want to use anything else again. And certainly not the session ;-)

c) CODI(not tried) It seems to be a
  nice JSR 299 implementation and,
  potentilly can solve all out problems
  but it is so scarcely documented and,
  since being an extension, sticks to
  the WELD which is another framework
  and we just want to use all the power
  of JSF.

If you want to use CODI you dont need Weld, both are JSR 299 implementations. At the time of writing, Weld is far better documented and used more often. I don't even know if CODI is final?

d) Spring web flow. Well, it is a very
  nice framework, documented abundantly,
  great IOC container, flow scope and
  all other nice things it provides can
  be a remedy. It solves multiply tab
  problem(it is my wording, so forgive
  me if it is unclear what I am getting
  at). Imagine we have an edit page and
  view scoped bean and we are filling
  the form. If users opens another page
  in a new tab, GET request is fired and
  the bean goes out of scope. Web flow
  can recognise such a problem and
  starts a new flow if a new tab has
  been opened.

The multitab problem is solved by Seam / Weld / CODI as well. That's so ninetieths...

We have almost run out of solutions.
  JSF is a great framework, has been
  tested extensively and used in many
  projects. But the developers refuse to
  include CDI in it.

The problem with JSF is that your projects are not greenfield. You need to connect to a backend, and you will have problems doing this with pure JSF-scopes and technologies. 
All I can tell you: I too had to convince my co-workers to use CDI. I did it with a working prototype in the described layout, and now, a year later everyone in the team is quite happy with our technology stack... :-)  
To summarize this rather lengthy answer:
Java EE 6 is great technology stack for that kind of applications, and you should give it a try. The problems you are describing are not only solvable with Java EE 6, instead they were rather the problems the spec team had in mind when they designed the APIs.
Feel free to post further questions / doubts, if you like.",https://stackoverflow.com/questions/5522688/jsf-2-0-possibilities-of-bean-scopes,2,0
__hinking in AngularJS_ if I have a jQuery background? [closed],"Closed. This question needs to be more focused. It is not currently accepting answers.
                            
                        










Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                        
Closed 4 years ago.











Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                            
                        




Suppose I'm familiar with developing client-side applications in jQuery, but now I'd like to start using AngularJS. Can you describe the paradigm shift that is necessary? Here are a few questions that might help you frame an answer:

How do I architect and design client-side web applications differently? What is the biggest difference?
What should I stop doing/using; What should I start doing/using instead?
Are there any server-side considerations/restrictions?

I'm not looking for a detailed comparison between jQuery and AngularJS.","1. Don't design your page, and then change it with DOM manipulations
In jQuery, you design a page, and then you make it dynamic. This is because jQuery was designed for augmentation and has grown incredibly from that simple premise.
But in AngularJS, you must start from the ground up with your architecture in mind. Instead of starting by thinking ""I have this piece of the DOM and I want to make it do X"", you have to start with what you want to accomplish, then go about designing your application, and then finally go about designing your view.
2. Don't augment jQuery with AngularJS
Similarly, don't start with the idea that jQuery does X, Y, and Z, so I'll just add AngularJS on top of that for models and controllers. This is really tempting when you're just starting out, which is why I always recommend that new AngularJS developers don't use jQuery at all, at least until they get used to doing things the ""Angular Way"".
I've seen many developers here and on the mailing list create these elaborate solutions with jQuery plugins of 150 or 200 lines of code that they then glue into AngularJS with a collection of callbacks and $applys that are confusing and convoluted; but they eventually get it working! The problem is that in most cases that jQuery plugin could be rewritten in AngularJS in a fraction of the code, where suddenly everything becomes comprehensible and straightforward.
The bottom line is this: when solutioning, first ""think in AngularJS""; if you can't think of a solution, ask the community; if after all of that there is no easy solution, then feel free to reach for the jQuery. But don't let jQuery become a crutch or you'll never master AngularJS.
3. Always think in terms of architecture
First know that single-page applications are applications. They're not webpages. So we need to think like a server-side developer in addition to thinking like a client-side developer. We have to think about how to divide our application into individual, extensible, testable components.
So then how do you do that? How do you ""think in AngularJS""? Here are some general principles, contrasted with jQuery.
The view is the ""official record""
In jQuery, we programmatically change the view. We could have a dropdown menu defined as a ul like so:
<ul class=""main-menu"">
    <li class=""active"">
        <a href=""#/home"">Home</a>
    </li>
    <li>
        <a href=""#/menu1"">Menu 1</a>
        <ul>
            <li><a href=""#/sm1"">Submenu 1</a></li>
            <li><a href=""#/sm2"">Submenu 2</a></li>
            <li><a href=""#/sm3"">Submenu 3</a></li>
        </ul>
    </li>
    <li>
        <a href=""#/home"">Menu 2</a>
    </li>
</ul>

In jQuery, in our application logic, we would activate it with something like:
$('.main-menu').dropdownMenu();

When we just look at the view, it's not immediately obvious that there is any functionality here. For small applications, that's fine. But for non-trivial applications, things quickly get confusing and hard to maintain.
In AngularJS, though, the view is the official record of view-based functionality. Our ul declaration would look like this instead:
<ul class=""main-menu"" dropdown-menu>
    ...
</ul>

These two do the same thing, but in the AngularJS version anyone looking at the template knows what's supposed to happen. Whenever a new member of the development team comes on board, she can look at this and then know that there is a directive called dropdownMenu operating on it; she doesn't need to intuit the right answer or sift through any code. The view told us what was supposed to happen. Much cleaner.
Developers new to AngularJS often ask a question like: how do I find all links of a specific kind and add a directive onto them. The developer is always flabbergasted when we reply: you don't. But the reason you don't do that is that this is like half-jQuery, half-AngularJS, and no good. The problem here is that the developer is trying to ""do jQuery"" in the context of AngularJS. That's never going to work well. The view is the official record. Outside of a directive (more on this below), you never, ever, never change the DOM. And directives are applied in the view, so intent is clear.
Remember: don't design, and then mark up. You must architect, and then design.
Data binding
This is by far one of the most awesome features of AngularJS and cuts out a lot of the need to do the kinds of DOM manipulations I mentioned in the previous section. AngularJS will automatically update your view so you don't have to! In jQuery, we respond to events and then update content. Something like:
$.ajax({
  url: '/myEndpoint.json',
  success: function ( data, status ) {
    $('ul#log').append('<li>Data Received!</li>');
  }
});

For a view that looks like this:
<ul class=""messages"" id=""log"">
</ul>

Apart from mixing concerns, we also have the same problems of signifying intent that I mentioned before. But more importantly, we had to manually reference and update a DOM node. And if we want to delete a log entry, we have to code against the DOM for that too. How do we test the logic apart from the DOM? And what if we want to change the presentation?
This a little messy and a trifle frail. But in AngularJS, we can do this:
$http( '/myEndpoint.json' ).then( function ( response ) {
    $scope.log.push( { msg: 'Data Received!' } );
});

And our view can look like this:
<ul class=""messages"">
    <li ng-repeat=""entry in log"">{{ entry.msg }}</li>
</ul>

But for that matter, our view could look like this:
<div class=""messages"">
    <div class=""alert"" ng-repeat=""entry in log"">
        {{ entry.msg }}
    </div>
</div>

And now instead of using an unordered list, we're using Bootstrap alert boxes. And we never had to change the controller code! But more importantly, no matter where or how the log gets updated, the view will change too. Automatically. Neat!
Though I didn't show it here, the data binding is two-way. So those log messages could also be editable in the view just by doing this: <input ng-model=""entry.msg"" />. And there was much rejoicing.
Distinct model layer
In jQuery, the DOM is kind of like the model. But in AngularJS, we have a separate model layer that we can manage in any way we want, completely independently from the view. This helps for the above data binding, maintains separation of concerns, and introduces far greater testability. Other answers mentioned this point, so I'll just leave it at that.
Separation of concerns
And all of the above tie into this over-arching theme: keep your concerns separate. Your view acts as the official record of what is supposed to happen (for the most part); your model represents your data; you have a service layer to perform reusable tasks; you do DOM manipulation and augment your view with directives; and you glue it all together with controllers. This was also mentioned in other answers, and the only thing I would add pertains to testability, which I discuss in another section below.
Dependency injection
To help us out with separation of concerns is dependency injection (DI). If you come from a server-side language (from Java to PHP) you're probably familiar with this concept already, but if you're a client-side guy coming from jQuery, this concept can seem anything from silly to superfluous to hipster. But it's not. :-)
From a broad perspective, DI means that you can declare components very freely and then from any other component, just ask for an instance of it and it will be granted. You don't have to know about loading order, or file locations, or anything like that. The power may not immediately be visible, but I'll provide just one (common) example: testing.
Let's say in our application, we require a service that implements server-side storage through a REST API and, depending on application state, local storage as well. When running tests on our controllers, we don't want to have to communicate with the server - we're testing the controller, after all. We can just add a mock service of the same name as our original component, and the injector will ensure that our controller gets the fake one automatically - our controller doesn't and needn't know the difference.
Speaking of testing...
4. Test-driven development - always
This is really part of section 3 on architecture, but it's so important that I'm putting it as its own top-level section.
Out of all of the many jQuery plugins you've seen, used, or written, how many of them had an accompanying test suite? Not very many because jQuery isn't very amenable to that. But AngularJS is.
In jQuery, the only way to test is often to create the component independently with a sample/demo page against which our tests can perform DOM manipulation. So then we have to develop a component separately and then integrate it into our application. How inconvenient! So much of the time, when developing with jQuery, we opt for iterative instead of test-driven development. And who could blame us?
But because we have separation of concerns, we can do test-driven development iteratively in AngularJS! For example, let's say we want a super-simple directive to indicate in our menu what our current route is. We can declare what we want in the view of our application:
<a href=""/hello"" when-active>Hello</a>

Okay, now we can write a test for the non-existent when-active directive:
it( 'should add ""active"" when the route changes', inject(function() {
    var elm = $compile( '<a href=""/hello"" when-active>Hello</a>' )( $scope );

    $location.path('/not-matching');
    expect( elm.hasClass('active') ).toBeFalsey();

    $location.path( '/hello' );
    expect( elm.hasClass('active') ).toBeTruthy();
}));

And when we run our test, we can confirm that it fails. Only now should we create our directive:
.directive( 'whenActive', function ( $location ) {
    return {
        scope: true,
        link: function ( scope, element, attrs ) {
            scope.$on( '$routeChangeSuccess', function () {
                if ( $location.path() == element.attr( 'href' ) ) {
                    element.addClass( 'active' );
                }
                else {
                    element.removeClass( 'active' );
                }
            });
        }
    };
});

Our test now passes and our menu performs as requested. Our development is both iterative and test-driven. Wicked-cool.
5. Conceptually, directives are not packaged jQuery
You'll often hear ""only do DOM manipulation in a directive"". This is a necessity. Treat it with due deference!
But let's dive a little deeper...
Some directives just decorate what's already in the view (think ngClass) and therefore sometimes do DOM manipulation straight away and then are basically done. But if a directive is like a ""widget"" and has a template, it should also respect separation of concerns. That is, the template too should remain largely independent from its implementation in the link and controller functions.
AngularJS comes with an entire set of tools to make this very easy; with ngClass we can dynamically update the class; ngModel allows two-way data binding; ngShow and ngHide programmatically show or hide an element; and many more - including the ones we write ourselves. In other words, we can do all kinds of awesomeness without DOM manipulation. The less DOM manipulation, the easier directives are to test, the easier they are to style, the easier they are to change in the future, and the more re-usable and distributable they are.
I see lots of developers new to AngularJS using directives as the place to throw a bunch of jQuery. In other words, they think ""since I can't do DOM manipulation in the controller, I'll take that code put it in a directive"". While that certainly is much better, it's often still wrong.
Think of the logger we programmed in section 3. Even if we put that in a directive, we still want to do it the ""Angular Way"". It still doesn't take any DOM manipulation! There are lots of times when DOM manipulation is necessary, but it's a lot rarer than you think! Before doing DOM manipulation anywhere in your application, ask yourself if you really need to. There might be a better way.
Here's a quick example that shows the pattern I see most frequently. We want a toggleable button. (Note: this example is a little contrived and a skosh verbose to represent more complicated cases that are solved in exactly the same way.)
.directive( 'myDirective', function () {
    return {
        template: '<a class=""btn"">Toggle me!</a>',
        link: function ( scope, element, attrs ) {
            var on = false;

            $(element).click( function () {
                on = !on;
                $(element).toggleClass('active', on);
            });
        }
    };
});

There are a few things wrong with this:

First, jQuery was never necessary. There's nothing we did here that needed jQuery at all!
Second, even if we already have jQuery on our page, there's no reason to use it here; we can simply use angular.element and our component will still work when dropped into a project that doesn't have jQuery.
Third, even assuming jQuery was required for this directive to work, jqLite (angular.element) will always use jQuery if it was loaded! So we needn't use the $ - we can just use angular.element.
Fourth, closely related to the third, is that jqLite elements needn't be wrapped in $ - the element that is passed to the link function would already be a jQuery element! 
And fifth, which we've mentioned in previous sections, why are we mixing template stuff into our logic?

This directive can be rewritten (even for very complicated cases!) much more simply like so:
.directive( 'myDirective', function () {
    return {
        scope: true,
        template: '<a class=""btn"" ng-class=""{active: on}"" ng-click=""toggle()"">Toggle me!</a>',
        link: function ( scope, element, attrs ) {
            scope.on = false;

            scope.toggle = function () {
                scope.on = !scope.on;
            };
        }
    };
});

Again, the template stuff is in the template, so you (or your users) can easily swap it out for one that meets any style necessary, and the logic never had to be touched. Reusability - boom!
And there are still all those other benefits, like testing - it's easy! No matter what's in the template, the directive's internal API is never touched, so refactoring is easy. You can change the template as much as you want without touching the directive. And no matter what you change, your tests still pass.
w00t!
So if directives aren't just collections of jQuery-like functions, what are they? Directives are actually extensions of HTML. If HTML doesn't do something you need it to do, you write a directive to do it for you, and then use it just as if it was part of HTML.
Put another way, if AngularJS doesn't do something out of the box, think how the team would accomplish it to fit right in with ngClick, ngClass, et al.
Summary
Don't even use jQuery. Don't even include it. It will hold you back. And when you come to a problem that you think you know how to solve in jQuery already, before you reach for the $, try to think about how to do it within the confines the AngularJS. If you don't know, ask! 19 times out of 20, the best way to do it doesn't need jQuery and to try to solve it with jQuery results in more work for you.",https://stackoverflow.com/questions/14994391/thinking-in-angularjs-if-i-have-a-jquery-background/15012542,15,0
What is the difference between declarative and imperative programming? [closed],"Closed. This question needs to be more focused. It is not currently accepting answers.
                            
                        










Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                        
Closed 2 years ago.



I have been searching the web looking for a definition for declarative and imperative programming that would shed some light for me. However, the language used at some of the resources that I have found is daunting - for instance at Wikipedia.
Does anyone have a real-world example that they could show me that might bring some perspective to this subject (perhaps in C#)?","A great C# example of declarative vs. imperative programming is LINQ.
With imperative programming, you tell the compiler what you want to happen, step by step.
For example, let's start with this collection, and choose the odd numbers:
List<int> collection = new List<int> { 1, 2, 3, 4, 5 };

With imperative programming, we'd step through this, and decide what we want:
List<int> results = new List<int>();
foreach(var num in collection)
{
    if (num % 2 != 0)
          results.Add(num);
}

Here, we're saying:

Create a result collection
Step through each number in the collection
Check the number, if it's odd, add it to the results

With declarative programming, on the other hand, you write code that describes what you want, but not necessarily how to get it (declare your desired results, but not the step-by-step):
var results = collection.Where( num => num % 2 != 0);

Here, we're saying ""Give us everything where it's odd"", not ""Step through the collection.  Check this item, if it's odd, add it to a result collection.""
In many cases, code will be a mixture of both designs, too, so it's not always black-and-white.",https://stackoverflow.com/questions/1784664/what-is-the-difference-between-declarative-and-imperative-programming,16,0
disaster recovery plan before refactoring an architecture,"I mainly have to design a backup and recovery plan for a client, but another duty is to make the systems reliable in terms of failover and load balancing which will make me to changes the system architecture.
I mainly think that should be better to design an apply the backup and recovery plan after refactoring the system, I mean, at once, designing the backup and recovery plan after the refactorization.
I feel that if do so in advance is going to be a big big headache.
Do you think that is good idea propose to my client to refactore the system before applying the DR plan?
Would you so?
Thanx","From your description, the reason the existing application needs refactoring is that it was built based on the functional requirements while the load balancing and availability aspects were ignored.  Since these were not taken into account in the initial application design, the application now needs to be redesigned.  The various impacts of HA and load balancing on the application design did not become issues until the client started trying to implement them against an application not designed for them.
What you are proposing is to perform the new application redesign without taking into account the disaster recovery aspects.  This is exactly the same mistake that was made during the first implementation, only this time the requirements being ignored are the DR aspects.  What will you do when you get to the DR design and discover that your freshly refactored app has unforeseen functionality gaps and defects which conflict with the DR design?
Before touching any code on this assignment you will need to have a very good understanding of the client's recovery requirements and then design the application with these in mind.  You should know the recovery time objective, the recovery point objective, how the application will reconcile its state with any upstream or downstream applications (and whether that is a manual or automated reconciliation), the licensing impact of a hot/warm/cold DR site, etc., etc.  Otherwise you are introducing unwarranted risk and the possibility of significant rework later on.",https://stackoverflow.com/questions/8489604/disaster-recovery-plan-before-refactoring-an-architecture,1,0
Android 3.1: Refactoring Button Code and Permissions to MVP Architecture,"I have a simple one button app. The app show a button to make a phone call. I am having trouble clearly refactoring the code into an MVP architecture. There is a similar SO question (What's the best way to check for permissions at runtime using MVP architecture?). However, in this question, the answers are not clear enough to apply it to my problem. 
The permissions request is not being recognized in the view from the presenter. The problem I'm having is in the view at startActivity(intent);. Android Studio underlines the intent in red and tells me the ""Call requires permission which may be rejected by the user. The code should explicitly check for permissions"".
Why is this refactoring wrong and how can I correct it to adhere to MVP architecture?
public class MainActivity extends AppCompatActivity {

@Override
protected void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState);
    setContentView(R.layout.activity_main);
}

public void BtnSetEmergency_onclick(View view) {
    String number = ""1112223333"";
    final int REQUEST_PHONE_CALL = 1;
    Intent intent = new Intent(Intent.ACTION_CALL);
    intent.setData(Uri.parse(""tel:"" + number));

    if (ActivityCompat.checkSelfPermission(this, Manifest.permission.CALL_PHONE) != PackageManager.PERMISSION_GRANTED) {

        Toast.makeText(MainActivity.this, ""One Button App"", LENGTH_SHORT).show();

        if (ContextCompat.checkSelfPermission(MainActivity.this, Manifest.permission.CALL_PHONE) != PackageManager.PERMISSION_GRANTED) {
            ActivityCompat.requestPermissions(MainActivity.this, new String[]{Manifest.permission.CALL_PHONE},REQUEST_PHONE_CALL);
        }
        else{
            startActivity(intent);
        }

        return;
    }
    startActivity(intent);
  }
}

My Attempt at refactoring to an MVP architecture
View
public class MainActivity extends AppCompatActivity {

mainActivityPresenter mainActivityPresenterObject = new mainActivityPresenter();

@Override
protected void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState);
    setContentView(R.layout.activity_main);

}


public void BtnSetEmergency_onclick(View view) {
    boolean phonePermissions = false;
    String number = ""1112223333"";
    final int REQUEST_PHONE_CALL = 1;
    Intent intent = new Intent(Intent.ACTION_CALL);
    intent.setData(Uri.parse(""tel:"" + number));


    phonePermissions = mainActivityPresenterObject.checkPhonePermissions(view);

    if(phonePermissions == true){
        startActivity(intent);
    }else if (phonePermissions == false){

    }
  }
}

Presenter
public class mainActivityPresenter {

public boolean checkPhonePermissions(View view){
    if (ActivityCompat.checkSelfPermission(this, Manifest.permission.CALL_PHONE) != PackageManager.PERMISSION_GRANTED) {

        Toast.makeText(MainActivity.this ""One Button App"", LENGTH_SHORT).show();

        if (ContextCompat.checkSelfPermission(MainActivity.this, Manifest.permission.CALL_PHONE) != PackageManager.PERMISSION_GRANTED) {
            ActivityCompat.requestPermissions(MainActivity.this, new String[]{Manifest.permission.CALL_PHONE},REQUEST_PHONE_CALL);
            return false;
        }
        else
        {
            return true;
        }
    }
    return false;
   }
}

===========================================================>  
EDIT: Applying Final Answer Based on Nilesh Rathod Answer
===========================================================>
View
public class MainActivity extends AppCompatActivity {

    mainActivityPresenter mainActivityPresenterObject = new mainActivityPresenter();

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);
    }


    public void BtnSetEmergency_onclick(View view) {
        boolean phonePermissions = false;
        int number = 1112223333;
        Intent intent = new Intent(Intent.ACTION_CALL);
        intent.setData(Uri.parse(""tel:"" + Integer.toString(number)));

        if (ActivityCompat.checkSelfPermission(MainActivity.this, Manifest.permission.CALL_PHONE)
            == PackageManager.PERMISSION_GRANTED) {
            startActivity(intent);
        }else {
            mainActivityPresenterObject.checkPhonePermissions(view, MainActivity.this);
        }
    }
 }

Presenter
public class mainActivityPresenter {
    final int REQUEST_PHONE_CALL = 1;

    public void checkPhonePermissions(View view, MainActivity mainActivity){
        ActivityCompat.requestPermissions(mainActivity, new String[]{Manifest.permission.CALL_PHONE},REQUEST_PHONE_CALL);
    }

}","""Call requires permission which may be rejected by the user. The code should explicitly check for permissions""

Its behaving as intended.
You need to check whether user granted permission for call or not before making call using startActivity(intent);
FYI
You can handle the result of permission dialog  using ActivityCompat.OnRequestPermissionsResultCall

This interface is the contract for receiving the results for permission requests

Try this
if (ActivityCompat.checkSelfPermission(MainActivity.this, Manifest.permission.CALL_PHONE)
                == PackageManager.PERMISSION_GRANTED) {
       startActivity(intent);
}else {
     // ask here for call permission
}",https://stackoverflow.com/questions/49745115/android-3-1-refactoring-button-code-and-permissions-to-mvp-architecture,1,0
What's the best way to check for permissions at runtime using MVP architecture?,"I'm developing an android app in which I have to ask for permissions at runtime. I'm wondering about the best way to implement that using Model-View-Presenter architecture.
My initial thought was to have the presenter call a component responsible for permissions(say a PermissionHandler), and update view accordingly.
The issue is that the code to check for permissions is tightly coupled with the Activity class. Here are some of the methods involved that require an Activity or Context:

ContextCompat.checkSelfPermission()
ActivityCompat.shouldShowRequestPermissionRationale()
ActivityCompat.requestPermissions()
onRequestPermissionsResult()(callback)

This means I would have to pass an activity object to the presenter, which I didn't like much because I've heard that keeping your presenter free from Android code is good for testing.
Due to that, I then thought about handling permissions at view level(in an activity), but then I guess this would hurt the purpose of leaving the view responsible only for UI updates, without business logic.
I'm not sure what would be the best approach to tackle that keeping the code as decoupled and maintainable as possible. Any ideas?","What I would do is:
The view will implement:
public Activity getViewActivity();

The presenter will implement:
public void requestPermissions();
public void onPermissionsResult();

Inside requestPermissions, the presenter will do: getViewActivity().checkSelfPermission; getViewActivity.requestPermissions(); etc.
The view will call inside the onRequestPermissionsResult callback to presenter.onPermissionsResult();
With this all the logic will be implemented inside the presenter.
In my opinion, your presenter is decoupled: it won't depend on any view implementation (it will only depend on the view interface).
""I've heard that keeping your presenter free from Android code is good for testing."" I don't understand this part. If the code is good, it can be tested without any problem.",https://stackoverflow.com/questions/41002174/whats-the-best-way-to-check-for-permissions-at-runtime-using-mvp-architecture,2,0
What is the single most influential book every programmer should read? [closed],"As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,   or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question   can be improved and possibly reopened, visit the help center for guidance.
                            
                        


Closed 7 years ago.










Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                            
                        




If you could go back in time and tell yourself to read a specific book at the beginning of your career as a developer, which book would it be?
I expect this list to be varied and to cover a wide range of things.
To search: Use the search box in the upper-right corner. To search the answers of the current question, use inquestion:this.  For example:
inquestion:this ""Code Complete""","Code Complete (2nd edition) by Steve McConnell
The Pragmatic Programmer
Structure and Interpretation of Computer Programs
The C Programming Language by Kernighan and Ritchie
Introduction to Algorithms by Cormen, Leiserson, Rivest & Stein
Design Patterns by the Gang of Four
Refactoring: Improving the Design of Existing Code
The Mythical Man Month
The Art of Computer Programming by Donald Knuth
Compilers: Principles, Techniques and Tools by Alfred V. Aho, Ravi Sethi and Jeffrey D. Ullman
G枚del, Escher, Bach by Douglas Hofstadter
Clean Code: A Handbook of Agile Software Craftsmanship by Robert C. Martin
Effective C++
More Effective C++
CODE by Charles Petzold
Programming Pearls by Jon Bentley
Working Effectively with Legacy Code by Michael C. Feathers
Peopleware by Demarco and Lister
Coders at Work by Peter Seibel
Surely You're Joking, Mr. Feynman!
Effective Java 2nd edition
Patterns of Enterprise Application Architecture by Martin Fowler
The Little Schemer
The Seasoned Schemer
Why's (Poignant) Guide to Ruby
The Inmates Are Running The Asylum: Why High Tech Products Drive Us Crazy and How to Restore the Sanity
The Art of Unix Programming
Test-Driven Development: By Example by Kent Beck
Practices of an Agile Developer
Don't Make Me Think
Agile Software Development, Principles, Patterns, and Practices by Robert C. Martin
Domain Driven Designs by Eric Evans
The Design of Everyday Things by Donald Norman
Modern C++ Design by Andrei Alexandrescu
Best Software Writing I by Joel Spolsky
The Practice of Programming by Kernighan and Pike
Pragmatic Thinking and Learning: Refactor Your Wetware by Andy Hunt
Software Estimation: Demystifying the Black Art by Steve McConnel
The Passionate Programmer (My Job Went To India) by Chad Fowler
Hackers: Heroes of the Computer Revolution
Algorithms + Data Structures = Programs
Writing Solid Code
JavaScript - The Good Parts
Getting Real by 37 Signals
Foundations of Programming by Karl Seguin
Computer Graphics: Principles and Practice in C (2nd Edition)
Thinking in Java by Bruce Eckel
The Elements of Computing Systems
Refactoring to Patterns by Joshua Kerievsky
Modern Operating Systems by Andrew S. Tanenbaum
The Annotated Turing
Things That Make Us Smart by Donald Norman
The Timeless Way of Building by Christopher Alexander
The Deadline: A Novel About Project Management by Tom DeMarco
The C++ Programming Language (3rd edition) by Stroustrup
Patterns of Enterprise Application Architecture
Computer Systems - A Programmer's Perspective
Agile Principles, Patterns, and Practices in C# by Robert C. Martin
Growing Object-Oriented Software, Guided by Tests
Framework Design Guidelines by Brad Abrams
Object Thinking by Dr. David West
Advanced Programming in the UNIX Environment by W. Richard Stevens
Hackers and Painters: Big Ideas from the Computer Age
The Soul of a New Machine by Tracy Kidder
CLR via C# by Jeffrey Richter
The Timeless Way of Building by Christopher Alexander
Design Patterns in C# by Steve Metsker
Alice in Wonderland by Lewis Carol
Zen and the Art of Motorcycle Maintenance by Robert M. Pirsig
About Face - The Essentials of Interaction Design
Here Comes Everybody: The Power of Organizing Without Organizations by Clay Shirky
The Tao of Programming
Computational Beauty of Nature
Writing Solid Code by Steve Maguire
Philip and Alex's Guide to Web Publishing
Object-Oriented Analysis and Design with Applications by Grady Booch
Effective Java by Joshua Bloch
Computability by N. J. Cutland
Masterminds of Programming
The Tao Te Ching
The Productive Programmer
The Art of Deception by Kevin Mitnick
The Career Programmer: Guerilla Tactics for an Imperfect World by Christopher Duncan
Paradigms of Artificial Intelligence Programming: Case studies in Common Lisp
Masters of Doom
Pragmatic Unit Testing in C# with NUnit by Andy Hunt and Dave Thomas with Matt Hargett
How To Solve It by George Polya
The Alchemist by Paulo Coelho
Smalltalk-80: The Language and its Implementation
Writing Secure Code (2nd Edition) by Michael Howard
Introduction to Functional Programming by Philip Wadler and Richard Bird
No Bugs! by David Thielen 
Rework by Jason Freid and DHH
JUnit in Action",https://stackoverflow.com/questions/1711/what-is-the-single-most-influential-book-every-programmer-should-read/1713,214,0
Refactoring God classes into manager/dao/do layered architecture,"I__ currently working on an open source project that requires me to refactor a lot of old code and make use of hibernate for my database access & spring to tie it all together. The code as it is now makes use of __od_ objects where the database queries, business logic, database row & all getters and setters are implemented into a single object.
What I want to do is refactor these objects into the following layers:

A management layer containing business logic contains a reference to a Database Acces Object for its database access (DAO)
A DAO layer that will query the database and return DatabaseObjects (DO__)
A database object (DO) with just getters & setters containing no logic what so ever

So far so good, but now a requirement comes in where I don__ want end users to use any of the DO getters & setters. So I made them package protected and added getters and setters to my management layer. 
The management class getters/setters will add the business logic side of things and then call the DO__ setters/getters.
So I now have something like this (there are interfaces for all the management/DAO classes but for simplicity reasons I left these out):
public class PersonManager{


    @Autowired(required = true)
    protected PersonDAO personDAO;

    public List<PersonDO> findAll(){
        return personDAO.findAll();
    }

    public void setPersonName(PersonDO person, String name)
    {
        ...Business logic authentication checks...
        person.setName(name);
    }
}

Again this approach is working well, but now every time a developer want to use the backend the management object is required alongside the database object. Since this is an open source project I fear that the temptation might be a little to much to just make a DO method public & skip past the manager (and add business logic code to the DO again). 
So what I had in mind now is to create an additional layer that encapsulates the manager and DO into a single object, something like this:
public class Person()
{
    protected PersonManager personManager;
    protected PersonDO personDO;


    public List<Person> findAll(){
         return personManager.findAll();
    }

    public void setPersonName(String name)
    {
         ...Business logic authentication checks...
         person.setName(name);
    }
}

This would encapsulate the backend into the old __od Object_ that used to exist but with the benefit that is completely hollow and just calls on the __ayered backend API_. So the other open source contributors can still make use of old code while the backend has been cleaned up.
But there is one small problem in my setup and that is that the __indAll()_ method would return a list of __ersonDO_ objects when we actually want Person objects ! (This is because this is how hibernate returns my objects)
So I__ at a los at how best to solve this.
Is there no other way but to use the manager for everything ? Or am I missing some other way I could encapsulate the manager & the DO into an object.","Commonly used pattern in this situation is separation of the value objects (VO) and the service classes. Individual public methods of the service classes usually follow the transaction script pattern. Some call this approach ""anemic object model"", but for better or worth it seemed to be a good fit with the modern approaches to testing (the TDD, mocking, etc.) and to dependency injection and database transaction management.
If you follow this project, you API would return simple VOs that are safe to use upstream. The database transaction boundary should be at the level of your individual API methods, so VOs returned from the API calls would be disconnected from the database and changes to their values would not make it back to the database, unless VOs are passed back to the API methods explicitly designed for that purpose.",https://stackoverflow.com/questions/22964498/refactoring-god-classes-into-manager-dao-do-layered-architecture,1,0
Refactoring a Controller in symfony to adapt to hexagonal architecture,"I have create a controller that creates a Owner record into database. Everything was done on the CreateOwnerController like this and working properly:
class CreateOwnerController extends Controller
{
     public function executeAction(Request $request)
     { 
       $em = $this->getDoctrine()->getManager();
       $owner = new Owner($request->request->get(""name""));
       $em->persist($owner);
       $em->flush();

    return new Response('Added',200);
}

}
Now,In order to refactor that I have created an interface that defines the OwnerRepository:
interface OwnerRepositoryInterface {
    public function save(Owner $owner);
}

And a OwnerRepository that implements this interface:
class OwnerRepository extends EntityRepository implements OwnerRepositoryInterface {
    public function save(Owner $owner) {
        $this->_em->persist($owner);
        $this->_em->flush();
    }
}

Then I have Created for the application layer a CreateOwnerUseCase Class that receives a OwnerRepository and executes a method to save in into OwnerRepository:
class CreateOwnerUseCase {
    private $ownerRepository;
    public function __construct(OwnerRepositoryInterface $ownerRepository) {
        $this->ownerRepository = $ownerRepository;
    }

    public function execute(string $ownerName) {
        $owner = new Owner($ownerName);
        $this->ownerRepository->save($owner);
    }
}

Ok, i'm spliting the initial Controller intro layer Domain / Aplication / Framework layers.
On the CreateOwnerController now i have instantiated that Use Case and passed as parameter the OwnerRepository like this:
class CreateOwnerController extends Controller {
    public function executeAction(Request $request) { 
        $createOwnerUseCase = new CreateOwnerUseCase(new OwnerRepository());
        $createOwnerUseCase->execute($request->request->get(""name""));
        return new Response('Added',200);
    }

}

But it fails when Make the request to create new Owner:

Warning: Missing argument 1 for Doctrine\ORM\EntityRepository::__construct(), called in /ansible/phpexercises/Frameworks/mpweb-frameworks-symfony/src/MyApp/Bundle/AppBundle/Controller/CreateOwnerController.php 

It happens on OwnerRepository passed as parameter. It wants an $em and Mapped Class... What is the meaning of this mapped Class? How solve this error?","This answer is for Symfony 3.3+/4+.
You need to register your repository as a service. Instead of extending it 3rd party code, you should use composition over inheritance.
final class OwnerRepository implements OwnerRepositoryInterface
{
    private $entityManager;

    public function __construct(EntityManager $entityManager)
    {
        $this->entityManager = $entityManager;
    }

    public function save(Owner $owner) 
    {
        $this->entityManager->persist($owner);
        $this->entityManager->flush();
    }
}

And register it as a service:
# app/config/services.yml
services:
    App\Repository\:
        # for location app/Repository
        resource: ../Repository

You might need to tune paths a bit, to make that work.
To get more extended answer, see How to use Repository with Doctrine as Service in Symfony",https://stackoverflow.com/questions/42608600/refactoring-a-controller-in-symfony-to-adapt-to-hexagonal-architecture,1,0
Refactoring and optimization [closed],"Closed. This question is off-topic. It is not currently accepting answers.
                            
                        










Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                        
Closed 6 years ago.



To me refactoring is mainly for the humans that will read and maintain the code after it is written. 
But in the wikipedia article for refactoring it says:

Advantages include improved code readability and reduced complexity to improve the maintainability of the source code, as well as a more expressive internal architecture or object model to improve extensibility.

Does the reduced complexity part only mean make it less complex to understand or does it also include less complex calculations for the computer to handle? And if so should I consider code optimization as a part of refactoring?","There is no apriori reason to believe that refactored code is ""more optimized"" in terms of resources consumed during execution; refactoring is really about improving code maintainability. Often more maintainable code is less resource efficient than highly-tuned code.
However, a peculiar indirect side effect of refactoring is probably more efficient resource usage.  If the programmers understand the code better, they will likely write more effective code.   More oddly, those programmers that refactor seriously  are likely to be better software engineers than those that can't spell ""refactor"".",https://stackoverflow.com/questions/13963234/refactoring-and-optimization,1,0
Practical refactoring using unit tests,"Having just read the first four chapters of Refactoring:  Improving the Design of Existing Code, I embarked on my first refactoring and almost immediately came to a roadblock.  It stems from the requirement that before you begin refactoring, you should put unit tests around the legacy code.  That allows you to be sure your refactoring didn't  change what the original code did (only how it did it).
So my first question is this:  how do I unit-test a method in legacy code?  How can I put a unit test around a 500 line (if I'm lucky) method that doesn't do just one task?  It seems to me that I would have to refactor my legacy code just to make it unit-testable.
Does anyone have any experience refactoring using unit tests?  And, if so, do you have any practical examples you can share with me?
My second question is somewhat hard to explain.  Here's an example:  I want to refactor a legacy method that populates an object from a database record.  Wouldn't I have to write a unit test that compares an object retrieved using the old method, with an object retrieved using my refactored method?  Otherwise, how would I know that my refactored method produces the same results as the old method?  If that is true, then how long do I leave the old deprecated method in the source code?  Do I just whack it after I test a few different records?  Or, do I need to keep it around for a while in case I encounter a bug in my refactored code?
Lastly, since a couple people have asked...the legacy code was originally written in VB6 and then ported to VB.NET with minimal architecture changes.","Good example of theory meeting reality. Unit tests are meant to test a single operation and many pattern purists insist on Single Responsibilty, so we have lovely clean code and tests to go with it. However, in the real (messy) world, code (especially legacy code) does lots of things and has no tests. What this needs is dose of refactoring to clean the mess.
My approach is to build tests, using the Unit Test tools, that test lots of things in a single test. In one test, I may be checking the DB connection is open, changing lots of data, and doing a before/after check on the DB. I inevitably find myself writing helper classes to do the checking, and more often than not those helpers can then be added into the code base, as they have encapsulated emergent behaviour/logic/requirements. I don't mean I have a single huge test, what I do mean is mnay tests are doing work which a purist would call an integration test - does such a thing still exist? Also I've found it useful to create a test template and then create many tests from that, to check boundary conditions, complex processing etc.
BTW which language environment are we talking about? Some languages lend themselves to refactoring better than others.",https://stackoverflow.com/questions/522334/practical-refactoring-using-unit-tests,7,0
"EF6 database architecture for shared entities: products2images, categories2images","I have three primary entities on working production application: Product, Category and HtmlPage 
public class Product {
    [Key]
    public Guid Id {get;set;}
    //and other properties
}
public class Category {
    [Key]
    public int Id {get;set;}
    //and other properties
}
public class HtmlPage {
    [Key]
    public string Id {get;set;}
    //and other properties
}

and one shared entity: Image
public class Image {
    public Guid Id {get;set;}
    //and other properties
}

all primary entities have junction with shared entity like this
public class ProductImage{
    public Guid ImageId {get;set;}
    public Guid ProductId {get;set;}
    //list of junction properies: order, isprimary and so on
}

I don't like this approach because in case of a large number of primary entities types it will be difficult to manage code and everything.
And i see some kind of troubles in case of a large number of shared entities types (Comments,Tags, Votes and so on)
and the problem is that i realy have to add new primary entities and shared entities in exists architecture right now.
So, i am thinking about database architecture refactoring
ideal solution as i see is 
public abstract class EntityProxy
{
    [Key]
    public Guid Id { get; set; }
    public virtual ICollection<EntityImageEntityProxy> Images { get; set; }
    public virtual ICollection<EntityTagEntityProxy> Tags { get; set; }
    public virtual ICollection<EntityCommentEntityProxy> Comments { get; set; }
}

// list of Shared Entities
public class EntityImage
{
    [Key]
    public Guid Id { get; set; }
    public string Name { get; set; }
    public virtual ICollection<EntityImageEntityProxy> Entities { get; set; }
}
public class EntityTag
{
    [Key]
    public Guid Id { get; set; }
    public string Name { get; set; }
    public virtual ICollection<EntityTagEntityProxy> Entities { get; set; }
}
public class EntityComment
{
    [Key]
    public Guid Id { get; set; }
    public string Text { get; set; }
    public virtual ICollection<EntityCommentEntityProxy> Entities { get; set; }
}

// shared entities to primary entities mapping
public class EntityImageEntityProxy
{
    [Key, Column(Order = 0)]
    public Guid ImageId { get; set; }
    [Key, Column(Order = 1)]
    public Guid EntityProxyId { get; set; }

    [ForeignKey(""ImageId"")]
    public virtual EntityImage Image { get; set; }
    [ForeignKey(""EntityProxyId"")]
    public virtual EntityProxy Entity { get; set; }
}
public class EntityTagEntityProxy
{
    [Key, Column(Order = 0)]
    public Guid TagId { get; set; }
    [Key, Column(Order = 1)]
    public Guid EntityProxyId { get; set; }

    [ForeignKey(""TagId"")]
    public virtual EntityTag Tag { get; set; }
    [ForeignKey(""EntityProxyId"")]
    public virtual EntityProxy Entity { get; set; }
}
public class EntityCommentEntityProxy
{
    [Key, Column(Order = 0)]
    public Guid CommentId { get; set; }
    [Key, Column(Order = 1)]
    public Guid EntityProxyId { get; set; }

    [ForeignKey(""CommentId"")]
    public virtual EntityComment Comment { get; set; }
    [ForeignKey(""EntityProxyId"")]
    public virtual EntityProxy Entity { get; set; }
}

// list of primary entities
[Table(""EntityProxyProducts"")]
public class EntityProxyProduct : EntityProxy
{
    //type properties
}
[Table(""EntityProxyCategories"")]
public class EntityProxyCategory : EntityProxy
{
    //type properties
}
[Table(""EntityProxyHtmlPages"")]
public class EntityProxyHtmlPage : EntityProxy
{
    //type properties
}

This implies rewriting the entire application due to the change in types of primary entities keys and i have my doubts about the performance
images - 100 000 - 200 000 entries
products - 30 000 - 50 000 entries
the second solution i see is merge ideal solution with existing models like this:
    // lis of exists primary entities
public class Product
{
    [Key]
    public Guid Id { get; set; }
    public Guid? EntityProxyId { get; set; }
    [ForeignKey(""EntityProxyId"")]
    public virtual EntityProxyProduct EntityProxy { get; set; }
    //and other properties
}
public class Category
{
    [Key]
    public int Id { get; set; }
    public Guid? EntityProxyId { get; set; }
    [ForeignKey(""EntityProxyId"")]
    public virtual EntityProxyCategory EntityProxy { get; set; }
    //and other properties
}
public class HtmlPage
{
    [Key]
    public string Id { get; set; }
    public Guid? EntityProxyId { get; set; }
    [ForeignKey(""EntityProxyId"")]
    public virtual EntityProxyHtmlPage EntityProxy { get; set; }
    //and other properties
}


public abstract class EntityProxy
{
    [Key]
    public Guid Id { get; set; }
    public virtual ICollection<EntityImageEntityProxy> Images { get; set; }
    public virtual ICollection<EntityTagEntityProxy> Tags { get; set; }
    public virtual ICollection<EntityCommentEntityProxy> Comments { get; set; }
}

// list of Shared Entities
public class EntityImage
{
    [Key]
    public Guid Id { get; set; }
    public string Name { get; set; }
    public virtual ICollection<EntityImageEntityProxy> Entities { get; set; }
}
public class EntityTag
{
    [Key]
    public Guid Id { get; set; }
    public string Name { get; set; }
    public virtual ICollection<EntityTagEntityProxy> Entities { get; set; }
}
public class EntityComment
{
    [Key]
    public Guid Id { get; set; }
    public string Text { get; set; }
    public virtual ICollection<EntityCommentEntityProxy> Entities { get; set; }
}

// shared entities to primary entities mapping
public class EntityImageEntityProxy
{
    [Key, Column(Order = 0)]
    public Guid ImageId { get; set; }
    [Key, Column(Order = 1)]
    public Guid EntityProxyId { get; set; }

    [ForeignKey(""ImageId"")]
    public virtual EntityImage Image { get; set; }
    [ForeignKey(""EntityProxyId"")]
    public virtual EntityProxy Entity { get; set; }
}
public class EntityTagEntityProxy
{
    [Key, Column(Order = 0)]
    public Guid TagId { get; set; }
    [Key, Column(Order = 1)]
    public Guid EntityProxyId { get; set; }

    [ForeignKey(""TagId"")]
    public virtual EntityTag Tag { get; set; }
    [ForeignKey(""EntityProxyId"")]
    public virtual EntityProxy Entity { get; set; }
}
public class EntityCommentEntityProxy
{
    [Key, Column(Order = 0)]
    public Guid CommentId { get; set; }
    [Key, Column(Order = 1)]
    public Guid EntityProxyId { get; set; }

    [ForeignKey(""CommentId"")]
    public virtual EntityComment Comment { get; set; }
    [ForeignKey(""EntityProxyId"")]
    public virtual EntityProxy Entity { get; set; }
}

// list of primary entities proxies
[Table(""EntityProxyProducts"")]
public class EntityProxyProduct : EntityProxy
{
    public virtual ICollection<Product> Products { get; set; }
}
[Table(""EntityProxyCategories"")]
public class EntityProxyCategory : EntityProxy
{
    public virtual ICollection<Category> Categories { get; set; }
}
[Table(""EntityProxyHtmlPages"")]
public class EntityProxyHtmlPage : EntityProxy
{
    public virtual ICollection<HtmlPage> HtmlPages { get; set; }
}

The negative in the second solution is the fact that it's impossible to create one-to-zero..one relation with non primary key properties and the only way i see is to create it like one-to-many and take care of db safe from the code - not good.
so, my question is what i have to do?
_ take a beer and do it fashioned way without implementing any architectural refactoring
_ must summon the courage to implement the perfect solution with rewriting everything
_ take the lesser of two evils and implement mixed solution
_ take a look at a more jedi solution
and what kind of performance difficulties I may face?
Any advice, links and docs are appreciated.","It seems like you prefer to work with a document-centric view of your data, have you considered using a document database instead of relational sql for this?",https://stackoverflow.com/questions/30981333/ef6-database-architecture-for-shared-entities-products2images-categories2image,1,0
How does TDD make refactoring easier?,"I've heard that projects developed using TDD are easier to refactor because the practice yields a comprehensive set of unit tests, which will (hopefully) fail if any change has broken the code. All of the examples I've seen of this, however, deal with refactoring implementation - changing an algorithm with a more efficient one, for example. 
I find that refactoring architecture is a lot more common in the early stages where the design is still being worked out. Interfaces change, new classes are added & deleted, even the behavior of a function could change slightly (I thought I needed it to do this, but it actually needs to do that), etc... But if each test case is tightly coupled to these unstable classes, wouldn't you have to be constantly rewriting your test cases each time you change a design? 
Under what situations in TDD is it okay to alter and delete test cases? How can you be sure that altering the test cases don't break them? Plus it seems that having to synchronize a comprehensive test suite with constantly changing code would be a pain. I understand that the unit test suite could help tremendously during maintenance, once the software is built, stable, and functioning, but that's late in the game wheras TDD is supposed to help early on as well.
Lastly, would a good book on TDD and/or refactoring address these sort of issues? If so, which would you recommend?","Plus it seems that having to
  synchronize a comprehensive test suite
  with constantly changing code would be
  a pain. I understand that the unit
  test suite could help tremendously
  during maintenance, once the software
  is built, stable, and functioning, but
  that's late in the game wheras TDD is
  supposed to help early on as well.

I do agree that the overhead of having a unit test suite in place can be felt at these early changes, when major architectural changes are taking place, but my opinion is that the benefits of having unit tests far outweigh this drawback.  I think too often the problem is a mental one - we tend to think of our unit tests as second class citizens of the code base, and we resent having to mess with them.  But over time, as I've come to depend on them and appreciate their usefulness, I've come to think of them as no less important and no less worthy of maintenance and work as any other part of the code base.
Are the major architecural ""changes"" taking place truly only refactorings?  If you are only refactoring, however dramatically, and tests begin to fail, that may tell you that you've inadvertantly changed functionality somewhere.  Which is just what unit tests are supposed to help you catch.  If you are making sweeping changes to functionality and architecture at the same time, you may want to consider slowing down and getting into that red/green/refactor groove: no new (or changed) functionality w/o additional tests, and no changes to functionality (and breaking tests) while refactoring.
Update (based on comments):
@Cybis has raised an interesting objection to my claim that refactoring shouldn't break tests because refactoring shouldn't change behavior.  His objection is that refactoring does change the API, and therefore tests ""break"".
First, I would encourage anyone to visit the canonical reference on refactoring: Martin Fowler's bliki.  Just now I reviewed it and a couple things jump out at me:

Is changing an interface
refactoring?  Martin refers to
refactoring as a
""behavior-preserving"" change, which
means when the interface/API changes
then all callers of that
interface/API must change as well. 
Including tests, I say.
That does not mean that the behavior has changed.  Again, Fowler emphasizes that his
definition of refactoring is that
the changes are behavior
preserving.

In light of this, if a test or tests has to change during a refactoring, I don't see this as ""breaking"" the test(s).  It's simply part of the refactoring, of preserving the behavior of the entire code base.  I see no difference between a test having to change and any other part of the code base having to change as part of a refactoring.  (This goes back to what I said before about considering tests to be first-class citizens of the code base.)
Additionally, I would expect the tests, even the modified tests, to continue to pass once the refactoring is done.  Whatever that test was testing (probably the assert(s) in that test) should still be valid after a refactoring is done.  Otherwise, that's a red flag that behavior changed/regressed somehow during the refactoring.
Maybe that claim sounds like nonsense but think about it: we think nothing about moving blocks of code around in the production code base and expecting them to continue to work in their new context (new class, new method signature, whatever).  I feel the same way about a test: perhaps a refactoring changes the API that a test must call, or a class that a test must use, but in the end the point of the test should not change because of a refactoring.
(The only exception I can think of to this is tests that test low-level implementation details that you may want to change during a refactoring, such as replacing a LinkedList with an ArrayList or something.  But in that case one could argue that the tests are over-testing and are too rigid and fragile.)",https://stackoverflow.com/questions/255509/how-does-tdd-make-refactoring-easier,8,0
What are the first tasks for implementing Unit Testing in Brownfield Applications?,"Do you refactor your SQL first? Your architecture? or your code base?
Do you change languages? Do you throw everything away and start from scratch? [Not refactoring]","I'm adding unit testing to a large, legacy spaghetti codebase. 
My approach is, when asked to solve a problem, I try to create a new wrapper around the part of the code-base which is relevant to my current task. This new wrapper is developed using TTD (writing the test first). Some of the time calling into the non-unit tested legacy code. At other times I make a new copy of an existing module and start to do serious violence to it. Sometimes I rewrite functionality from scratch. 
But as I'm keeping it fairly well tested I feel pretty in control.
What I find with this code-base, which has been developed with far too much copy and pasting, is that once I get an understanding a particular part, and extract some functions from it (which are done test-first) ... these functions often turn out to be usable in many other places and so the rate of replacing the legacy code with my own, unit tested libraries increases.
I don't (and have no authority to) try to rewrite or add tests to parts of the code that are not touched by my current problem (usually a bug I'm trying to fix) but I do have a fairly aggressive proactive stance on anything that is touched and might be relevant.
Update : Penguinix asked : ""What languages do you work in? Is there a specific Testing Harness you recommend?"" 
Right now I'm working in ... er ... Mumps! But the same principle works anywhere. 
Something that transformed my understanding of UT was MinUnit : http://www.jera.com/techinfo/jtns/jtn002.html 
When I saw MinUnit, that was kind of a ""zen"" moment of enlightenment for me. It stripped away the misunderstandings I had about unit testing being something complicated requiring sophisticated OO frameworks etc. I understood that UT was just about writing a bunch of tests. The ""harness"" you can write yourself, in about 3 minutes, in any language you like. Just get on and do it.",https://stackoverflow.com/questions/66361/what-are-the-first-tasks-for-implementing-unit-testing-in-brownfield-application,2,0
Refactoring for Testability on an existing system,"I've joined a team that works on a product. This product has been around for ~5 years or so, and uses ASP.NET WebForms. Its original architecture has faded over time, and things have become relatively disorganized throughout the solution. It's by no means terrible, but definitely can use some work; you all know what I mean.
I've been performing some refactorings since coming on to the project team about 6 months ago. Some of those refactorings are simple, Extract Method, Pull Method Up, etc. Some of the refactorings are more structural. The latter changes make me nervous as there isn't a comprehensive suite of unit tests to accompany every component. 
The whole team is on board for the need to make structural changes through refactoring, but our Project Manager has expressed some concerns that we don't have adequate tests to make refactorings with the confidence that we aren't introducing regression bugs into the system. He would like us to write more tests first (against the existing architecture), then perform the refactorings. My argument is that the system's class structure is too tightly coupled to write adequate tests, and that using a more Test Driven approach while we perform our refactorings may be better. What I mean by this is not writing tests against the existing components, but writing tests for specific functional requirements, then refactoring existing code to meet those requirements. This will allow us to write tests that will probably have more longevity in the system, rather than writing a bunch of 'throw away' tests.
Does anyone have any experience as to what the best course of action is? I have my own thoughts, but would like to hear some input from the community.","Your PM's concerns are valid - make sure you get your system under test before making any major refactorings.
I would strongly recommend getting a copy of Michael Feather's book Working Effectively With Legacy Code (by ""Legacy Code"" Feathers means any system that isn't adequately covered by unit tests).  This is chock full of good ideas for how to break down those couplings and dependencies you speak of, in a safe manner that won't risk introducing regression bugs.
Good luck with the refactoring programme; in my experience it's an enjoyable and cathartic process from which you can learn a lot.",https://stackoverflow.com/questions/20262/refactoring-for-testability-on-an-existing-system,5,0
Exception Handling Block - Manually registering the ExceptionManager class,"Is it possible to register the abstract ExceptionManager type with it's concrete implementation ExceptionManagerImpl ?
The default relies on Unity container, and I am already using Windsor so I would like to register the Exception Handling Block components on Windsor manually.","Yes, it's possible. You will need to build a Windsor configurator for EntLib. We touch on this in the Architectural Refactoring overview.",https://stackoverflow.com/questions/5968725/exception-handling-block-manually-registering-the-exceptionmanager-class/5973383,1,0
Java doc patterns,"I refactored a class by moving some of the methods to different class. Since this was like an Architecture refactoring and not code refactoring, I was wondering if it is a good practice to mention in the javadoc of the new classes that it contains the methods moved from previously existing X class?. For example
/**
Processor that sets sequence to the payment group. This processor has been added as part of checkout refactor project and xxxMethod() method has been moved from {@link XXXFormHandler} to this pipeline processor.
**/
I like this approach since it gives clear picture to any developer involved in maintaining this code. My only concern is if this is exposed as an API, then those information will be shown to everyone.","I don__ think that this is useful. You had strong reasons to refactor the code so developers knowing the new code only should not bother with the old architecture. Developers knowing the old code only don__ know where in the new code they have to look for that javadoc and once they know, they don__ need it anymore. So they need a migration guide which exists independently of the javadoc.
Regarding the last concern, if API users shall not see the particular documentation, regular comments do the right thing as maintainer seeing the source code see the comments.",https://stackoverflow.com/questions/19504859/java-doc-patterns,1,0
Where to put styles for classes that are used across code base using sass 7-1 architecture,"I am refactoring a legacy web app and I am trying to use the Sass 7-1 architecture. The code has several 'global' class names i.e;
.flex-expand {
    flex-grow: 1;
    flex-shrink: 1;
    position: relative;
}
.flex-scroll-area {
    overflow-y: auto;
    overflow-x: hidden;
}

Where would classes that are used in many different files in the code base be placed?","What about something like abstracts/_extends.scss?
.my-expandable-element {
   @extend .flex-expand;
   background: yellow;
   color: pink;
}",https://stackoverflow.com/questions/50002900/where-to-put-styles-for-classes-that-are-used-across-code-base-using-sass-7-1-ar,1,0
Program architecture questions,"I have couple of questions about Program architecture / design.
1. I am refactoring one of my program completely, the purpose of the program is to transfer file from client side to server side (like drop box or cyber ark). It can create number of clients according to the configuration, its all under window service.
lets look at the start code:
public class Master
{
  private List<Box> _boxes = new List<Box>();

  public Master()
  {
     Initialize(); ...
  }

  public void Run()
  {
     foreach(var box in _boxes)
     {
        box.Run();
      }
  }
  }

Each Box class is ""Living"" by its own, but if one of them fail with unmanaged exception then the whole service is lost, or by managing the resources better. my thought was that the window service is the master class that will start number of Boxes as a different Process, or using the appDomain class.  

My second question is - i simply lack the knowledge of program architecture / design, do any of you guys know of a recommended  book / article / link to read ?
Thanks 

P.S.
Does any of you also cant see the arrows and the StackExchange link in Google chrome browser ?","You can't use single try/catch in the foreach because you are running threads inside the Box class when calling box.Run(), however you should predict the exceptions inside the Box class and handle them there, but if you are unable to do that ""maybe because you call another third party library that itself running some threads, that may cause exceptions"", then is it an option for you to create and run each box in a different application domain? here an example how to do it, by doing so you can handle each box exception separately by handling each of the boxes` application domain exceptions:
myBoxDomain.UnhandledException += OnCurrentDomain_UnhandledException;

So you can handle each box failure here without affecting the other boxes.",https://stackoverflow.com/questions/6875456/program-architecture-questions,3,0
Refactoring class design to convey the design intention,"I have following class design.  The complete code is available in "" How to achieve this functionality using Generics? "". The code  works fine and resolves the casting issue mentioned in "" Refactoring Code to avoid Type Casting ""
In the RetailInvestmentReturnCalculator class, the GetInvestmentProfit() method utilizes CalculateBaseProfit() method present in InvestmentReturnCalculator abstract base class. This fact is not evident from the class design. 
QUESTION

How to refactor this class design to convey the above mentioned fact?
What is the design guideline that will prevent this sort of design mistakes?

Note: Martin Fowler: Is Design Dead? says

What do we mean by a software architecture? To me the term architecture conveys a notion of the core elements of the system, the pieces that are difficult to change. A foundation on which the rest must be built

Class Diagram

Abstract
public abstract class InvestmentReturnCalculator
{
    #region Public

    public double ProfitElement { get; set; }
    public abstract double GetInvestmentProfit();

    #endregion

    #region Protected

    protected  double CalculateBaseProfit()
    {
        double profit = 0;
        if (ProfitElement < 5)
        {
            profit = ProfitElement * 5 / 100;
        }
        else
        {
            profit = ProfitElement * 10 / 100;
        }
        return profit;
    }

    #endregion
}

public abstract class InvestmentReturnCalculator<T> : InvestmentReturnCalculator where T : IBusiness
{
    public T BusinessType { get; set; }
}

Concrete
public class RetailInvestmentReturnCalculator : InvestmentReturnCalculator<IRetailBusiness>
{
    public RetailInvestmentReturnCalculator(IRetailBusiness retail)
    {
        BusinessType = retail;
        //Business = new BookShop(100);
    }

    public override double GetInvestmentProfit()
    {
        ProfitElement = BusinessType.GrossRevenue;
        return CalculateBaseProfit();
    }
}","The ProfitElement field is rather ugly. I would make it an abstract property on InvestmentReturnCalculator and implement it in base classes (rather than setting the value) - this is called the template method pattern. Then you don't need the GetInvestmentProfit() method.
public abstract class InvestmentReturnCalculator
{
    #region Public

    public abstract double ProfitElement { get; }

    #endregion

    #region Protected

    public double GetInvestmentProfit()
    {
        double profit = 0;
        if (ProfitElement < 5)
        {
            profit = ProfitElement * 5 / 100;
        }
        else
        {
            profit = ProfitElement * 10 / 100;
        }
        return profit;
    }

    #endregion
}

public abstract class InvestmentReturnCalculator<T> : InvestmentReturnCalculator where T : IBusiness
{
    public T BusinessType { get; set; }
}

public class RetailInvestmentReturnCalculator : InvestmentReturnCalculator<IRetailBusiness>
{
    public RetailInvestmentReturnCalculator(IRetailBusiness retail)
    {
        BusinessType = retail;
        //Business = new BookShop(100);
    }

    public override double ProfitElement {get { return BusinessType.GrossRevenue;}}

}",https://stackoverflow.com/questions/21579407/refactoring-class-design-to-convey-the-design-intention,1,0
How to achieve this functionality using Generics?,"I am not experienced in using .Net generics.  I am designing a software system for and Investment Holding Company in .Net 4.0. The company has Retail business and IntellectualRights business. BookShop and AudioCDShop are examples of Retail business. EngineDesignPatent and BenzolMedicinePatent are examples of IntellectualRights business. These two business types are totally unrelated. 
The investment company has a concept called InvestmentReturn.  It is the profit gained from each business. For each __usiness Type_ (Retail, IntellectualRights ), the calculation logic is different for Investment return.
I need to create a InvestmentReturnCalculator by calculating investment of each __usiness Type_.
public static class InvestmentReturnCalculator
{
    public static double GetNetInvestementReturn(List<IBusiness> allMyProfitableBusiness, List<InvestmentReturnElement<IBusiness>> profitElements)
    {
        double totalReturn = 0;
        foreach (IBusiness b in allMyProfitableBusiness)
        {
            //How to do calculation?
        }
        return totalReturn;
    }
}

QUESTION

How to add various business elements into List<InvestmentReturnElement<IBusiness>> profitElements in the Main function ? 
Note: I am getting compilation error when I do the following
profitElements.Add(salesProfitBook);
How to implement the GetNetInvestementReturn method in a generic way? If I make the code as follows, there is repetition of the algorithm for different types.. And generics can be used when algorithm is same for multiple types. So the following approach is not DRY.

Note: The following code does not compile.
    foreach (IBusiness b in allMyProfitableBusiness)
    {
        if (b is IRetailBusiness)
        {
            RetailProfit<IRetailBusiness> retailInvestmentProfit = new RetailProfit<IRetailBusiness>();
            totalReturn = totalReturn + retailInvestmentProfit.GetInvestmentProfit(b);
        }
        else if (b is IIntellectualRights)
        {
            IntellectualRightsProfit<IIntellectualRights> intellectualRightsInvestmentProfit = new IntellectualRightsProfit<IIntellectualRights>();
            totalReturn = totalReturn + intellectualRightsInvestmentProfit.GetInvestmentProfit(b);
        }
    }

UPDATE:
The BookShop, EngineDesignPatent inherits a different base class. So I cannot make IBusiness, IRetailBusiness, and IIntellectualRights as abstract classes. They should remain as interfaces.
Now @Grzenio suggestion is to implement a GetInvestmentProfit method in each entity (BookShop, AudioCDShop, etc). Here I will be repeating the same code. Again this is not satisfying DRY.
Moreover the InvestmentReturn concept is for the investment holding company. The individual business types are unaware of such a concept.
Investment Return Element
    public abstract class InvestmentReturnElement<T>
    {
        public abstract double GetInvestmentProfit(T obj);
    }

    public class RetailProfit<T> : InvestmentReturnElement<T> where T : IRetailBusiness
    {
        public override double GetInvestmentProfit(T item)
        {
            return item.Revenue * 5/100;
        }
    }

    public class IntellectualRightsProfit<T> : InvestmentReturnElement<T> where T : IIntellectualRights
    {
        public override double GetInvestmentProfit(T item)
        {
            return item.Royalty * 10/100;
        }
    }      

Business Type Abstractions
public interface IBusiness
{

}

public interface IRetailBusiness : IBusiness
{
    bool IsOnSale { get; set; }
    double Revenue { get; set; }
}

public interface IIntellectualRights : IBusiness
{
    double Royalty { get; set; }
}

Concrete Businesses
    #region Intellectuals
    public class EngineDesignPatent : IIntellectualRights
    {
        public double Royalty { get; set; }
    }

    public class BenzolMedicinePatent : IIntellectualRights
    {
        public double Royalty { get; set; }
    }
    #endregion

    #region Retails
    public class BookShop : IRetailBusiness
    {
        public bool IsOnSale { get; set; }
        public double Revenue { get; set; }
    }

    public class AudioCDShop : IRetailBusiness
    {
        public bool IsOnSale { get; set; }
        public double Revenue { get; set; }
    }
    #endregion

Client
    class Program
    {

        static void Main(string[] args)
        {

            #region MyBusines

            List<IBusiness> allMyProfitableBusiness = new List<IBusiness>();

            BookShop bookShop1 = new BookShop();
            AudioCDShop cd1Shop = new AudioCDShop();
            EngineDesignPatent enginePatent = new EngineDesignPatent();
            BenzolMedicinePatent medicinePatent = new BenzolMedicinePatent();

            allMyProfitableBusiness.Add(bookShop1);
            allMyProfitableBusiness.Add(cd1Shop);
            allMyProfitableBusiness.Add(enginePatent);
            allMyProfitableBusiness.Add(medicinePatent);

            #endregion

            List<InvestmentReturnElement<IBusiness>> profitElements = new List<InvestmentReturnElement<IBusiness>>();

            var salesProfitBook = new RetailProfit<BookShop>();
            var salesProfitAudioCD = new RetailProfit<AudioCDShop>();
            var intellectualProfitEngineDesign = new IntellectualRightsProfit<EngineDesignPatent>();
            var intellectualProfitBenzolMedicine = new IntellectualRightsProfit<BenzolMedicinePatent>();

            //profitElements.Add(salesProfitBook);
            Console.ReadKey();
        }

    }","To answer your questions
(1) Try making InvestmentReturnElement contravariant (and all inheritors):
public abstract class InvestmentReturnElement<in T>
{
    public abstract double GetInvestmentProfit(T obj);
}

(2) I might be wrong, but I don't think it is possible to magically create InvestmentReturnElement<T> parameterised for the calling type, when you don't know T statically when you are calling it.
Option 1: Either you can make the business class calculate its profit
public interface IBusiness
{
    double Profit {get;}
    double OtherProfit (SomeCalculatorObject o);
}

Option 2: Or you might conclude that the businesses are so different from profit calculation perspective that you will keep the list of one type separate from the other type and handle them without generics.",https://stackoverflow.com/questions/21462210/how-to-achieve-this-functionality-using-generics/21575319,3,0
Refactoring Code to avoid Type Casting,"I have following C# code in .Net 4.0.  It requires a type casting of IBusiness to IRetailBusiness.
//Type checking
if (bus is IRetailBusiness)
{
       //Type casting
       investmentReturns.Add(new RetailInvestmentReturn((IRetailBusiness)bus));
}

if (bus is IIntellectualRights)
{
       investmentReturns.Add(new IntellectualRightsInvestmentReturn((IIntellectualRights)bus));
}

Business Scenario:
I am designing a software system for and Investment Holding Company. The company has Retail business and IntellectualRights business. BookShop and AudioCDShop are examples of Retail business. EngineDesignPatent and BenzolMedicinePatent are examples of IntellectualRights business. These two business types are totally unrelated.
The investment company has a concept called InvestmentReturn  (But each individual business is totally ignorant about this concept). InvestmentReturn is the profit gained from each business and it is calulated using ProfitElement. For each __usiness Type_ (Retail, IntellectualRights ), the ProfitElement used is different.
QUESTION
How to refactor this class design to avoid this type casting and type checking?
Abstract Investment
public abstract class InvestmentReturn
{
    public double ProfitElement { get; set; }
    public IBusiness Business{ get;  set; }

    public abstract double GetInvestmentProfit();

    public double CalculateBaseProfit()
    {
       double profit = 0;

       if (ProfitElement < 5)
       {
           profit = ProfitElement * 5 / 100;
       }
       else if (ProfitElement < 20)
       {
           profit = ProfitElement * 7 / 100;
       }
       else
       {
           profit = ProfitElement * 10 / 100;
       }

       return profit;
    }
}

Extensions
public class RetailInvestmentReturn : InvestmentReturn
{
    public RetailInvestmentReturn(IRetailBusiness retail)
    {
        Business = retail;
    }

    public override  double GetInvestmentProfit()
    {
        //GrossRevenue is the ProfitElement for RetailBusiness
        ProfitElement = ((IRetailBusiness)Business).GrossRevenue;
        return base.CalculateBaseProfit();
    }  
}

public class IntellectualRightsInvestmentReturn : InvestmentReturn
{

    public IntellectualRightsInvestmentReturn(IIntellectualRights intellectual)
    {
        Business = intellectual;
    }

    public override double GetInvestmentProfit()
    {
        //Royalty is the ProfitElement for IntellectualRights Business
        ProfitElement = ((IIntellectualRights)Business).Royalty;
        return base.CalculateBaseProfit();
    }
}

Client
class Program
{

    static void Main(string[] args)
    {

        #region MyBusines

        List<IBusiness> allMyProfitableBusiness = new List<IBusiness>();

        BookShop bookShop1 = new BookShop(75);
        AudioCDShop cd1Shop = new AudioCDShop(80);
        EngineDesignPatent enginePatent = new EngineDesignPatent(1200);
        BenzolMedicinePatent medicinePatent = new BenzolMedicinePatent(1450);

        allMyProfitableBusiness.Add(bookShop1);
        allMyProfitableBusiness.Add(cd1Shop);
        allMyProfitableBusiness.Add(enginePatent);
        allMyProfitableBusiness.Add(medicinePatent);

        #endregion

        List<InvestmentReturn> investmentReturns = new List<InvestmentReturn>();

        foreach (IBusiness bus in allMyProfitableBusiness)
        {
            //Type checking
            if (bus is IRetailBusiness)
            {
                //Type casting
                investmentReturns.Add(new RetailInvestmentReturn((IRetailBusiness)bus));
            }

            if (bus is IIntellectualRights)
            {
                investmentReturns.Add(new IntellectualRightsInvestmentReturn((IIntellectualRights)bus));
            }
        }

        double totalProfit = 0;
        foreach (var profitelement in investmentReturns)
        {
            totalProfit = totalProfit + profitelement.GetInvestmentProfit();
            Console.WriteLine(""Profit: {0:c}"", profitelement.GetInvestmentProfit());
        }

        Console.ReadKey();
    }
}

Business Domain Entities
public interface IBusiness
{

}

public abstract class EntityBaseClass
{

}

public interface IRetailBusiness : IBusiness
{
    double GrossRevenue { get; set; }
}

public interface IIntellectualRights : IBusiness
{
    double Royalty { get; set; }
}



#region Intellectuals
public class EngineDesignPatent : EntityBaseClass, IIntellectualRights
{
    public double Royalty { get; set; }
    public EngineDesignPatent(double royalty)
    {
        Royalty = royalty;
    }
}

public class BenzolMedicinePatent : EntityBaseClass, IIntellectualRights
{
    public double Royalty { get; set; }
    public BenzolMedicinePatent(double royalty)
    {
        Royalty = royalty;
    }
}
#endregion

#region Retails
public class BookShop : EntityBaseClass, IRetailBusiness
{
    public double GrossRevenue { get; set; }
    public BookShop(double grossRevenue)
    {
        GrossRevenue = grossRevenue;
    }
}

public class AudioCDShop : EntityBaseClass, IRetailBusiness
{
    public double GrossRevenue { get; set; }
    public AudioCDShop(double grossRevenue)
    {
        GrossRevenue = grossRevenue;
    }
}
#endregion

REFERENCES

Refactor my code : Avoiding casting in derived class
Cast to generic type in C#
How a Visitor implementation can handle unknown nodes
Open Closed Principle and Visitor pattern implementation in C#","This solution uses the notions that business interfaces know they must create a return and that their concrete implementations know what kind of concrete return to create.
Step 1 Split InvestmentReturn into two interfaces; the original minus the Business property and a new generic subclass:
public abstract class InvestmentReturn
{
    public double ProfitElement { get; set; }
    public abstract double GetInvestmentProfit();

    public double CalculateBaseProfit()
    {
        // ...
    }
}

public abstract class InvestmentReturn<T>: InvestmentReturn where T : IBusiness
{
    public T Business { get; set; }        
}

Step 2 Inherit from the generic one so you can use Business without casting:
public class RetailInvestmentReturn : InvestmentReturn<IRetailBusiness>
{
    // this won't compile; see **Variation** below for resolution to this problem...
    public RetailInvestmentReturn(IRetailBusiness retail)
    {
        Business = retail;
    }

    public override double GetInvestmentProfit()
    {
        ProfitElement = Business.GrossRevenue;
        return CalculateBaseProfit();
    }
}

Step 3 Add a method to IBusiness that returns an InvestmentReturn:
public interface IBusiness
{
    InvestmentReturn GetReturn();
}

Step 4 Introduce a generic sublcass of EntityBaseClass to provide the default implementation of the above method. If you don't do this you'll have to implement it for all the businesses. If you do do this it means all of your classes where you don't want to repeat the GetReturn() implementation must inherit from the class below, which in turn means they must inherit from EntityBaseClass.
public abstract class BusinessBaseClass<T> : EntityBaseClass, IBusiness where T : InvestmentReturn, new()
{
    public virtual InvestmentReturn GetReturn()
    {
        return new T();
    }
}

Step 5 Implement that method for each of your subclasses if necessary. Below is an example for the BookShop:
public class BookShop : BusinessBaseClass<RetailInvestment>, IRetailBusiness
{
    public double GrossRevenue { get; set; }
    public BookShop(double grossRevenue)
    {
        GrossRevenue = grossRevenue;
    }

    // commented because not inheriting from EntityBaseClass directly
    // public InvestmentReturn GetReturn()
    // {
    //     return new RetailInvestmentReturn(this);
    // }
}

Step 6 Modify your Main to add the instances of InvestmentReturn. You don't have to typecast or type-check because that's already been done earlier in a type safe way:
    static void Main(string[] args)
    {
        var allMyProfitableBusiness = new List<IBusiness>();
        // ...
        var investmentReturns = allMyProfitableBusiness.Select(bus => bus.GetReturn()).ToList();
        // ...
    }

If you don't want your concrete businesses to know anything about creating an InvestmentReturn__nly knowing that they must create one when asked__hen you'll probably want to modify this pattern to incorporate a factory that creates returns given input (e.g. a map between IBusiness implementations and InvestmentReturn subtypes).
Variation
All of the above works fine and will compile if you remove the investment return constructors that set the Business property. Doing this means setting Business elsewhere. That might not be desirable.
An alternative to that would be to set the Business property inside GetReturn. I found a way to do that, but it really starts to make the classes look messy. It's here for your evaluation as to whether its worth it.
Remove the non-default constructor from RetailInvestmentReturn:
public class RetailInvestmentReturn : InvestmentReturn<IRetailBusiness>
{
   public override double GetInvestmentProfit()
   {
       ProfitElement = Business.GrossRevenue;
       return CalculateBaseProfit();
   }
}

Change BusinessBaseClass. This is where it gets messy with a double-cast, but at least it's limited to one place.
public abstract class BusinessBaseClass<T, U> : EntityBaseClass, IBusiness
    where T : InvestmentReturn<U>, new()
    where U : IBusiness
{
    public double GrossRevenue { get; set; }

    public virtual InvestmentReturn GetReturn()
    {
        return new T { Business = (U)(object)this };
    }
}

Finally change your businesses. Here's an example for BookShop:
public class BookShop : BusinessBaseClass<RetailInvestmentReturn, IRetailBusiness>, IRetailBusiness
{
    // ...
}",https://stackoverflow.com/questions/21482850/refactoring-code-to-avoid-type-casting,6,0
Visual Studio 2012 Could not prepare files for refactoring,"When I try to do a simple refactoring (rename a function parameter) I receive the following message.
---------------------------
Microsoft Visual Studio
---------------------------
Could not prepare '<path to my project>\DesktopModules\Admin\ModuleCreator\Templates\C#\Class File\template.cs' for refactoring.
---------------------------
__   
---------------------------

I was able to do such refactorings few days ago. I do not know what happened.
I have the following software installed. I did not install anything since the refactoring worked last time.
Microsoft Visual Studio Ultimate 2012
Version 11.0.61030.00 Update 4
Microsoft .NET Framework
Version 4.5.50938

Installed Version: Ultimate

Architecture and Modeling Tools   04940-004-0038003-02455
LightSwitch for Visual Studio 2012   04940-004-0038003-02455
Office Developer Tools   04940-004-0038003-02455
Team Explorer for Visual Studio 2012   04940-004-0038003-02455
Visual Basic 2012   04940-004-0038003-02455
Visual C# 2012   04940-004-0038003-02455
Visual C++ 2012   04940-004-0038003-02455
Visual F# 2012   04940-004-0038003-02455
Visual Studio 2012 Code Analysis Spell Checker   04940-004-0038003-02455
Visual Studio 2012 SharePoint Developer Tools   04940-004-0038003-02455
ASP.NET and Web Tools   2012.3.41009
NuGet Package Manager   2.6.40627.9000
PreEmptive Analytics Visualizer   1.0
SQL Server Data Tools   11.1.20627.00
VisualSVN   4.0.7

I have a solution with 4 projects. One of them is a DNN Web Site, and the other 3 are DLLs used by the DNN project.
Please help.","Remove your ~/DesktopModules/Admin/ModuleCreator folder (unless you use it)
See https://dnnmodulecreator.codeplex.com/ for what it does, and  decide for yourself.",https://stackoverflow.com/questions/24164725/visual-studio-2012-could-not-prepare-files-for-refactoring,1,0
Application architecture for client to WCF service,"I'm curious about the correct way to architect an application that consists of the following (that needs refactoring):
Excel Addin
COM-Visible client library that includes WinForms and Methods exposed to Excel (calculation calls and form activation methods)
This then uses functionality in the client library to connect to the WCF services.
WCF services currently contain calculation logic, validation logic, database access via ORM tool.
i.e. Addin -> Winform/Direct call in client DLL -> WCF -> DB or calculation
Currently this exists in just 2 projects. My first though would be to re-architect as follows:
Client Side Projects

Excel ""View"" (Project.Client.Excel), this limits the level of COM visibility to one project.
WinForm ""view"" (Project.Client.UI)
Presentation for both sets of ""views"" (Project.Client.Presenter)

Server Side Projects

WCF ""view"" including data transfer objects? (Project.Server.WCF or Service)
Server side presenter (Project.Server.Presenter)?
Business Logic (Project.Business)
Data Access Layer (Project.DAL)

My questions are:

Where should the DTOs sit, in the WCF project or as their own library/project?
Where do the entity conversion routines belong (Data entity <> Business Entity <> DTO)? All in the business logic layer or some there and some in a server presenter?
What should the correct architecture be for this type of scheme?
Plenty else I've probably missed?

Part of the idea for the refactoring is to correct the architecture, separate concerns etc, and enable the inclusion of unit tests into the design.","This is how I would structure that but there is no 100% correct answer to this question. Many variations will make sense until they make your work comfortable.

Excel ""View"" (Project.Client.Excel), this limits the level of COM visibility to one project.
WinForm ""view"" (Project.Client.UI)
Presentation for both sets of ""views"" (Project.Presenter)
WCF Host (Project.Service) - Web Site with *.svc files if you host in IIS (no contracts here). No much business code here it is only for hosting methods implemented in BLL. 
Business Logic (Project.Business)
Data Access Layer (Project.DAL)
Contracts (Project.Contract) - Operation and Data Contracts. This is library used by both WCF client, server and BLL.
Shared (Project.Shared) - common helpers to better structure dependencies. 


Where should the DTOs sit, in the WCF project or as their own
  library/project?

Contracts

Where do the entity conversion routines belong (Data entity <>
  Business Entity <> DTO)?  All in the business logic layer or some there and some in a server presenter?

Business for middle/small size project. 

What should the correct architecture be for this type of scheme?

Your seems to be fine.
Server side presenter (Project.Server.Presenter) - this makes no sense for me because there is no GUI that consumes it",https://stackoverflow.com/questions/11023294/application-architecture-for-client-to-wcf-service,2,0
Downside to extjs __frame architecture_ (should I refactor to MVC pattern?),"I have an existing intranet webapp (solely internal) built with ExtJS using an ""iframe architecture"", i.e. it has a top menu and a tabpanel on an index page and about 30 other separate webpages that open as iframes ""tabs"" inside the main tabpanel.
There is not any particular reason to use iframes, everything is on the same domain and most of these other seperate pages are written using the ExtJS library almost solely in javascript. The html on almost all of them consists of empty HTML, HEAD and BODY tags.
I would really like to refactor this using the ExtJS MVC architecture and ditch the iframes but because ""everything works"" I can't justify taking the time to do this.
One thought I had but haven't been able to test is: with every one of these separate pages having it's own Ext.onReady event and viewport, etc, this webapp must be loading the full ExtJS framework for every iframe-tab it opens, seriously magnifying client resource usage. Can anyone confirm that this type of architecture would do that with the ExtJS framework? 
Is there any other very solid reasons this should be refactored?
Or, would refactoring to MVC architecture only give me easier code maintenance and no performance boosts? (being that currently everything works as intended)","Unfortunately I don't have project similar to what you have handy so I can't test it myself but here are my 2c... :)

I do think each page will initiate its own copy of ExtJs framework but I think it only has an affect on CPU and memory usage. Network traffic should not be very different as core ExtJs files will be cached. 
I do recommend to check out network traffic while running this app as you will see how exactly browser handles all this. You might want to add some additional logic into core ExtJs functions to confirm if framework is in fact instantiated several times. 
If end users experience some performance issues - it might be very good point to justify re-factoring. Otherwise it's kind of hard. Unless of course you have some plans of expanding functionality in near future and planning on continue working on this application.",https://stackoverflow.com/questions/10237023/downside-to-extjs-iframe-architecture-should-i-refactor-to-mvc-pattern,1,0
Refactoring / Re-engineering a large system [closed],"Closed. This question needs to be more focused. It is not currently accepting answers.
                            
                        










Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                        
Closed 2 years ago.



I am currently in the process of re-engineering a system which is written in struts 1.3 / PostgreSQL and Hibernate.
The system is architecture'd as an MVC system with presentation logic in JSP's / Actions having View Logic and EJB's having Business logic. 
They have used Hibernate as a mapping layer,but some areas still use JDBC functions which have large number of business logic written inside them
The system has about 800k lines of code. 
Though the initial idea had been to have only view based logics in action classes and JSP's, there is huge amount of business logic also in the Actions and JSP's. 
The database columns are having ambiguous  and duplicated columns as with each requirement, the columns have just been added. This has a nasty effect on the application when picking data, the same data item in two different reports may show different values.
There are no JUnit test cases and documentation is moderate.
In some areas I have managed to remove the business logic out of JSP's / Action tier and bring them to EJB tier.
However my question is where would be the ideal place to start the refactoring,
is it first refactoring the code in JSP's / Actions then move to EJB's for re-engineering logics and finally move to DB?
Or is to start from the DB, refactor the DB then come up the stack?
Is start at the enterprise business logic layer and refactor the business logic?
Where should I start and what should my approach be. I am now going through Martin Fowler's books and lot of articles on refactoring and re-engineering. But I would really appreciate some guidance.","I would start by adding unit tests.  I would use these to ensure that what worked still works after you refactor it.  This will allow you to take on more significant changes.  
I would also focus on the area which you intend to work on anyway.  Improve the testability and design of these sections of code as you work on them. i.e. make this part of the functional requirements.",https://stackoverflow.com/questions/20854041/refactoring-re-engineering-a-large-system,5,0
Refactoring with patterns,"I haven't had a lot of practice with patterns and application architecture. In a nutshell, I have to find certain attributes which object features. Some code will better describe task:
IAttribute {
  IAttribute analyze(IFunction func);
}

//up to 10 different attributes
ArgumentsAttribute implements Attribute {
  Map<String, ArgType> args = new HashMap<>();
  IAttribute analyze(IFunction func) {
    for (Argument arg : func.getArgs()) {
      args.put(arg.getName(), arg.getType());
    }

    if (!args.isEmpty()) return this;
    return null;
  }
}

ReturnAttribute implements Attribute {
  IAttribute analyze(IFunction func) {
    if (func.hasReturn) return this;
    return null;
  }
}


AttributeAnalyzer {
  List<Attributes> analyzeAttributes(IFunction func) {


    List<IAttribute> attributes = new ArrayList<IAttribute>();
    attributes.add(new ArgumentAttribute());
    attributes.add(new ReturnAttribute());
    ...

    for (IAttribute attr : attributes) {
        attr = attr.analyze(func);
        if (null == attr) attributes.remove(attr);
    }

    return attributes;
  }
}

However, this implementation seems to be a little strange. I don't like the fact that Attribute is sort of holder, but it has to implement method to find itself. In my opinion, the best practice would be an opportunity to overload static methods, but obviously its not possible. In this way, we would separate holder from analyzing logic without adding new abstractions(maybe I am not right).
IAttribute {
  static IAttribute analyze();
}

ConcreteAttribute1 {
  int x = 0;
  static IAttribute analyze() {
    ...
    if (x != 0) return new ConcreteAttribute1();
    return null;
  }
}

ConcreteAttribute2 {
  String s = """";
  static IAttribute analyze() {
  ...
  if (!s.equals("""")) return new ConcreteAttribute2();
  return null;
  }
}


AttributeAnalyzer {
  List<Attributes> analyzeAttributes() {


    List<IAttribute> attributes = new ArrayList<IAttribute>();
    attributes.add(ConcreteAttribute1.analyze());
    attributes.add(ConcreteAttribute2.analyze());
    ...

    for (IAttribute attr : attributes) {
        if (null == attr) attributes.remove(attr);
    }

    return attributes;
 }

}
In addition, I have to filter spoiled Attributes. So, are there any ways of refactoring to make this code looks better?","If you have a distinct analyze function for each concrete attribute, with little or no overlap, then your initial code sample may not be all that bad. However, I would then change the signature of the method to boolean analyze().
If there is more overlap in the way attributes are analyzed then you might consider a single method boolean analyze(IAttribute) inside your AttributeAnalyzer class (or in a dedicated class).",https://stackoverflow.com/questions/43343032/refactoring-with-patterns,1,0
"MySQL: How do I test my database architecture (foreign key consistency, stored procedures, etc)","I'm just about designing a larger database architecture. It will contain a set of tables, several views and quite some stored procedures. Since it's a database of the larger type and in the very early stage of development (actually it's still only in the early design stage) I feel the need of a test suite to verify integrity during refactoring.
I'm quite familiar with testing concepts as far as application logic is concerned, both on server side (mainly PHPUnit) and client side (Selenium and the Android test infrastructure).
But how do I test my database architecture?

Is there some kind of similar testing strategies and tools for databases in general and MySQL in particular?
How do I verify that my views, stored procedures, triggers and God-knows-what are still valid after I change an underlying table?
Do I have to wrap the database with, say, a PHP layer to enable testing of database logic (stored procedures, triggers, etc)?","To test a database, some of the things you will need are: 

A test database containing all your data test cases, initial data, and so on. This will enable you to test from a known start position each time. 
A set of transactions (INSERT, DELETE, UPDATE) that move your database through the states you want to test. These can themselves be stored in the test database.
Your set of tests - expressed as queries on the database, that do the actual checking of the results of your actions. These results will be tested by your test suite.
Exceptions can be thrown by a database, but if you are getting exceptions, you are likely to have much more serious concerns in your database and data. You can test the action of the database in a similar fashion, but except for ""corner cases"" this should be less necessary, as modern database engines are pretty robust at their task of data serving.

You should not need to wrap your database with a PHP layer - if you follow the above structure it should be possible to have your complete test suite in the DML and DDL of your actual database combined with your normal test suite.",https://stackoverflow.com/questions/10958663/mysql-how-do-i-test-my-database-architecture-foreign-key-consistency-stored-p,2,0
java: refactoring case (M and C of MVC?),"I have a situation in Java where I have an external device that I want to communicate via serial I/O. I know how to do this, but I'm now in a refactoring mode to make sure I've got a maintainable software package, & was looking for advice on what to do / not to do at a high level (specific questions below)
Conceptually, let's say I have a low-level DeviceIOChannel interface with several methods (getInputStream, getOutputStream, and some other ones for controlling connection / disconnection detection, etc.), implemented by one or more classes that handle the I/O for various data link types (RS232, TCPIP, etc). Some of my software, let's call it a Device class, is devoted to managing the I/O (parsing the input, constructing output, managing low-level state machines), but without knowing the details of how DeviceIOChannel does its thing (so I can use it with RS232 or TCPIP without having to change the Device class). So I'll probably pass in DeviceIOChannel as a parameter to Device's constructor. I also would like to expose some sort of data model to the outside world.

Does my partitioning of DeviceIOChannel  / Device sound right?
Device needs to be doing some things actively on a worker thread. What's the best way to set this up? Should I have it create and manage its own Thread or ScheduledExecutorService? Or should I pass in a ScheduledExecutorService as a construction parameter?
any thoughts (links to good articles on the web would be ideal!) about how to whether or not the Device class should have a startup() method distinct from construction? (doing all the initialization in construction makes me nervous... seems like class instance construction should be quick, and then lengthy stuff should be reserved for an init or startup phase that comes later.) 
What about whether to have a Device class with a pair of shutdown / restart methods, vs. no shutdown + requiring a new Device instance to be created?
I'm still new to MVC architecture: would it make sense to create a DeviceDataModel interface that Device implements, or should I have some separate class DeviceDataModel that somehow has two-way communication with the Device class?","To answer your question one point at a time.

Yes, it does sound reasonable.
Yes, passing an abstraction for your threading will definitely make the class much more testable. Two dependencies in a constructor doesn't sound unreasonable.
Having a startup method adds more overhead (on method calls you have to check if startup was called, you can't assume it), however I agree that such network activity on a constructor always looks weird to me when I see it. I think it is really a matter of style, but one advantage of a separate method is that if you need to debug or log state before starting, your Device class can express its configuration as an instance, rather than making it impossible for something else to get a handle on the object.
I think the answer to this question depends almost entirely on how you handle #3. An API with no surprises would not have a way to restart if it started in its constructor, and would have a way if it started via a method.
Given the network IO nature of this class, a DeviceDataModel interface is going to make the rest of the code much more testable. However, it need not be implemented directly by the Device class, but rather returned from a method of the Device class as an inner class, so it could communicate with the device class easily, but still be something that can be mocked or stubbed during testing. At least, as long as serialization of the the DeviceDataModel isn't a requirement.",https://stackoverflow.com/questions/2156783/java-refactoring-case-m-and-c-of-mvc,1,0
Method paratemers refactoring in software-design,"I guess the question applies to any OO-Language.
According to software-design guidelines(1), fields (= variables here), what comes together in the method parameter list , let it be for example 
String bikeName;
Wheel wheel1;
Wheel wheel2;

Used in the lots methods together, for example
firstMethod(String bikeName, Wheel wheel1, Wheel wheel2) ...
secondMethod(String bikeName, Wheel wheel1, Wheel wheel2) ...

according to softwaredesign, is is better to refactor the code to have the Class Bike with this fields, and methods:
firstMethod(Bike bike)
secondMethod(Bike bike)

This operation is trivial and can even be done automated to any code by a one-button click refactoring function.
Consider the case, firstMethod has following in his body:
firstMethod(Bike bike){

Wheel rwheel = bike.getRearWheel();
int somePower = rwheel.getPower(); //whatever

secondMethod(bike);
}


secondMethod(Bike bike){
...
Wheel rwheel = bike.getRearWheel();
// or even:
int somePower = rwheel.getPower(); //whatever


}

But in the secondMethod, the bike.getRearWheel() or further rwheel.getSpeed() is called again. 
In order to do not call the same methods twice (also a software-design guideline), one could change a method to 
secondMethod(Bike bike, int someSpeed )...

But this is can become cumbersome, especially in the real case with lost of variables and is against the guideline (1).
What variabt would you choose in the praxis?","guidelines are not solutions and cannot cover every situation. My (strictly personal) recommendation is to try it as follows:
firstMethod(Bike bike){
    ... // other stuff
    int somePower = bike.getRearWheek().getPower();
    ...
    secondMethod(bike, somePower);
    ...
}

Then define:
secondMethod(Bike bike){
    secondMethod(bike, -1);
}

secondMethod(Bike bike, int knownPower){
    ...
    int somePower = (knownPower < 0)? bike.getRearWheel().getPower() : knownPower;
    ...
}

Or along those lines, anyway.",https://stackoverflow.com/questions/36763109/method-paratemers-refactoring-in-software-design,1,0
DbUnit.NET Alternatives,"Are there other '.NET (2.0) Unit Testing Framework for Database Access Code' besides DbUnit.NET?
I've been trying DbUnit.NET and some things are not supported. Also, the project seems to be in alpha since 22nd May 2006...
We are refactoring our architecture to be able to do tests with mocking frameworks, but until that work is done I'd like to have a framework like DbUnit.NET (but better).","I ran into this problem a few years ago.  I was annoyed at the state of DBUnit.Net.  It was missing features that were important to me.  Thanks to IKVM, it's not very difficult to use the normal Java version of DBUnit from dotnet.  As a matter of fact, I'm running C# integration tests right now that are using the original DBUnit.  Here's how I converted the java version of DBUnit into a .Net assembly:

Download IKVM
Place the following jars into a common directory: commons-collections-3.2.jar commons-logging-1.1.jar junit-4.1.jar commons-lang-2.2.jar dbunit-2.2.jar sqljdbc.jar

Now, from the command line with a working directory of the common jar directory:
ikvmc -target:libary -keyfile:yoursignature.snk -debug -version:2.2.0.0 -out:dbunit.dll *.jar

You can get necessary libraries from the following locations:

commons-*    Apache Commons
dbunit.jar   DbUnit homepage
sqljdbc.jar  MSDN
junit.jar    Junit homepage

If you are not using SQL Server as your database, then replace sqljdbc.jar with the appropriate JDBC driver.  To use DBUnit directly from your .Net code, include dbunit.dll and the appropriate IKVM assemblies.  
The jar versions I have given here are old.  My notes on this subject are almost three years old.  Newer versions will probably work, but I have not tried them.",https://stackoverflow.com/questions/780614/dbunit-net-alternatives,3,0
recurring scheduler in RxJava,"I wanna remove old stuff from the ListView, so I defined recurring scheduler as a handler in the Activity.
private Handler handler = new Handler();
private Runnable runnableCode = new Runnable() {
    @Override
    public void run() {
        doSomethingOnRecyclerView();
        handler.postDelayed(runnableCode, 2000);
    }
};
handler.post(runnableCode);

It works, however, I'm refactoring the app to use RxJava, mvp and dagger. How to do recurring scheduler in RxJava?
Where to keep it in MVP architecture, in presenter?","I'd have the handler live in the presenter class and call the appropriate view methods at each interval. With RX you can have an emitter (or flowable?) that fires every x seconds. This will accomplish the same thing that you have in your code.
Be sure to wire up the scheduler to the presenter's lifecycle",https://stackoverflow.com/questions/43170732/recurring-scheduler-in-rxjava,1,0
what are the pros and cons using LINQ to SQL?,performance ? architecture ? the system in question will always be a MS SQL database there is an existing DAL but there will be refactoring done on the system and Linq-to-SQL seems a good candidate to avoid maintaining a DAL or SP's,"For advantages, see the accepted answer on this related question: What are the advantages of LINQ to SQL?.
For disadvantages, you may be interested in the discussion at this related question: linq2sql disadvantages",https://stackoverflow.com/questions/3575222/what-are-the-pros-and-cons-using-linq-to-sql,2,0
What are the advantages of LINQ to SQL?,"I've just started using LINQ to SQL on a mid-sized project, and would like to increase my understanding of what advantages L2S offers.
One disadvantage I see is that it adds another layer of code, and my understanding is that it has slower performance than using stored procedures and ADO.Net.  It also seems that debugging could be a challenge, especially for more complex queries, and that these might end up being moved to a stored proc anyway.
I've always wanted a way to write queries in a better development environment, are L2S queries the solution I've been looking for?  Or have we just created another layer on top of SQL, and now have twice as much to worry about?","Advantages L2S offers: 

No magic strings, like you have in SQL queries
Intellisense
Compile check when database changes
Faster development
Unit of work pattern (context)
Auto-generated domain objects that are usable small projects
Lazy loading.
Learning to write linq queries/lambdas is a must learn for .NET developers.

Regarding performance:

Most likely the performance is not going to be a problem in most solutions. To pre-optimize is an anti-pattern. If you later see that some areas of the application are to slow, you can analyze these parts, and in some cases even swap some linq queries with stored procedures or ADO.NET.
In many cases the lazy loading feature can speed up performance, or at least simplify the code a lot.

Regarding debuging:

In my opinion debuging Linq2Sql is much easier than both stored procedures and ADO.NET. I recommend that you take a look at Linq2Sql Debug Visualizer, which enables you to see the query, and even trigger an execute to see the result when debugging. 
You can also configure the context to write all sql queries to the console window, more information here

Regarding another layer:

Linq2Sql can be seen as another layer, but it is a purely data access layer. Stored procedures is also another layer of code, and I have seen many cases where part of the business logic has been implemented into stored procedures. This is much worse in my opinion because you are then splitting the business layer into two places, and it will be harder for developers to get a clear view of the business domain.",https://stackoverflow.com/questions/593808/what-are-the-advantages-of-linq-to-sql,5,0
linq2sql disadvantages,"I am hearing a lot of rumours that Linq2Sql is not going to be supported any more in the next version of .net. I like Linq2Sql a lot and find it very easy / lightweight to work with. I can understand some of the problems people have had with it (ppl used to nhibernate...) but used correctly I think most problems can be solved. I currently use it in all my projects now and I would hate to see that it isn't going to be supported any more.
So list the disadvantes you see with Linq2Sql","If there were another version of Linq to SQL, here's my feature wishlist:
Things I actually wanted whilst building my last Linq-to-SQL project (and had to work around manually in most cases)

Many-many associations 
Better visual designer (including a ""refresh table"" feature)
Control over cascade delete/update/SET NULL that doesn't involve hacking the XML
Specific mappings for culture/string comparisons into SQL COLLATE statements (e.g. specifying whether passwords should be case sensitive or not, rather than relying on the default collation of the underlying database.)

Things I might want but haven't actually needed yet:

Support for ordered collections (persisting lists to the DB so they are automatically retrieved in the same order they were persisted)",https://stackoverflow.com/questions/339786/linq2sql-disadvantages,4,0
do I really need to write 'ngInject',"I'm currently refactoring my code to be be compliant with component-base architecture (for further migration to Angular).
Do I really need the 'ngInject' string and the $inject at the end?
In my current code I'm using ng-annotate with gulp and it seems like it deals with all the dependencies injection for minifications.
   class SecuritySettingsCtrl {
    constructor($scope) {
        'ngInject';
        _self = this;

        _self.$onInit = function () {
            //....
            }

        };
    }

SecuritySettingsCtrl.$inject = ['$scope'];
export default SecuritySettingsCtrl","You need either 'ngInject' (if it configured correctly and works) or .$inject = ...,
or export default ['$scope', ...] 
JavaScript minifiers in general know nothing about Angular. So they minify 
function ctrl($scope) { $scope.name = 'test'}

to something like
function c(a) { a.name = 'test'}

If you add ng-anotate or angularjs-annotate or whatever plugin you like and it works, it will add ctrl.$inject = ['$scope'] for you, so you have:
function c(a) { a.name = 'test'}
c.$inject = ['$scope']

If your minified app works - you configured ngInject (or whatever else) correctly.
P.S. Using 'ngInject' together with manual inject SecuritySettingsCtrl.$inject of course makes no sense.
""It works"" means it works :D you can check it manualy in result minified js file, with test or in real app.",https://stackoverflow.com/questions/51930397/do-i-really-need-to-write-nginject,1,0
should we pack our 3rd party libraries with our components?,"We are doing some architecture refactoring. We are a SaaS company so all deploys are to our own self managed servers. Current model packs all of our binaries along with 3rd party libraries we use into ears, wars, tars, etc. These packages include all of the libraries they depend on. 
When they are deployed they are manually exploded (untar'd) or picked up by whatever target container they were built for. 
Since the libraries don't change much we are wondering if it's a better idea to deploy the libraries before hand, as part of environment setup, and update them as needed. We aren't leaning one way or the other, and I am just looking for some feedback.","It is generally better to pack libraries with the ears,wars, etc.  Some reasons are:

It saves time when you configure a new server machine.  If you don't package your dependencies with your deployable it can take a long time to get all the correct libraries on a new target server machine.
You can deploy different .war files to an application server that depend on different versions of the same library
Upgrading a library is straight forward if you package your dependencies with your deployable (simply redeploy).  If your libraries are separate you have additional steps to deploy a new library (and another place where things can go wrong).
You can be sure that a .war that is deployed on a test environment will behave the same in a prod environment if you include dependencies.  Subtle differences in test/prod environments with centralised libraries and versions often cause problems. 
Your dependency list is really explicit.  You can for example do an open source license audit.  If you don't package dependencies with your package, you never know...",https://stackoverflow.com/questions/2105463/should-we-pack-our-3rd-party-libraries-with-our-components,1,0
How to manage huge sized classes and refactoring in unit testing (junit),"In our project which has huge sized classes and sub relations we have been developing for months. Also, we have been developing junit test cases.
Automatic unit tests are good in general, but in real life it is not easy as we think. Managing the unit test architecture is easier than creating mock objects and stubs etc. Also, we are testing the dao and service layers.
The problem is that our classes have so many attributes. (I know it's not a good object oriendted design but it's legacy architecture.)
For example; customer class has 58 attributes and it's related to address, marsaccounts and etc. Totally if you want to test this class you have to create inputs, inputs with 90 or more attributes.
Our architecture has many  business rule on Customer, so that we have to create more than 50 customer inputs to test every rule, method or flow. 
In short, you have to create 4500 (90 x 50) attributes for all, but less for a reliable test (only necessary attributes). 
Preparing the test inputs are painful and annoying. Imagine, 2 columns added to Customer object and they store critical values.  it seems easy, but refactoring the test inputs are     soul-destroying.
How can I manage the test stub and How can I overcome the huge input set ?
Regards.","it looks like you have only two options: make less attributes relevant or learn to easily set-up tests with large number of attributes.
less attributes: refactor. and no one will give you much more detailed help because we don't know your business. try to find smaller groups of attributes that control some logic and test that logic using only those attributes.
easy test set-up: you can use customer builders. by default they create customer with some standard/most common settings and you tune the result however you want like 
customer = makeCustomer().withActiveStatus(false).withDebit(3000).build()

Then, when new attribute appears you just have to change makeCustomer() in one place.
you can also use parameterized tests to define a test case as a one liner or load data from spreadsheet which may be easier to maintain.
Often when new attribute appears it's not that it changes everything completely. usually it just adds new behaviour in one spot when that attribute is non-standard. therefore usually it's just adding the default attribute to the builder and a few tests that override that attribute
another way to make tests easy is to do property testing (QuickCheck family) although not always it's easy to use it in business logic",https://stackoverflow.com/questions/33080488/how-to-manage-huge-sized-classes-and-refactoring-in-unit-testing-junit,5,0
One vs. multiple entry points in web apps,"I'm working on a major refactoring of a web application.
It currently consists of some common scripts included from all pages (common, functions, header, footer...), plus entry points for all kinds of actions (login.php, viewthis.php, viewthat.php).
Now I'm thinking of introducing a front controller and using some kind of model architecture instead.
I'd like to know:
What are the pros and cons of both architectures? Any special benefits? What should be used in general (if any)?","I'd recommend a controller. You'll gain in centralized validation of in-data, easier adaptation for mod_rewrite etc.
Can't see any real point of the other technique. It's just old.
regards,
/t",https://stackoverflow.com/questions/4552926/one-vs-multiple-entry-points-in-web-apps,1,0
ASP Best Practices Overhead,"Assume a typical, database and session driven ASP application developed using best practices just before the first release of .NET. Assign the amount of effort required for seasoned professionals to implement it properly and efficiently to be 1.0.  
What would you estimate to be the amount of effort required to maintain it using best non-aggressive refactoring practices from then until now?
Would that have been a good investment (Option A)? Or would it have been better (option B) to have done new work with current best practices, but no refactoring? Or (Option C) would it have been best to keep the architecture consistent by doing new work with the old architecture?
In light of your choice, what should you be doing from now on?","I don't think you're giving us enough information to give a sensible answer.

Is this a product or an in-house system?
What is the history?
Is there a current project or current budget?
Are there any politics, historical or current?
Is the business domain behind the system well understood and documented?
Etc, etc.

In general and all other issues being equal, I would always start with option C to make sure that decisions are driven by the business rather than by technologists.",https://stackoverflow.com/questions/313506/asp-best-practices-overhead,1,0
Refactoring advice: maps to POJOs,"I currently am part of a project where there is an interface like this:
public interface RepositoryOperation {

    public OperationResult execute(Map<RepOpParam, Object> params);  
}

This interface has about ~100 implementers.
To call an implementer one needs to do the following:
final Map<RepOpParam, Object> opParams = new HashMap<RepOpParam, Object>();
opParams.put(ParamName.NAME1, val1);
opParams.put(ParamName.NAME2, val2);

Now I think that there is obviously something wrong with anything with a<Something, Object> generic declaration.
Currently this causes a caller of a OperationImpl to have to actually read the code of the operation in order to know how to build the argument map. (and this is not even the worst of the problems, but I don't want to cite them all since they are fairly obvious)
After some discussion I managed to convince my colleagues to let me do some refactoring.
It seems to me that the simplest 'fix' would be to change the interface like so:
public interface RepositoryOperation {

    public OperationResult execute(OperationParam param);  
}

After all the concrete operations will define (extend) their own OperationParam and the needed arguments would be visible to everybody. (which is the 'normal way' to do things like that IMHO)
So as I see it since the interface implementers are quite numerous I have several choices:

Try to change the interface and rewrite all the Operation calls to use objects instead of maps. This seems the cleanest, but I think that since the operations are a lot it might be too much work in practice. (~2 weeks with tests probably)
Add an additional method to the interface like so:
public interface RepositoryOperation {
    public OperationResult execute(Map<String, Object> params);
    public OperationResult execute(OperationParam params);  
}

and fix the map calls whenever I come across them during functionality implementation.
Live with it (please no !).

So my question is.
Does anyone see a better approach for 'fixing' the maps and if you do would you fix them with method 1 or 2 or not fix them at all. 
EDIT:
Thanks for the great answers. I would accept both Max's and Riduidel's answers if I could, but since I can't I'm leaning a bit more towards Riduidel's.","I can see a third way.
You have a map made of <RepOpParam, Object>. If I understand you correctly, what bothers you is the fact that there is no type checking. And obviously, it's not ideal. But, it is possible to move the type-checking issue from the whole parameter (your OperationParam) to individual RepOpParam. Let me explain it.
Suppose your RepOpParam interface (which currently seems like a tagging interface) is modified as it :
public interface RepOpParam<Value> {
    public Value getValue(Map<RepOpParam, Object> parameters);
}

You can then update modern code by replacing old calls to 
String myName = (String) params.get(ParamName.NAME1);

with new calls to
String myName = ParamName.NAME1.getValue(params);

The obvious collateral advantage being that you can now have a default value for your parameter, hidden in its very definition.
I have however to make clear that this third way is nothing more than a way to merge your two operations of the second way into only one, respecting old code prototype, while adding new powers in it. As a consequence, I would personnally go the first way, and rewrite all that ""stuff"", using modern objects (besides, consider taking a look at configuration librarires, which may lead you to interesting anwsers to this problem).",https://stackoverflow.com/questions/4440198/refactoring-advice-maps-to-pojos,3,0
Caching architecture for Memcached/wcf/web/ravendb,"I have an architecture question - related to my ravendb based setup.
I have the following:
ravendb -> wcf service -> (web/iphone/android)
the web/iphone/android level actually has (at the moment - this is growing) connections to 7 wcf services
at the moment the 7 services talk to the same ravendb - this is likely to be segmented in a future refactoring blitz as they don't need to be on the same instance - there is minimal - if none at all - crossover of the model.
My question is this:
I am looking at using memcached - at which points (i have little experience setting this up) can i / should i use memcached? 
between ravendb and wcf?
between wcf and (web/iphone/android)?
between all?
am i likely to run into stale data issues? is this taken care of or am i over simplifying things?","As many people will tell you: Premature optimization is the root of all evil (and they are all quoting Donald Knuth I think). So wait when you have performance issues before doing anything (You don't need to wait for the system to crush. Wait till you see 90% utilization of your resources)
That being said, You should use memcached (or any kind of caching for that matter) when you expect to use the cached data before it is being invalidated (The improvement factor will change upon many other factors like: the operation cost and the frequency in which the data accessed)  
To answer your ""where"" questions that really depends where you will be saving most on resources and it is really application specific and can not be answered here.",https://stackoverflow.com/questions/4766638/caching-architecture-for-memcached-wcf-web-ravendb,2,0
Is the Roslyn model so C#/VB.NET centric that it precludes XAML analysis now and in the future?,"I have just read the blog entry by JetBrains (Resharper) that suggests that Roslyn could never do XAML analysis:

Another core difference is that Roslyn covers exactly two languages, C# and VB.NET, whereas ReSharper architecture is multilingual

(quote from resharper blog)
For the uninitiated Resharper can do very good static analysis on XAML code allowing code completion and refactoring together with C#.
I am curious. Is the Roslyn architecture general enough to be extended to other languages than C# and VB.Net such as XAML or is it very specific.
To avoid suggesting this is opinion based I am looking for evidence in the source. Obviously any code can be refactored /re-engineered over time to fit some other purpose but I'm only interested in current evidence in the source or references to quotes from Roslyn develepors indicating that there is intent to extend Roslyn as an analysis engine to other languages such as XAML.","Hard to teach ""languages"" in a paragraph or two. People tend top call many things ""language"" and in some very general sense they might be but in real-world of programming ""language"" means programming language. In that world, XAML is not language at all :-) because you can't write code in it. 
XAML is a data description format and yes in a very general sense it can be called a language (has some basic constructs, their combination rules, and the result denotes something meaningful). But it doesn't have even its own syntax - it's XML. So it belongs to a group with say MSBuild and HTML. The kind of analysis you could, in theory, do with that is entirely different and very much related to the actual domain that the format describes. 
For MSBuild files, you could write some code to analyze dependencies it denotes and says try to find and point out holes in that but all you need for that are a few standard .NET classes. You don't need a parser, don't have any code generation, etc. You just load XML like any other and you already have everything and cam cruise around and dig out relations. Same for XAML. For everything XML-based ""Roslyn"" is XmlDocument, or XPathDocument.
Roslyn breaks it basks to produce a tree structure that allows you the same kind of cruising freedom that you get just by loading the XAML document into XmlDocument.",https://stackoverflow.com/questions/23005827/is-the-roslyn-model-so-c-vb-net-centric-that-it-precludes-xaml-analysis-now-and,2,0
What architecture for a TypeScript project?,"I'm starting a personal web project and I would like to use the new TypeScript release to build JS part.
I'm wondering what would be the best architecture to build one: usually, I like having one class per file. Here, with what I read from official doc, it would be hard to match this pattern (with reference tags or import keyword)...
I would like to know your tricks to avoid long dependency edits each time you're refactoring something. 
I know it's possible to compile your whole project in a single file but, for my part, I don't like this for performances (and you're still using reference tags). I prefer using requireJS but with TS, you're using relative paths and strings to import your classes, what annoy me a bit ^^
Feel free to give me some advices :) I'll appreciate your help!","I haven't documented it perfectly but grunt-ts can help you with the multiple import problem (https://github.com/grunt-ts/grunt-ts/issues/85#issue-29515541). 
What you do is type in ///ts:import=filename and grunt-ts keeps the generated import statement in sync as you refactor your project.
You can also load entire folders using the same syntax i.e. ///ts:import=foldername and even generate an index.ts for a folder using the same syntax and them import that index.ts instead of importing the folder everytime. 
To get started with grunt-ts just follow the readme : https://github.com/grunt-ts/grunt-ts#grunt-ts
Advantages : 

faster to get started. Relative paths are hard to type
easier to refactor (as long as you keep the filenames the same). And when you do change the filenames .. you get compile errors.",https://stackoverflow.com/questions/23310033/what-architecture-for-a-typescript-project,1,0
what is the best way to visualize technical investment for the business [closed],"Closed. This question needs to be more focused. It is not currently accepting answers.
                            
                        










Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                        
Closed 5 months ago.



we have a number of functional deliverable planned for 2010 but we also have a technology agenda (architectural refactorings, consolidation, upgrade a platform).  any suggestions on the best way to include these in a roadmap to help the business understand why they are important.
one option is just saying trust us as this is the right thing to do to keep everything healthy but i would like some better visualization if possible","Being a bit cynical about it, I would say phrase every thing in terms of money. If you can't re-write your technical agenda in terms of money made or money saved, then why are you doing it at all?
Also, there is an article on ""technical debt in financial terms"" that I found very useful at:
http://forums.construx.com/blogs/stevemcc/archive/2007/11/01/technical-debt-2.aspx 
One of the more interesting points, to me, is ""One of the important implications of technical debt is that it must be serviced, i.e., once you incur a debt there will be interest charges.""
There is a brief follow up at 
http://forums.construx.com/blogs/stevemcc/archive/2007/12/12/technical-debt-decision-making.aspx",https://stackoverflow.com/questions/1741578/what-is-the-best-way-to-visualize-technical-investment-for-the-business,2,0
Why does android.arch.navigation cause Program type already present: android.support.v4.os.ResultReceiver$1?,"I am using Android Studio version
Android Studio 3.2 Canary 14
Build #AI-181.4668.68.32.4763614, built on May 4, 2018
JRE: 1.8.0_152-release-1136-b02 x86_64
JVM: OpenJDK 64-Bit Server VM by JetBrains s.r.o
Mac OS X 10.11.6

While investigating the new Architectural navigation components android.arch.navigation I have encountered this build failure.
AGPBI: {""kind"":""error"",""text"":""Program type already present: android.support.v4.os.ResultReceiver$1"",""sources"":[{}],""tool"":""D8""}
:app:transformDexArchiveWithExternalLibsDexMergerForDebug FAILED
FAILURE: Build failed with an exception.
* What went wrong:
Execution failed for task ':app:transformDexArchiveWithExternalLibsDexMergerForDebug'.
> com.android.builder.dexing.DexArchiveMergerException: Error while merging dex archives: ...
...
  Program type already present: android.support.v4.os.ResultReceiver$1

My app gradle build resembles:-
apply plugin: 'com.android.application'
apply plugin: ""androidx.navigation.safeargs""

android {
    compileSdkVersion 'android-P'
    defaultConfig {
        applicationId ""com.research.frager""
        minSdkVersion 19
        targetSdkVersion 28
        versionCode 1
        versionName ""1.0""
        testInstrumentationRunner ""androidx.test.runner.AndroidJUnitRunner""
    }
    buildTypes {
        release {
            minifyEnabled false
            proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.pro'
        }
    }
}

dependencies {
    def nav_version = ""1.0.0-alpha01""

    implementation ""android.arch.navigation:navigation-fragment:$nav_version""
    implementation ""android.arch.navigation:navigation-ui:$nav_version""

    // optional - Test helpers
    androidTestImplementation ""android.arch.navigation:navigation-testing:$nav_version""

    implementation fileTree(dir: 'libs', include: ['*.jar'])
    implementation 'androidx.appcompat:appcompat:1.0.0-alpha1'

    implementation 'androidx.constraintlayout:constraintlayout:1.1.0'
    implementation 'androidx.lifecycle:lifecycle-extensions:2.0.0-alpha1'
    testImplementation 'junit:junit:4.12'

    androidTestImplementation 'androidx.test:runner:1.1.0-alpha2'
    androidTestImplementation 'androidx.test.espresso:espresso-core:3.1.0-alpha2'
}

and project level gradle build:-
buildscript {

    repositories {
        google()
        jcenter()
    }
    dependencies {
        classpath 'com.android.tools.build:gradle:3.2.0-alpha14'
        classpath ""android.arch.navigation:navigation-safe-args-gradle-plugin:1.0.0-alpha01""
    }
}

allprojects {
    repositories {
        google()
        jcenter()
    }
}

task clean(type: Delete) {
    delete rootProject.buildDir
}

I have tried refactoring to AndroidX, however I get a message stating No usages found in project, so why is this ""v4"" class still being mentioned?","I've been looking at this issue and I have sorted the main issue here by excluding the support package when adding the dependency to the library, doing this:
implementation(""android.arch.navigation:navigation-fragment:$nav_version"") {
  exclude group: 'com.android.support'
}

That would allow you to run the application. However, this artifact is using the support artifacts rather than the androidx artifacts. Looking at the documentation, we can see that NavHostFragment is extending support.v4.Fragment
https://developer.android.com/reference/androidx/navigation/fragment/NavHostFragment
So, in short, you are presented with three options, as far as I can see. The first one is to drop the androidx artifacts and use the support ones which eventually depends on how big your app is. 
The second option is to drop the navigation library and go back to the classic way of dealing with navigation which, I guess, is probably undesirable for you. 
The third option is to implement a navigation host of your own which I don't know how much work it would be. 
This response will remain accurate until Google releases the androidx version of the library which I am surprised they haven't already. 
Hope that was helpful.",https://stackoverflow.com/questions/50365581/why-does-android-arch-navigation-cause-program-type-already-present-android-sup,1,0
Is UML the correct language for making software blueprints? [closed],"Closed. This question is opinion-based. It is not currently accepting answers.
                            
                        










Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.
                        
Closed 2 years ago.



I was having a conversation last week with a coworker about architecture (real architecture, as in designing buildings).  During our talk it came up that architectural blueprints give an architect, civil engineer, and contractor all the detail they need to build something.  It got both of us thinking about the state of software engineering and that there is no universally adopted approach for describing the design of software.  
We have UML, but I find that it is often hard to convey enough detail without the diagrams being overly complex.  Are there good examples of large software that was designed out using elaborate UML diagrams?
Then again, is having a large set of software blueprints even useful?  After all refactoring and rebuilding software is much cheaper than rebuilding a skyscraper.  Are architectural blueprints the wrong analogy for software design?  Is there a better analogy that you can think of?","Architectural blueprints are a nearly-precise representation of the actual house. They are not - usually - an abstraction conforming to a model of how houses should look, they are a representation of how the house will be. 
Contrast that with UML/Flowcharts/Rational Rose/Methodology-of-the-month - those are models. They abstract away implementation details, and presume that a given model(Say, OO) is how software should be, while in reality, software is always breaking those abstractions, because the models are not a good representation.
In a sense, this ties into a question of explanatory power and computability: a house blueprint is a fixed representation with a fixed expression, and a fixed input; whereas a software blueprint must account for variable input, possibly even of potentially unbounded length. Software that permits plugins or other ""computing"" tie-ins now has what amounts to a Turing machine operator embedded into it, which gives rise to a host of unpredictability. So the input space of software vis-a-vis a house is mathematically larger, meaning the representational techniques must be correspondingly more computationally powerful. And this is where UML et al. falls down - they are not homomorphic with real software.",https://stackoverflow.com/questions/776889/is-uml-the-correct-language-for-making-software-blueprints,12,0
Two questions regarding Scrum [closed],"Closed. This question is off-topic. It is not currently accepting answers.
                            
                        










Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                        
Closed 2 years ago.



I have two related question regarding Scrum.
Our company is trying to implement it and sure we are jumping over hoops.
Both question are about ""done means Done!""
1) It's really easy to define ""Done"" for tasks which are/have
- clear test acceptance criterias
- completely standalone 
- tested at the end by testers
What should be done with tasks like:
- architecture design
- refactoring
- some utility classes development
The main issue with it, that it's almost completely internal entity 
and there is no way to check/test it from outside.
As example feature implementation is kind of binary - it's done (and
passes all test cases) or it's not done (don't pass some test cases).
The best thing which comes to my head is to ask another developer to review
that task. However, it's any way doesn't provide a clear way to determine
is it completely done or not.
So, the question is how do you define ""Done"" for such internal tasks?
2) Debug/bugfix task
I know that agile methodology doesn't recommend to have big tasks. At least
if task is big, it should be divided on smaller tasks.
Let say we have some quite large problem - some big module redesign (to
replace new outdate architecture with new one). Sure, this task is divided
on dozens of small tasks. However, I know that at the end we will have
quite long session of debug/fix. 
I know that's usually the problem of waterfall model. However, I think
it's hard to get rid of it (especially for quite big changes).
Should I allocate special task for debug/fix/system integrations 
and etc?
In the case, if I do so, usually this task is just huge comparing to
everything else and it's kind of hard to divide it on smaller tasks.
I don't like this way, because of this huge monolith task.
There is another way. I can create smaller tasks (associated with bugs), 
put them in backlog, prioritize and add them to iterations at the end
of activity, when I will know what are the bugs.
I don't like this way, because in such case the whole estimation will became
fake. We estimate the task, mark it ask complete at any time. And we will
open the new tasks for bugs with new estimates. So, we will end up with
actual time = estimate time, which is definitely not good.
How do you solve this problem?
Regards,
Victor","For the first part "" architecture design - refactoring - some utility classes development""  These are never ""done"" because you do them as you go.  In pieces.
You want to do just enough architecture to get the first release going.  Then, for the next release, a little more architecture.
Refactoring is how you find utility classes (you don't set out to create utility classes -- you discover them during refactoring).  
Refactoring is something you do in pieces, as needed, prior to a release.  Or as part of a big piece of functionality.  Or when you have trouble writing a test.  Or when you have trouble getting a test to pass and need to ""debug"".
Small pieces of these things are done over and over again through the life of the project.  They aren't really ""release candidates"" so they're just sprints (or parts of sprints) that gets done in the process of getting to a release.",https://stackoverflow.com/questions/150447/two-questions-regarding-scrum/150473,7,0
Why would you make your product SOA compatible?,"You have a good software product, so why would you make it SOA compatible?","In my opinion, only one good reason: you need interoperability between different platforms or technologies. Otherwise, save yourself a lot of grief and ""just"" make a well-mudlarized architecture - tell your boss it is SOA if that's what he wants to hear. Don't do it because you might move to or use other platforms in the future - you don't have an interoperability problem yet.",https://stackoverflow.com/questions/669837/why-would-you-make-your-product-soa-compatible/669871,5,0
Programming Language Properties that facilitate refactoring?,"What are common traits/properties of programming languages that facilitate (simplify) the development of widely automated source code analysis and re-engineering (transformation) tools?
I am mostly thinking in terms of programming language features that make it easier to develop static analysis and refactoring tools (i.e. compare Java vs. C++, the former of which has better support for refactoring).
In other words, a programming language that would be explicitly designed to provide support for automated static analysis and refactoring right from the beginning, what characteristics would it preferably feature?
For example, for Ada, there's the ASIS:

The Ada Semantic Interface Specification (ASIS) is a layered, open architecture providing vendor-independent access to the Ada Library Environment. It allows for the static analysis of Ada programs and libraries.
  ASIS, the Ada Semantic Interface Specification, is a library that gives applications access to the complete syntactic and semantic structure of an Ada compilation unit. This library is typically used by tools that need to perform some sort of static analysis on an Ada program.
ASIS information: ASIS provides a standard way for tools to extract data that are best collected by an Ada compiler or other source code analyzer. Tools which use ASIS are themselves written in Ada, and can be very easily ported between Ada compilers which support ASIS. Using ASIS, developers can produce powerful code analysis tools with a high degree of portability. They can also save the considerable expense of implementing the algorithms that extract semantic information from the source program. For example, ASIS tools already exist that generate source-code metrics, check a program's conformance to coding styles or restrictions, make cross-references, and globally analyze programs for validation and verification. 

Also see, ASIS FAQ
Can you think of other programming languages that provide a similarly comprehensive and complete interface to working with source code specifically for analysis/transformation purposes?
I am thinking about specific implementation techniques to provide the low level hooks, for example core library functions that provide a way to inspect an AST or ASG at runtime.",The biggest has to be static typing.  This allows tools to have much more insight into what the code is doing.  Without it refactoring becomes many times more difficult.,https://stackoverflow.com/questions/977474/programming-language-properties-that-facilitate-refactoring,7,0
Architectural design for data consistency on distributed analytic system,"I am refactoring an Analytic system that will do a lot of calculation, and I need some ideas on possible architectural designs to a data consistency issue I am facing.
Current Architecture
I have a queue based system, in which different requesting applications create messages that are eventually consumed by workers. 
Each ""Requesting App"" breaks down a large calculation into smaller pieces that will be sent to the queue and processed by the workers.
When all the pieces are finished, the originating ""Requesting app"" will consolidate the results.
Also, the workers consume information from a centralized database (SQL Server) in order to process the requests (Important: the workers do not change any data on the database, only consume it).

Problem
Ok. So far, so good. The problem arises when we include a web service that updates the information on the database. This can happen at any time, but it is critical that each ""large calculation"" originated from the same ""Requesting App"" sees the same data on the database. 
For Example:

App A generates messages A1 and A2, sending it to queue
Worker W1 picks up message A1 for processing.
The web server updates the database, changing from state S0 to S1.
Worker W2 picks up message A2 for processing

I just can麓t have worker W2 using state S1 of the database. for the whole calculation to be consistent it should use the previous S0 state.
Thoughts

A lock pattern to prevent the web server from changing the database while there is a worker consuming information from it. 

cons: The lock might be on for a long time, since the calculation form different ""Request Apps"" might overlap (A1, B1, A2, B2, C1, B3, etc.).

Create new layer between the database and the workers (a server that controls db caching by req. app)

cons: Adding another layer might impose significant overhead (maybe?), and it is a lot of work, since I will have to rewrite the persistence of the workers (a lot of code).


I am pending to the second solution, but not very confident about it.
Any brilliant ideas ? Am I designing it wrong, or missing something ? 
OBS: 

This is a HUGE 2-tier legacy system (in C#) that we are trying to
evolve into a more scalable solution with as minimal effort as
possible.
Each worker is potentially running on different servers.","Can you version your DB ? 
Lets say the requesting application stamps the start of the calculation with ct1. Now every message this calculation generates is stamped with the same timestamp. 
And also each DB update stamps the DB state with the time of the update. So state S0 is on time t0, state S1 on t1 etc.
Now when a worker gets a message it needs to get the DB state where the update time is the largest that is smaller or equal to the message time. In your example, if A1 and A2 are stamped with ct1, and t1 > ct1, both workers will retrieve S0 and not S1.
This means of course that you need to hold several versions in your DB. You can clean those versions after a certain time if you know that your computations must have finished after some time window.",https://stackoverflow.com/questions/24788337/architectural-design-for-data-consistency-on-distributed-analytic-system,3,0
Refactoring google's NetworkBoundResource class to use RxJava instead of LiveData,"Google's android architecture components tutorial here has a part that explains how to abstract the logic of getting data over the network. In it, they create an abstract class called NetworkBoundResource using LiveData to create a reactive stream as the basis for all reactive network requests.
public abstract class NetworkBoundResource<ResultType, RequestType> {
private final AppExecutors appExecutors;

private final MediatorLiveData<Resource<ResultType>> result = new MediatorLiveData<>();

@MainThread
NetworkBoundResource(AppExecutors appExecutors) {
    this.appExecutors = appExecutors;
    result.setValue(Resource.loading(null));
    LiveData<ResultType> dbSource = loadFromDb();
    result.addSource(dbSource, data -> {
        result.removeSource(dbSource);
        if (shouldFetch()) {
            fetchFromNetwork(dbSource);
        } else {
            result.addSource(dbSource, newData -> result.setValue(Resource.success(newData)));
        }
    });
}

private void fetchFromNetwork(final LiveData<ResultType> dbSource) {
    LiveData<ApiResponse<RequestType>> apiResponse = createCall();
    // we re-attach dbSource as a new source, it will dispatch its latest value quickly
    result.addSource(dbSource, newData -> result.setValue(Resource.loading(newData)));
    result.addSource(apiResponse, response -> {
        result.removeSource(apiResponse);
        result.removeSource(dbSource);
        //noinspection ConstantConditions
        if (response.isSuccessful()) {
            appExecutors.diskIO().execute(() -> {
                saveCallResult(processResponse(response));
                appExecutors.mainThread().execute(() ->
                        // we specially request a new live data,
                        // otherwise we will get immediately last cached value,
                        // which may not be updated with latest results received from network.
                        result.addSource(loadFromDb(),
                                newData -> result.setValue(Resource.success(newData)))
                );
            });
        } else {
            onFetchFailed();
            result.addSource(dbSource,
                    newData -> result.setValue(Resource.error(response.errorMessage, newData)));
        }
    });
}

protected void onFetchFailed() {
}

public LiveData<Resource<ResultType>> asLiveData() {
    return result;
}

@WorkerThread
protected RequestType processResponse(ApiResponse<RequestType> response) {
    return response.body;
}

@WorkerThread
protected abstract void saveCallResult(@NonNull RequestType item);

@MainThread
protected abstract boolean shouldFetch();

@NonNull
@MainThread
protected abstract LiveData<ResultType> loadFromDb();

@NonNull
@MainThread
protected abstract LiveData<ApiResponse<RequestType>> createCall();
}

From What I understand, the logic of this class is to:
a) Create a MediatorLiveData called ""result"" as the main return object and set its initial value to Resource.loading(null)
b) Get the data from Android Room db as dbSource LiveData and add it to ""result"" as a source LiveData
c) On dbSource LiveData's first emission, remove the dbSource LiveData from ""result"" and call ""shouldFetchFromNetwork()"" which will

IF TRUE, call ""fetchDataFromNetwork(dbSource)"" which creates a network call through ""createCall()"" that returns a LiveData of the response encapsulated as an ApiResponse object
add back dbSource LiveData to ""result"" and on set emitted values to Resource.loading(data)
add apiResponce LiveData to ""result"" and on first emission remove dbSource and apiResponce LiveDatas
If apiResponse is successful, call ""saveCallResult(processResponse(response))"" and add back dbSource LiveData to ""result"" and set emitted values to Resource.success(newData)
If apiResponse failed, call ""onFetchFailed()"" and add back dbSource LiveData to ""result"" and set emitted values to Resource.error(response.errorMessage, newData))
IF FALSE, just add the dbSource LiveData to ""result"" and set emitted values to Resource.success(newData)

Given that this logic is the correct interpretation, I have tried to refactor this class to use RxJava Observables instead of LiveData. This is my attempt at a successful refactoring (I removed the initial Resource.loading(null) as I see this as superfluous).
public abstract class NetworkBoundResource<ResultType, RequestType> {

private Observable<Resource<ResultType>> result;

@MainThread
NetworkBoundResource() {
    Observable<Resource<ResultType>> source;
    if (shouldFetch()) {
        source = createCall()
                .subscribeOn(Schedulers.io())
                .doOnNext(apiResponse -> saveCallResult(processResponse(apiResponse)))
                .flatMap(apiResponse -> loadFromDb().toObservable().map(Resource::success))
                .doOnError(t -> onFetchFailed())
                .onErrorResumeNext(t -> {
                    return loadFromDb()
                            .toObservable()
                            .map(data -> Resource.error(t.getMessage(), data))

                })
                .observeOn(AndroidSchedulers.mainThread());
    } else {
        source = loadFromDb()
                .toObservable()
                .map(Resource::success);
    }

    result = Observable.concat(
            loadFromDb()
                    .toObservable()
                    .map(Resource::loading)
                    .take(1),
            source
    );
}

public Observable<Resource<ResultType>> asObservable() {return result;}

protected void onFetchFailed() {}

@WorkerThread
protected RequestType processResponse(ApiResponse<RequestType> response) {return response.body;}

@WorkerThread
protected abstract void saveCallResult(@NonNull RequestType item);

@MainThread
protected abstract boolean shouldFetch();

@NonNull
@MainThread
protected abstract Flowable<ResultType> loadFromDb();

@NonNull
@MainThread
protected abstract Observable<ApiResponse<RequestType>> createCall();
}

As I am new to RxJava, my question is am I correctly refactoring to RxJava and maintaining the same logic as the LiveData version of this class?","public abstract class ApiRepositorySource<RawResponse extends BaseResponse, ResultType> {

    // result is a Flowable because Room Database only returns Flowables
    // Retrofit response will also be folded into the stream as a Flowable
    private Flowable<ApiResource<ResultType>> result; 
    private AppDatabase appDatabase;

    @MainThread
    ApiRepositorySource(AppDatabase appDatabase) {
        this.appDatabase = appDatabase;
        Flowable<ApiResource<ResultType>> source;
        if (shouldFetch()) {
            source = createCall()
                .doOnNext(this::saveCallResult)
                .flatMap(apiResponse -> loadFromDb().toObservable().map(ApiResource::success))
                .doOnError(this::onFetchFailed)
                .onErrorResumeNext(t -> {
                    return loadFromDb()
                            .toObservable()
                            .map(data -> {
                                ApiResource apiResource;

                                if (t instanceof HttpException && ((HttpException) t).code() >= 400 && ((HttpException) t).code() < 500) {
                                    apiResource = ApiResource.invalid(t.getMessage(), data);
                                } else {
                                    apiResource = ApiResource.error(t.getMessage(), data);
                                }

                                return apiResource;
                            });
                })
                .toFlowable(BackpressureStrategy.LATEST);
        } else {
            source = loadFromDb()
                    .subscribeOn(Schedulers.io())
                    .map(ApiResource::success);
        }

        result = Flowable.concat(initLoadDb()
                            .map(ApiResource::loading)
                            .take(1),
                            source)
                .subscribeOn(Schedulers.io());
    }

    public Observable<ApiResource<ResultType>> asObservable() {
        return result.toObservable();
    }

    @SuppressWarnings(""WeakerAccess"")
    protected void onFetchFailed(Throwable t) {
        Timber.e(t);
    }

    @WorkerThread
    protected void saveCallResult(@NonNull RawResult resultType) {
        resultType.saveResponseToDb(appDatabase);
    }

    @MainThread
    protected abstract boolean shouldFetch();

    @NonNull
    @MainThread
    protected abstract Flowable<ResultType> loadFromDb();

    @NonNull
    @MainThread
    protected abstract Observable<RawResult> createCall();

    @NonNull
    @MainThread
    protected Flowable<ResultType> initLoadDb() {
        return loadFromDb();
    }
}

So here is what I have decided on using after many iterations. This is currently in production and is working well for my app. Here are some take away notes:

Create a BaseResponse interface
    public interface BaseResponse {
         void saveResponseToDb(AppDatabase appDatabase);
    }

and implement it in all of your api response object classes. Doing this means you don't have to implement save_to_database logic in every ApiResource, you can just default it to what ever the response's implementation is, if you want.
I have chosen to handle Retrofit error responses in the onErrorResumeNext block for simplicity, but I recommend you create a Transformer class that can hold all this logic. In this case, I added an extra Status enum value for ApiResources called INVALID for 400-level responses.
You might be tempted to use the the Reactive Streams architecture component library for LiveData 
implementation ""android.arch.lifecycle:reactivestreams:$lifecycle_version"" and add a method to this class called 
    public LiveData<ApiResource<ResultType>> asLiveData {
         return LiveDataReactiveStreams.fromPublisher(result);
    }

In theory, this would work perfectly as our ViewModels wouldn't have to convert Observable emissions to LiveData emissions or implement lifecycle logic for Observables in Views. Unfortunately, this stream gets rebuilt on every configuration change because it disposes of the LiveData in any onDestroy called (whether isFinishing is true or false). Thus, we have to manage the lifecycle of this stream, which defeats the purpose of using it in the first place, or have duplicated calls every time the device rotates. 

Here is an example of a UserRepository creating an instance of an ApiNetworkResource
@Singleton
public class UserRepository {

    private final RetrofitApi retrofitApi;
    private final AppDatabase appDatabase;

    @Inject
    UserRepository(RetrofitApi retrofitApi, AppDatabase appDatabase) {
        this.retrofitApi = retrofitApi;
        this.appDatabase = appDatabase;
    }

    public Observable<ApiResource<User>> getUser(long userId) {
        return new ApiRepositorySource<UserResponse, User>(appDatabase) {

            @Override
            protected boolean shouldFetch() {
                return true;
            }

            @NonNull
            @Override
            protected Flowable<User> loadFromDb() {
                return appDatabase.userDao().getUserFlowable(userId);
            }

            @NonNull
            @Override
            protected Observable<UserResponse> createCall() {
                return retrofitApi.getUserById(userId);
            }
        }.asObservable();
    }

}",https://stackoverflow.com/questions/49845532/refactoring-googles-networkboundresource-class-to-use-rxjava-instead-of-livedat,1,0
Increasing stack size with WinRM for ScriptMethods,"We are currently refactoring our administration scripts.
It had just appeared that a combination of WinRM, error handling and ScriptMethod dramatically decreases available recursion depth.
See the following example:
Invoke-Command -ComputerName . -ScriptBlock {
    $object = New-Object psobject
    $object | Add-Member ScriptMethod foo {
        param($depth)
        if ($depth -eq 0) {
            throw ""error""
        }
        else {
            $this.foo($depth - 1)
        }
    }

    try {
        $object.foo(5) # Works fine, the error gets caught
    } catch {
        Write-Host $_.Exception
    }

    try {
        $object.foo(6) # Failure due to call stack overflow
    } catch {
        Write-Host $_.Exception
    }
}

Just six nested calls are enough to overflow the call stack!
Indeed, more than 200 local nested calls work fine, and without the try-catch the available depth doubles. Regular functions are also not that limited in recursion.
Note: I used recursion only to reproduce the problem, the real code contains many different functions on different objects in different modules. So trivial optimizations as ""use functions not ScriptMethod"" require architectural changes
Is there a way to increase the available stack size? (I have an administrative account.)","You have two problems that conspire to make this difficult. Neither is most effectively solved by increasing your stack size, if such a thing is possible (I don't know if it is).
First, as you've experienced, remoting adds overhead to calls that reduces the available stack. I don't know why, but it's easily demonstrated that it does. This could be due to the way runspaces are configured, or how the interpreter is invoked, or due to increased bookkeeping -- I don't know the ultimate cause(s). 
Second and far more damningly, your method produces a bunch of nested exceptions, rather than just one. This happens because the script method is, in effect, a script block wrapped in another exception handler that rethrows the exception as a MethodInvocationException. As a result, when you call foo(N), a block of nested exception handlers is set up (paraphrased, it's not actually PowerShell code that does this):
try {
    try {
         ...
         try {
             throw ""error""
         } catch {
             throw [System.Management.Automation.MethodInvocationException]::new(
                 ""Exception calling """"foo"""" with """"1"""" argument(s): """"$($_.Exception.Message)"""""", 
                 $_.Exception
             )
         }
         ...
     } catch {
         throw [System.Management.Automation.MethodInvocationException]::new(
             ""Exception calling """"foo"""" with """"1"""" argument(s): """"$($_.Exception.Message)"""""", 
             $_.Exception
         )
     }
 } catch {
     throw [System.Management.Automation.MethodInvocationException]::new(
         ""Exception calling """"foo"""" with """"1"""" argument(s): """"$($_.Exception.Message)"""""", 
         $_.Exception
     )
 }

This produces a massive stack trace that eventually overflows all reasonable boundaries. When you use remoting, the problem is exacerbated by the fact that even if the script executes and produces this huge exception, it (and any results the function does produce) can't be successfully remoted -- on my machine, using PowerShell 5, I don't get a stack overflow error but a remoting error when I call foo(10).
The solution here is to avoid this particular deadly combination of recursive script methods and exceptions. Assuming you don't want to get rid of either recursion or exceptions, this is most easily done by wrapping a regular function:
$object = New-Object PSObject
$object | Add-Member ScriptMethod foo {
    param($depth)

    function foo($depth) {
        if ($depth -eq 0) {
            throw ""error""
        }
        else {
            foo ($depth - 1)
        }
    }
    foo $depth
}

While this produces much more agreeable exceptions, even this can quite quickly run out of stack when you're remoting. On my machine, this works up to foo(200); beyond that I get a call depth overflow. Locally, the limit is far higher, though PowerShell gets unreasonably slow with large arguments.
As a scripting language, PowerShell wasn't exactly designed to handle recursion efficiently. Should you need more than foo(200), my recommendation is to bite the bullet and rewrite the function so it's not recursive. Classes like Stack<T> can help here:
$object = New-Object PSObject
$object | Add-Member ScriptMethod foo {
    param($depth)

    $stack = New-Object System.Collections.Generic.Stack[int]
    $stack.Push($depth)

    while ($stack.Count -gt 0) {
        $item = $stack.Pop()
        if ($item -eq 0) {
            throw ""error""
        } else {
            $stack.Push($item - 1)
        }
    }
}

Obviously foo is trivially tail recursive and this is overkill, but it illustrates the idea. Iterations could push more than one item on the stack.
This not only eliminates any problems with limited stack depth but is a lot faster as well.",https://stackoverflow.com/questions/41822726/increasing-stack-size-with-winrm-for-scriptmethods,2,0
Running Java remotely using PowerShell,"When I run PowerShell in a remote session (etsn {servername}), I sometimes can't seem to run Java processes, even the most simple:
[chi-queuing]: PS C:\temp> java -cp .\hello.jar Hello
Error occurred during initialization of VM
Could not reserve enough space for object heap

Hello.jar is an ""Hello, world!"" application that should just print ""Hello"" to standard output.
So, the question is, is there something special about running processes on the other side of a PowerShell session? Is there something special about how the Java VM works that might not allow treatment like this? The memory is allocated on the remote computer, right? Here is a readout on the physical memory available:
[chi-queuing]: PS C:\temp> $mem = Get-wmiobject -class Win32_OperatingSystem
[chi-queuing]: PS C:\temp> $mem.FreePhysicalMemory
1013000

But, when I remote desktop to the server and ask the OS how much free memory there is, it says 270_MB physical memory free. Let me know what you think!","According to this:
http://msdn.microsoft.com/en-us/library/aa384372(VS.85).aspx
MaxMemoryPerShellMB
 Specifies the maximum amount of memory allocated per shell, including the shell's child processes. The default is 150 MB.
Increase Max Memory Per Shell MB
winrm set winrm/config/winrs '@{MaxMemoryPerShellMB=""1000""}'",https://stackoverflow.com/questions/4741676/running-java-remotely-using-powershell,3,0
Why does DigitalOcean use uuid for user id and integers for Droplet id?,"I'm building a REST-API and while I was researching for what id type to give certain objects I saw the DigitalOcean API Documentation.
The objects: volume, volume snapshot, certificate, domain, firewall and load balancer are all having a string uuid.
The objects: action, domain record, droplet, droplet snapshot, droplet kernel, droplet backup, droplet neighbor, image and SSH key have a integer id.
But Droplets have an unique integer id.
What are the intentions of using integer ids or string ids in the situations of each object?
The only thing I thought off, DigitalOcean had used string ids in the early years couldn't just switch all string ids to integer ids.
Or
All objects which are short-lived or being created a massively often have a integer id for performance reasons and objects with string ids are like the opposite, long-lived and created less often.
I've made two tables to see better which objects have a string/integer id.","At DigitalOcean, we have standardized on using string uuids going forward. One of the main motivations was that primary keys tie are tied to a specific datastore implementation and can make architecture refactoring more difficult. So the resources using integer IDs are doing so for backwards compatibility and have simply been around longer (i.e. our Droplets were our first product while things like Load Balancers and Firewalls are more recent additions).
Full disclosure: Among other things, I maintain DigitalOcean's API documentation.",https://stackoverflow.com/questions/51216994/why-does-digitalocean-use-uuid-for-user-id-and-integers-for-droplet-id/51217768,1,0
Software Design vs. Software Architecture [closed],"Closed. This question is opinion-based. It is not currently accepting answers.
                            
                        










Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.
                        
Closed 5 years ago.











Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                            
                        




Could someone explain the difference between Software Design and Software Architecture?
More specifically; if you tell someone to present you the 'design' - what would you expect them to present? Same goes for 'architecture'. 
My current understanding is:  

Design: UML diagram/flow chart/simple wireframes (for UI) for a specific module/part of the system
Architecture: component diagram (showing how the different modules of the system communicates with each other and other systems), what language is to be used, patterns...? 

Correct me if I'm wrong. I have referred Wikipedia has articles on http://en.wikipedia.org/wiki/Software_design and http://en.wikipedia.org/wiki/Software_architecture, but I'm not sure if I have understood them correctly.","You're right yes. The architecture of a system is its 'skeleton'. It's the highest level of abstraction of a system. What kind of data storage is present, how do modules interact with each other, what recovery systems are in place. Just like design patterns, there are architectural patterns: MVC, 3-tier layered design, etc. 
Software design is about designing the individual modules / components. What are the responsibilities, functions, of module x? Of class Y? What can it do, and what not? What design patterns can be used?
So in short, Software architecture is more about the design of the entire system, while software design emphasizes on module / component / class level.",https://stackoverflow.com/questions/704855/software-design-vs-software-architecture/1958762,41,0
Ctor Initializer: self initialization causes crash?,"I had a hard time debugging a crash on production. Just wanted to confirm with folks here about the semantics. We have a class like ...
class Test {
public:
  Test()
  {
    // members initialized ...
    m_str = m_str;
  }
  ~Test() {}
private:
  // other members ...
  std::string m_str;
};

Someone changed the initialization to use ctor initialization-lists which is reasonably correct within our code semantics. The order of initialization and their initial value is correct among other things. So the class looks like ...
class Test {
public:
  Test() 
    : /*other inits ,,, */ m_str(m_str)
  {
  }
  ~Test() {}
private:
  // other members ...
  std::string m_str;
};

But the code suddenly started crashing! I isolated the long list of inits to this piece of code m_str(m_str). I confirmed this via link text.
Does it have to crash? What does the standard say about this? (Is it undefined behavior?)","The first constructor is equivalent to 
  Test()
  : m_str()
  {
    // members initialized ...
    m_str = m_str;
  }

that is, by the time you get to the assignment within the constructor, m_str has already been implicitly initialized to an empty string. So the assignment to self, although completely meaningless and superfluous, causes no problems (since std::string::operator=(), as any well written assignment operator should, checks for self assignment and does nothing in this case).
However, in the second constructor, you are trying to initialize m_str with itself in the initializer list - at which point it is not yet initialized. So the result is undefined behaviour.
Update: For primitive types, this is still undefined behaviour (resulting in a field with garbage value), but it does not crash (usually - see the comments below for exceptions) because primitive types by definition have no constructors, destructors and contain no pointers to other objects.
Same is true for any type that does not contain pointer members with ownership semantics. std::string is hereby demonstrated to be not one of these :-)",https://stackoverflow.com/questions/3892098/ctor-initializer-self-initialization-causes-crash/3892209,5,0