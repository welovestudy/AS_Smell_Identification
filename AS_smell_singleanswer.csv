tittle,question,answer,url,ans_count,tag
Is using a root persistent class or base persistable object an architecture smell?,"One of the major gripes voiced by the Alt.Net community against the Microsoft Entity Framework is that it forces you to use a Base Persistable Object for everything being stored in the database. I have two questions related to this:

Is it acceptable to have a ""Root Persistent Class"" as the base for the domain objects in your application, or is this an architecture smell? 
If you feel it is OK for you to have one within your application, is it also OK for an ORM framework to force you to use one? Are there reasons to avoid a framework that makes you do this?

I've been using an abstract base object as the root of all my peristable classes for some time. It makes several housekeeping chores much easier.","My feeling is that it's ok presuming the context is such that it

stays out of the way
doesn't add features that aren't used outside of the scope of the entity
doesn't tie you to any particular ORM (sort of in keeping with #2)

So if the base class is used to describe, for instance, an ID and the meaning of Equality (since many times entities are considered equal if they have the same ID), then that's fine.  However, when it starts describing database-centric information (such as tables, columns, state of the entity, etc), then yes, I think it begins to smell.",https://stackoverflow.com/questions/2232492/is-using-a-root-persistent-class-or-base-persistable-object-an-architecture-smel,3,1
Metrics for measuring successful refactoring [closed],"Are there objective metrics for measuring code refactoring? 
Would running findbugs, CRAP or checkstyle before and after a refactoring be a useful way of checking whether the code was actually improved rather than just changed?
I'm looking for trends we can capture that can help us improve the code review process without wasting time on code that gets changed for simple personal preference.","Would running findbugs, CRAP or checkstyle before and after a refactoring be a useful way of checking if the code was actually improved rather than just changed?

Actually, as I have detailed in the question ""What is the fascination with code metrics?"", the trend of any metrics (findbugs, CRAP, whatever) is the true added value of metrics.
It (the evolution of metrics) allows you to prioritize the main fixing action you really need to make to your code (as opposed to blindly try to respect every metric out there)
A tool like Sonar can, in this domain (monitoring of metrics) can be very useful.

Sal adds in the comments:

The real issue is on checking what code changes add value rather than just adding change

For that, test coverage is very important, because only tests (unit tests, but also larger ""functional tests"") will give you a valid answer.
But refactoring should not be done without a clear objective anyway. To do it only because it would be ""more elegant"" or even ""easier to maintain"" may be not in itself a good reason enough to change the code.
There should be other measures like some bugs which will be fixed in the process, or some new functions which will be implemented much faster as a result of the ""refactored"" code.
In short, the added value of a refactoring is not solely measured with metrics, but should also be evaluated against objectives and/or milestones.",https://stackoverflow.com/questions/798243/metrics-for-measuring-successful-refactoring/36119147,9,1
Should HTTP Based Micro Services always be Rest,"I'm currently developing a micro service that basically provides calculation services to other micro services. It does not store data or have any resources like a sales order. It only calls other micro services and then calculates metrics and prices to return a result.
I'm kind of struggling  trying to make a rest API with resources names that are nouns when all I do is calculate stuff and return results (more like an action).
So can we have a micro services that behaves more like an HTTP API than a Restful service (is it a bad practice, an anti pattern , an architecture smell, ....)
Regards","You can use whatever you want and in your particular case I am pretty sure you won't see any drawbacks. From my point of view only difference with rest is mostly semantic -some people may also argue about cacheability but I don't think so- 
Apart from rest/rpc creating microservices without any actual domain could cause a maintenance issue in the long run as you totally depend on some other microservices data whenever a change required in their side you may also need change this microservice. That is why I don't recommend those kind of calculation services unless you have a valid scalability requirement.",https://stackoverflow.com/questions/49260669/should-http-based-micro-services-always-be-rest,1,1
JAX-RS Webservice Architecture Patterns,"What are the common practices for developing JAX-RS Webservices?
I think my architecture has kind of a smell to it:
The webservice is acting as a proxy, collecting information from different sources. There are RSS Feeds, SOAP Services and a Database.
I would like to abstract away the datasource in my business logic. Thats why I came up with something like this:
Persistence Layer: 
| RSS Connector   Parser         SOAP Interface(s)             Entities        |
|     SomeRssDataDAO       SomeSoapDao   AnotherSoapDap    UserDao  ...Dao     |

Service Layer
|    SomeRssDataService            SoapDataService        UserFavoritesService |

""Resource"" Layer
|       JerseyResources that map HTTP to service methods                       |

The service layer as well as the persistence layer would be EJBs. 
The problem I'm facing is that I would have transactions in the persistence layer.
What if a services needs to use mutliple steps to do it's work, that wouldn't be right then.
But using transactions / the entitymanager in my service layer doesn't seem right to.
What's the way to go? 
Also general Enterprise Application Architecture Tipps would be appreciated.","I don't see anything wrong with transactions being managed by the service layer. That's where the context to decide what should be atomic is and should be. 
Your service layer doesn't need to be EJBs unless you plan to distribute them. You can use JDO or JPA transactions to manage persistence operations.
Your ""Persistence Layer"" doesn't really need to be EJBs either unless you plan to distribute them.",https://stackoverflow.com/questions/17031387/jax-rs-webservice-architecture-patterns,1,1
Stop session from expiring when browser closes,"It seems to be that my PHP session is expiring/deleting when the browser is closed. 
Is there a way I can prevent this to happen ? Is it only a PHP setting or is there a browser say in this as well ?","You need:

Change session.cookie_lifetime value
Change session.gc_maxlifetime value so that it should not be lesser than cookie life time value. Otherwise your session cookie will point to invalid session storage

However, if you want your session to persist after browser was closed, it seems that it's not ""session"" by definition. I.e. technically, you can do that - but logically that seems to be architecture smell.",https://stackoverflow.com/questions/19807161/stop-session-from-expiring-when-browser-closes/19807209,2,1
Is it possible to cancel function override in parent class and use function from top level parent,"class TopParent
{
    protected function foo()
    {
        $this->bar();
    }

    private function bar()
    {
       echo 'Bar';
    }
}

class MidParent extends TopParent
{
    protected function foo()
    {
        $this->midMethod();
        parent::foo();
    }

    public function midMethod()
    {
        echo 'Mid';
    }

    public function generalMethod()
    {
       echo 'General';
    }
}

Now the question is if I have a class, that extends MidParent because I need to call
class Target extends MidParent
{
    //How to override this method to return TopParent::foo(); ?
    protected function foo()
    {
    }
}

So I need to do this:
$mid = new MidParent();
$mid->foo(); // MidBar
$taget = new Target();
$target->generalMethod(); // General
$target->foo(); // Bar

UPDATE
Top parent is ActiveRecord class, mid is my model object. I want to use model in yii ConsoleApplication. I use 'user' module in this model, and console app doesn't support this module. So I need to override method afterFind, where user module is called. So the Target class is the class that overrides some methods from model which uses some modules that console application doesn't support.","Directly - you can't. This is how OOP works.
You can do it by a little redesign, e.g. in MidParent add method:
protected function parentFoo()
{
    parent::foo();
}

and in Target:
public function foo()
{
    $this->parentFoo();
}

But, again, this is only a workaround to solve your question and not a solution.",https://stackoverflow.com/questions/19808308/is-it-possible-to-cancel-function-override-in-parent-class-and-use-function-from/19808535,4,1
Update enum using Reflection in java,"I have declared an enum
enum Animal {
    Cat(""Tom""), Rat(""Jerry"");
}

I have to add test cases on the values. Is there a way to add another Animal Type on the go using reflection in enum, for which I have to fail the test case.
P.S. These values have some resemblance with another module runs parallel on server, so in future if some other coder updates enum, to ensure he has to make those changes in another module as well.
So, I am adding a functional test case around it.","All the values in an enum should be known upfront. In this case, you can also write a test case which 'gets' all the values from this enum and asserts that the size is 2 , values are as expected etc. On a separate note, having duplicate enums in two modules which should remain in sync seems like a design/architecture smell. May be both modules should pick up these values from a shared db or a shared cache to properly mitigate this.",https://stackoverflow.com/questions/48089494/update-enum-using-reflection-in-java/48089779,1,1
How do I use streaming with WCF services returning a large data set?,"I'm fairly new to WCF and I'm trying to figure out how streaming works.I basically have a web service that is supposed to return some information from a db.The data returned is potentially very large.I was wondering if anyone could point out a good example of how is this done.If I'm understanding things correctly, my web service method should be returning a stream object.But how do I actually pass data to the stream? Is it just a matter of serializing each object and writing it to the stream? If so, how exactly would the serializaition be done? Once again, a simple example would be very much appreciated.","You should just set the Binding's TransferMode property to the Streamed value and work with your WCF services as you would do.  The WCF infrastructure will do all the job automatically.
Here is one more useful link:
Streaming Message Transfer",https://stackoverflow.com/questions/6737534/how-do-i-use-streaming-with-wcf-services-returning-a-large-data-set/6751349,2,1
"Custom roles architecture in ASP.NET MVC 5, Dependency Injection issues","I've got an architecture issue that I'm hoping someone can be of assistance to guide me in a more ideal strategy. The way I've been forced to do this reeks of ""code smell"".
I've got two different kinds of ""Roles"". I've got the built in Identity Roles, and I've got a custom set of roles (User Group Roles). I store these User Group Roles in a database, essentially a relationship between a user id, a usergroup role id, and a usergroup id. I'm using Ninject for dependency injection of my UserGroupService that handles all the CRUD operations of assigning users with certain usergroup roles to usergroups.
My first plan of attack was to create a custom authorization attribute that I could place on actions, similar to the Identity [Authorize(Role="""")] attribute. I did not have any luck with this because I cannot inject a service into an attribute class (needs a parameterless constructor). 
After that didn't work, my second plan of attack was to write an extension method for IPrincipal, essentially mimicking User.IsInRole("""") with User.IsInUserGroupRole(""""). This didn't work because I cannot inject a service into a static class.
Currently I am stuck including some booleans in the model of every view that has role based logic involved. So for instance:
public ActionResult Navigation()
    {
        var isSystemAdmin = User.IsInRole(""Administrator"");
        var isUserGroupAdmin = _userGroupService.IsUserGroupAdmin(User.Identity.GetUserId()) && !isSystemAdmin;
        var isGeneralUser = !isSystemAdmin && !isUserGroupAdmin;

        var model = new NavigationViewModel
        {
            IsSystemAdmin = isSystemAdmin,
            IsUserGroupAdmin = isUserGroupAdmin,
            IsGeneralUser = isGeneralUser
        };

        return PartialView(""_Navigation"", model);
    }

The issue here is that I have to do this  any time I want to determine what kind of roles the user is currently in. It works, but it smells.
Am I missing something here? I think the most ideal option would be the extension method strategy of being able to call it right off of User, but cannot seem to make that work.","Constructor DI is not the only way to get access to a dependency.
Each IOC has a way of resolving a dependency, all you need is a reference to the IOC container. So, even if your attribute requires a parameterless constructor you could still resolve the dependency manually. 
Something like this should help :
http://www.c-sharpcorner.com/UploadFile/47fc0a/resolving-dependency-using-ninject/
Is it a great way to use your IOC this way? Probably not but it sure beats what you're doing now.",https://stackoverflow.com/questions/39337958/custom-roles-architecture-in-asp-net-mvc-5-dependency-injection-issues,1,1
@Component annotation to be used with qualifier,"Let's say I have a single class with 
@Component annotation 
@Component
public class MyClass {}

Now I wanna use it with two different scopes: prototype and singleton without defining it in configuration. 
Is that possible?","There should never be a use case for having the same bean in two different scopes. It's a huge architectural smell and you should probably redesign your application.
That said, perhaps you mean you need to inject a prototype-scoped bean into a singleton bean? That's a common use case and the implications are adressed in these manual sections:

Singleton beans with prototype-bean dependencies
Scoped beans as dependencies

But if you do insist to have the same bean in two different scopes, that can be achieved by using @Bean methods:
@Configuration
public class MyConfig{
    @Bean public MyClass singletonScope(){ return new MyClass(); }
    @Bean @Scope(""prototype"") public MyClass prototypeScope(){ return new MyClass(); }
}",https://stackoverflow.com/questions/37608022/component-annotation-to-be-used-with-qualifier/37608621,1,1
Message Bus and Message Based Architecture With Winforms/Desktop Application and Strategies/Policies for View/UI Logic,"Our company has been building out a product for year that is using a message-based architecture where modules register with the message bus and listen for messages that are dispatched to the bus and handled in their handler(s). 
This is very similar if not exactly the same to what is provided from a product like NServiceBus, but we have some edge cases and needs that are not covered by products we have found, so we rolled our own. it has worked out very nicely thus far.
As we have had to implement some more complicated business logic recently, we have been noticing the code is starting to 'smell'. We are starting to see some of the following happen:

Logic is making its way into some of the view(code behinds)
We have a hard time with items in the view that need immediate feedback from a message that has been dispatched to the message queue.

An example of where this came up was when we needed to do a moderately complex wizard that conditionally determines the flow of the user through the wizard pages.
In order to try to keep the logic outside of the view, we dispatch messages and keep reference to the view originally in the message handler for that module, but this breaks down sometimes.
Is there anything out there that shows best practices or a common pattern for actual realistic examples of message bus systems, or do I have to make my own and just tolerate some of the more complex edge cases.
There doesn't seem to be much 'official' documentation on these types of system, just high level architecture overviews.
I imagine asking poeple who have done it before have discovered or stumbled upon a system that makes this easier to handle.","There are a couple of things you can look at.
One should never query over a service bus.  Your 'immediate feedback' is not really feasible.  You can implement a priority queue for scenarios where you need to deal with low latency requirements.  All queries should be handled by a synchronous query layer.
Having a UI respond to service bus messages should be fine but typically a UI's inbox should be transient and one should be able to purge it at system start up.
Your wizard scenario sounds quite a bit like an RPC call.  This may not be the best idea.  Your wizard path definition probably needs a re-look.  A service bus is not really designed to be an interactive affair :)",https://stackoverflow.com/questions/20059948/message-bus-and-message-based-architecture-with-winforms-desktop-application-and,2,1
Circular Dependency Solution,"Our current project has ran into a circular dependency issue. Our business logic assembly is using classes and static methods from our SharedLibrary assembly. The SharedLibrary contains a whole bunch of helper functions, such as a SQL Reader class, Enumerators, Global Variables, Error Handling, Logging and Validation.
The SharedLibrary needs access to the Business objects, but the Business objects need access to SharedLibrary.  The old developers solved this obvious code smell by replicating the functionality of the business objects in the shared library (very anti-DRY). I've spent a day now trying to read about my options to solve this but i'm hitting a dead end.
I'm open to the idea of architecture redesign, but only as a last resort. So how can i have a Shared Helper Library which can access the business objects, with the business objects still accessing the Shared Helper Library?","You could create a separate project only for value objects (no logic) and interfaces.
Have your shared library classes implement the interfaces, and the Business library depend on the interfaces (do I hear more testable and decoupled code here? Not to mention you remove the dependency from the Shared Library). 
Ideally, you could have the business objects on which your shared library depend on this extra project too. If the business objects are too complex, you could also transform them into interfaces.
You will have both projects not depending on each other, but only on another project with only ""dummy"" objects (no logic):
Business ---> Interfaces and value objects <--- Shared Library
Now those are decoupled =)",https://stackoverflow.com/questions/2596461/circular-dependency-solution,3,1
What would be a nice architecture so I can pass information of eventual problems to a higher layer?,"I have a standard 3 layer project and my data layer either accesses the database or an API. I would like to be able to show information to the user in case there are problems occurring in the data layer.
I would like to show in the interface if there was any error (some required info by the API was not set, problems connecting to the database/API or any other problem that could happen).
Currently I would have an ""out string error"" parameter that would return the problem message. But this smells bad and this will also require my business layer to have the same paramter, which smells even worse.
What would be a nice architecture so I can pass information of eventual problems to a higher layer? 
Thanks!","There could be two ways:

Raising some custom exception
Raising event at database layer and handle it in business layer.

Now while raising the event/exception at data layer you can pass the information which will help you in business layer.
The only change at business layer will be the listening of these events or catching these exceptions.",https://stackoverflow.com/questions/20122199/what-would-be-a-nice-architecture-so-i-can-pass-information-of-eventual-problems,2,1
Domain Validation in a CQRS architecture,"Danger ... Danger Dr. Smith... Philosophical post ahead
The purpose of this post is to determine if placing the validation logic outside of my domain entities (aggregate root actually) is actually granting me more flexibility or it's kamikaze code
Basically I want to know if there is a better way to validate my domain entities. This is how I am planning to do it but I would like your opinion
The first approach I considered was:
class Customer : EntityBase<Customer>
{
   public void ChangeEmail(string email)
   {
      if(string.IsNullOrWhitespace(email))   throw new DomainException(_..._);
      if(!email.IsEmail())  throw new DomainException();
      if(email.Contains(__mailinator.com_))  throw new DomainException();
   }
}

I actually do not like this validation because even when I am encapsulating the validation logic in the correct entity, this is violating the Open/Close principle (Open for extension but Close for modification) and I have found that violating this principle, code maintenance becomes a real pain when the application grows up in complexity. Why? Because domain rules change more often than we would like to admit, and if the rules are hidden and embedded in an entity like this, they are hard to test, hard to read, hard to maintain but the real reason why I do not like this approach is: if the validation rules change, I have to come and edit my domain entity. This has been a really simple example but in RL the validation could be more complex
So following the philosophy of Udi Dahan, making roles explicit, and the recommendation from Eric Evans in the blue book, the next try was to implement the specification pattern, something like this
class EmailDomainIsAllowedSpecification : IDomainSpecification<Customer>
{
   private INotAllowedEmailDomainsResolver invalidEmailDomainsResolver;
   public bool IsSatisfiedBy(Customer customer)
   {
      return !this.invalidEmailDomainsResolver.GetInvalidEmailDomains().Contains(customer.Email);
   }
}

But then I realize that in order to follow this approach I had to mutate my entities first in order to pass the value being valdiated, in this case the email, but mutating them would cause  my domain events being fired which I wouldn__ like to happen until the new email is valid
So after considering these approaches, I came out with this one, since I am going to implement a CQRS architecture:
class EmailDomainIsAllowedValidator : IDomainInvariantValidator<Customer, ChangeEmailCommand>
{
   public void IsValid(Customer entity, ChangeEmailCommand command)
   {
      if(!command.Email.HasValidDomain())  throw new DomainException(_..._);
   }
}

Well that__ the main idea, the entity is passed to the validator in case we need some value from the entity to perform the validation, the command contains the data coming from the user and since the validators are considered injectable objects they could have external dependencies injected if the validation requires it.
Now the dilemma, I am happy with a design like this because my validation is encapsulated in individual objects which brings many advantages: easy unit test, easy to maintain, domain invariants are explicitly expressed using the Ubiquitous Language, easy to extend, validation logic is centralized and validators can be used together to enforce complex domain rules. And even when I know  I am placing the validation of my entities outside of them (You could argue a code smell - Anemic Domain) but I think the trade-off is acceptable
But there is one thing that I have not figured out how to implement it in a clean way. How should I use this components...
Since they will be injected, they won__ fit naturally inside my domain entities, so basically I see two options:

Pass the validators to each method of my entity
Validate my objects externally (from the command handler)

I am not happy with the option 1 so I would explain how I would do it with the option 2
class ChangeEmailCommandHandler : ICommandHandler<ChangeEmailCommand>
{
   // here I would get the validators required for this command injected
   private IEnumerable<IDomainInvariantValidator> validators;
   public void Execute(ChangeEmailCommand command)
   {
      using (var t = this.unitOfWork.BeginTransaction())
      {
         var customer = this.unitOfWork.Get<Customer>(command.CustomerId);
         // here I would validate them, something like this
         this.validators.ForEach(x =. x.IsValid(customer, command));
         // here I know the command is valid
         // the call to ChangeEmail will fire domain events as needed
         customer.ChangeEmail(command.Email);
         t.Commit();
      }
   }
}

Well this is it. Can you give me your thoughts about this or share your experiences with Domain entities validation
EDIT
I think it is not clear from my question, but the real problem is: Hiding the domain rules has serious implications in the future maintainability of the application, and also domain rules change often during the life-cycle of the app. Hence implementing them with this in mind would let us extend them easily. Now imagine in the future a rules engine is implemented, if the rules are encapsulated outside of the domain entities, this change would be easier to implement
I am aware that placing the validation outside of my entities breaks the encapsulation as @jgauffin mentioned in his answer, but I think that the benefits of placing the validation in individual objects is much more substantial than just keeping the encapsulation of an entity. Now I think the encapsulation makes more sense in a traditional n-tier architecture because the entities were used in several places of the domain layer, but in a CQRS architecture, when a command arrives, there will be  a command handler accessing an aggregate root and performing operations against the aggregate root only creating a perfect window to place the validation.
I'd like to make a small comparison between the advantages to place validation inside an entity vs placing it in individual objects

Validation in Individual objects

Pro. Easy to write
Pro. Easy to test
Pro. It's explicitly expressed
Pro. It becomes part of the Domain design, expressed with the current Ubiquitous Language
Pro. Since it's now part of the design, it can be modeled using UML diagrams
Pro. Extremely easy to maintain
Pro. Makes my entities and the validation logic loosely coupled
Pro. Easy to extend
Pro. Following the SRP
Pro. Following the Open/Close principle
Pro. Not breaking the law of Demeter (mmm)?
Pro. I'is centralized
Pro. It could be reusable
Pro. If required, external dependencies can be easily injected
Pro. If using a plug-in model, new validators can be added just by dropping the new assemblies without the need to re-compile the whole application
Pro. Implementing a rules engine would be easier
Con. Breaking encapsulation
Con. If encapsulation is mandatory, we would have to pass the individual validators to the entity  (aggregate) method

Validation encapsulated inside the entity

Pro. Encapsulated?
Pro. Reusable?


I would love to read your thoughts about this","I agree with a number of the concepts presented in other responses, but I put them together in my code.
First, I agree that using Value Objects for values that include behavior is a great way to encapsulate common business rules and an e-mail address is a perfect candidate. However, I tend to limit this to rules that are constant and will not change frequently. I'm sure you are looking for a more general approach and e-mail is just an example, so I won't focus on that one use-case.
The key to my approach is recognizing that validation serves different purposes at different locations in an application.  Put simply, validate only what is required to ensure that the current operation can execute without unexpected/unintended results.  That leads to the question what validation should occur where?
In your example, I would ask myself if the domain entity really cares that the e-mail address conforms to some pattern and other rules or do we simply care that 'email' cannot be null or blank when ChangeEmail is called?  If the latter, than a simple check to ensure a value is present is all that is needed in the ChangeEmail method.
In CQRS, all changes that modify the state of the application occur as commands with the implementation in command handlers (as you've shown). I will typically place any 'hooks' into business rules, etc. that validate that the operation MAY be performed in the command handler. I actually follow your approach of injecting validators into the command handler which allows me to extend/replace the rule set without making changes to the handler. These 'dynamic' rules allow me to define the business rules, such as what constitutes a valid e-mail address, before I change the state of the entity - further ensuring it does not go into an invalid state. But 'invalidity' in this case is defined by the business logic and, as you pointed out, is highly volitile.
Having come up through the CSLA ranks, I found this change difficult to adopt because it does seem to break encapsulation. But, I agrue that encapsulation is not broken if you take a step back and ask what role validation truly serves in the model.
I've found these nuances to be very important in keeping my head clear on this subject.  There is validation to prevent bad data (eg missing arguments, null values, empty strings, etc) that belongs in the method itself and there is validation to ensure the business rules are enforced.  In the case of the former, if the Customer must have an e-mail address, then the only rule I need to be concerned about to prevent my domain object from becoming invalid is to ensure that an e-mail address has been provided to the ChangeEmail method.  The other rules are higher level concerns regarding the validity of the value itself and really have no affect on the validity of the domain entity itself.
This has been the source of a lot of 'discussions' with fellow developers but when most take a broader view and investigate the role validation really serves, they tend to see the light.
Finally, there is also a place for UI validation (and by UI I mean whatever serves as the interface to the application be it a screen, service endpoint or whatever). I find it perfectly reasonably to duplicate some of the logic in the UI to provide better interactivity for the user.  But it is because this validation serves that single purpose why I allow such duplication.  However, using injected validator/specification objects promotes reuse in this way without the negative implications of having these rules defined in multiple locations.
Not sure if that helps or not...",https://stackoverflow.com/questions/10879421/domain-validation-in-a-cqrs-architecture,11,1
.NET MVC Controller with multiple Repositories and Services?,"Look at my Controller (I'm using Dependency Injection to manager dependencies):
public RoleController(IRoleRepository roleRepository, ISiteRepository siteRepository, IUserRepository userRepository, IDbContext dbContext)
{
    _roleRepository = roleRepository;
    _siteRepository = siteRepository;
    _userRepository = userRepository;
    _dbContext = dbContext;
}

Having a class with many dependencies is a code smell? Right?
But, In my example I need to associate Users and Sites in a Role, then I need to these dependencies to doing this association.
Some people on a mailing list I was told that having too many dependencies is a sign that something is probably wrong. But I see no other way. I separated my responsibilities, there is something in that situation I do not know how to treat? Is something wrong?
Update:
I need Repositories and DbContext because DbContext is my UnitOfWork, repositories don't save.
This example is a simple CRUD with some other functionalities like Associations in the View with a GRID.
Update 2:
I'm using a architecture where my UI Layer is the MVC.","I don't believe it's a bad thing, given that you manage dependencies with a good DI framework (i.e. not by using poor man's DI). This way, you explicitly say that the controller will need all these things, because it will. (Note that in many other parts of your application, this might not be a valid argument - the controller is special in the way that it is where you control and direct the program flow, so there's a natural explanation why it needs to see lots of parts of the application...)
However, if you really want to limit the number of dependencies in this specific case, it could make sense to create a MembershipService, which does all the work concerned with Users, Sites and Roles. That would then have a dependency on those three repositories, and your controller would only have a dependency on the membership service.

In response to your update: You could register the unit of work (i.e. the db context) as a ""per web request"" singleton - this is possible with Castle Windsor and many other DI frameworks. Then you can let your repositories depend on it and do all the changes, and let the controller depend on it for saving, and they will all get the same instance handed to them by the DI framework.",https://stackoverflow.com/questions/7353491/net-mvc-controller-with-multiple-repositories-and-services,1,1
How can I properly break a big controller action to smaller pieces in asp.net mvc?,"I am currently working with a team that use mongoDb for the project, it has one super big entity called Project, this project entity contain other classes, and all of this should be inserted in one action as one project, for example:
public class Project
{
    public GeneralInfo BuildinGeneralInfo { get; set; }

    public List<ResidentalGeneralInfo> ResidentalGeneralInfo { get; set;     }

    public List<CommercialOffice> OfficeGeneralInfo { get; set; }

    public List<CommercialOffice> BusinessGeneralInfo { get; set; }
//etc...
}

and in GeneralInfo for example we have:
public class GeneralInfo
{

    public string Elevation { get; set; }

    public string ElevationEn { get; set; }

    public string Floors { get; set; }

    public DateTime BuildDate { get; set; }

    public string LandArea { get; set; } 
//etc...
}

what I did was create a method as follow:
private GeneralInfo InsertGeneralInfo(string Elevation, string ElevationEn, string Floors, DateTime BuildDate, string LandArea, string ResidentalUnit, string BusinessUnit, string OfficeUnit, string ArchitectureStyle, string ArchitectureStyleEn, string StructureType, string StructureTypeEn, string ResidentalUnitInFloor, string BusinessUnitInFloor, string OfficeUnitInFloor, string Yard, string ParkingCapasity, string HeatCool, string HeatCoolEn, string Foundation)
    {
        var generalInfoObj = new GeneralInfo
        {
            Elevation = Elevation,
            ElevationEn = ElevationEn,
            Floors = Floors,
            BuildDate = BuildDate,
            LandArea = LandArea,
            ResidentalUnit = ResidentalUnit,
            BusinessUnit = BusinessUnit,
            OfficeUnit = OfficeUnit,
            ArchitectureStyle = ArchitectureStyle,
            ArchitectureStyleEn = ArchitectureStyleEn,
            StructureType = StructureType,
            StructureTypeEn = StructureTypeEn,
            ResidentalUnitInFloor = ResidentalUnitInFloor,
            BusinessUnitInFloor = BusinessUnitInFloor,
            OfficeUnitInFloor = OfficeUnitInFloor,
            Yard = Yard,
            ParkingCapasity = ParkingCapasity,
            HeatCool = HeatCool,
            HeatCoolEn = HeatCoolEn,
            Foundation = Foundation
        };

        return generalInfoObj;
    }

and then used it in my action like this:
public ActionResult Create(GeneralInfo generalInfoModel)
    {
        var project = new Project();


        project.BuildinGeneralInfo = InsertGeneralInfo(generalInfoModel.Elevation, generalInfoModel.ElevationEn,
            generalInfoModel.Floors, generalInfoModel.BuildDate, generalInfoModel.LandArea,
            generalInfoModel.ResidentalUnit, generalInfoModel.BusinessUnit, generalInfoModel.OfficeUnit,
            generalInfoModel.ArchitectureStyle, generalInfoModel.ArchitectureStyleEn, generalInfoModel.StructureType,
            generalInfoModel.StructureTypeEn, generalInfoModel.ResidentalUnitInFloor, generalInfoModel.BusinessUnitInFloor,
            generalInfoModel.OfficeUnitInFloor, generalInfoModel.Yard, generalInfoModel.ParkingCapasity,
            generalInfoModel.HeatCool, generalInfoModel.HeatCoolEn, generalInfoModel.Foundation);

        return View();
    }

but I feel this might be a naive implementation and there might be far better approach then this for breaking a big controller action, I'd appreciate any help in this regard, and on top of that I think my team shouldn't be using this big entity like that, but I have no control over that, I'd also appreciate any comment as to whether using entity like this is a code smell and bad architecture design.","If I understand correctly you get object of type GeneralInfo as input parameter and then use another method that creates new object of the same type which is exactly the same as input one, and then you assign it to Project object. It would be the same as if you would just assign the input object to Project object property.
project.BuildinGeneralInfo = generalInfoModel;

Maybe I missed something, but it looks like you over complicated everything.",https://stackoverflow.com/questions/32045270/how-can-i-properly-break-a-big-controller-action-to-smaller-pieces-in-asp-net-mv,1,1
How to reduce number of injected dependencies on controller,"I am using MVC3, Entity Framework v4.3 Code First, and SimpleInjector. I have several simple classes that look like this:
public class SomeThing
{
    public int Id { get; set; }
    public string Name { get; set; }
}

I have another entity that looks like this:
public class MainClass
{
    public int Id { get; set; }
    public string Name { get; set; }
    public virtual AThing AThingy { get; set; }
    public virtual BThing BThingy { get; set; }
    public virtual CThing CThingy { get; set; }
    public virtual DThing DThingy { get; set; }
    public virtual EThing EThingy { get; set; }
}

Each Thingy (currently) has its own Manager class, like so:
public class SomeThingManager
{
    private readonly IMyRepository<SomeThing> MyRepository;

    public SomeThingManager(IMyRepository<SomeThing> myRepository)
    {
        MyRepository = myRepository;
    }
} 

My MainController consequently follows:
public class MainController
{
    private readonly IMainManager MainManager;
    private readonly IAThingManager AThingManager;
    private readonly IBThingManager BThingManager;
    private readonly ICThingManager CThingManager;
    private readonly IDThingManager DThingManager;
    private readonly IEThingManager EThingManager;

    public MainController(IMainManager mainManager, IAThingManager aThingManager, IBThingManager bThingManager, ICThingManager cThingManager, IDThingManager dThingManager, IEThingManager eThingManager)
    {
        MainManager = mainManager;
        AThingManager = aThingManager;
        BThingManager = bThingManager;
        CThingManager = cThingManager;
        DThingManager = dThingManager;
        EThingManager = eThingManager;
    }

    ...various ActionMethods...
}

In reality, there are twice as many injected dependencies in this controller. It smells. The smell is worse when you also know that there is an OtherController with all or most of the same dependencies. I want to refactor it.
I already know enough about DI to know that property injection and service locator are not good ideas. 
I can not split my MainController, because it is a single screen that requires all these things be displayed and editable with the click of a single Save button. In other words, a single post action method saves everything (though I'm open to changing that if it makes sense, as long as it's still a single Save button). This screen is built with Knockoutjs and saves with Ajax posts if that makes a difference.
I humored the use of an Ambient Context, but I'm not positive it's the right way to go.
I humored the use of injecting a Facade as well.
I'm also wondering if I should implement a Command architecture at this point.
(Don't all of the above just move the smell somewhere else?)
Lastly, and perhaps independent of the three above approaches, is should I instead have a single, say, LookupManager with explicit methods like GetAThings(), GetAThing(id), GetBThings(), GetBThing(id), and so on? (But then that LookupManager would need several repositories injected into it, or a new type of repository.)
My musings aside, my question is, to reiterate: what's a good way to refactor this code to reduce the crazy number of injected dependencies?","Using a command architecture is a good idea, since this moves all business logic out of the controller, and allows you to add cross-cutting concerns without changes to the code. However, this will not fix your problem of constructor over-injection. The standard solution is to move related dependencies into a aggregate service. However, I do agree with Mark that you should take a look at the unit of work pattern.",https://stackoverflow.com/questions/12046690/how-to-reduce-number-of-injected-dependencies-on-controller,3,1
How can I measure the quality of my software architecture? [closed],"I have a relatively complex software I have made, and now I want to evaluate how ""good"" my architecture is. Is it modular? Is it simple? Could it be better? Where are the biggest flaws?
The only metrics I know are Coupling and Cohesion. But I have nothing to compare them to (they are relative metrics right? Or are there any standards stating something like Coupling less than X is good, etc...).
Thanks!
For the ones interested:
Coupling - Measures the degree to which each program module relies on each one of the other modules. The greater the coupling is, more difficult is to modify or exchange some module without affecting other modules.
Cohesion - defines how well defined is some module__ responsibilities. If a module has an high cohesion, it means that this module has a well defined task, otherwise, the module does several tasks that don__ have much in common, which usually leads to system disorganization.
There are several ways to calculate these metrics, but I know no standard way.",You can use code smells detection to evaluate your code. You can find further discussion at this question on programmers.stackexchange regarding code smells for architecture.,https://stackoverflow.com/questions/26934714/how-can-i-measure-the-quality-of-my-software-architecture/26947051,1,1
"Rhino Mocks, Dependency Injection, and Separation of Concerns","I am new to mocking and dependency injection and need some guidance.
My application is using a typical N-Tier architecture where the BLL references the DAL, and the UI references the BLL but not the DAL. Pretty straight forward. 
Lets say, for example, I have the following classes:
class MyDataAccess : IMyDataAccess {}
class MyBusinessLogic {}

Each exists in a separate assembly.
I want to mock MyDataAccess in the tests for MyBusinessLogic. So I added a constructor to the MyBusinessLogic class to take an IMyDataAccess parameter for the dependency injection. But now when I try to create an instance of MyBusinessLogic on the UI layer it requires a reference to the DAL. 
I thought I could define a default constructor on MyBusinessLogic to set a default IMyDataAccess implementation, but not only does this seem like a codesmell it didn't actually solve the problem. I'd still have a public constructor with IMyDataAccess in the signature. So the UI layer still requires a reference to the DAL in order to compile.
One possible solution I am toying with is to create an internal constructor for MyBusinessLogic with the IMyDataAccess parameter. Then I can use an Accessor from the test project to call the constructor. But there's still that smell. 
What is the common solution here. I must just be doing something wrong. How could I improve the architecture?","You can define your classes like this:
public class MainForm : Form
{
    private readonly businessLogic;

    public MainForm(IBusinessLogic businessLogic)
    {
        this.businessLogic = businessLogic;
    }
}

public class BusinessLogic : IBusinessLogic
{
    private IDataLayer dataLayer;

    public BusinessLogic(IDataLayer dataLayer)
    {
        this.dataLayer = dataLayer;
    }
}

public class DataLayer : IDataLayer
{
    public DataLayer(string connectionString)
    {
    }
}

Note how the main form doesn't know about the DAL here. Now we do need a piece of code which knows all the classes, so that they can be wired together. This is typically done at the start of the application:
public static void Main(string[] args)
{
   var dataLayer = new DataLayer(""foo"");
   var businessLogic = new BusinessLogic(dataLayer);
   var mainForm = new MainForm(businessLogic);

   Application.Run(mainForm);
}

Of course, this is a simplified example. If you have dozens or hundreds of classes in practice, then such start-up wiring can get very big and complicated, especially when cyclic dependencies come into play. That's why dependency injection frameworks were created to replace that code by XML configuration files, configuration by code, or .NET attributes. The basic idea is the same though.
Examples of dependency injection frameworks for .NET: AutoFac, Castle, Spring.NET, StructureMap, Ninject and the Managed Extensibility Framework.",https://stackoverflow.com/questions/2473942/rhino-mocks-dependency-injection-and-separation-of-concerns,3,1
Using ViewModels instead DTOs as the result of a CQRS query,"Reading a SO question, I realized that my Read services could provide some smarter object like ViewModels instead  plain DTOs. This makes me reconsider what information should be provided by the objects returned by the Read Services
Before, using just DTOs, my Read Service just made flat view mapping of a database query into hash like structure with minimum normalization and no behavior.
However I tend to think of a ViewModel as something ""smarter"" that can have generated information not provided by the database, like status icon, calculated values, reformatted values, default values, etc.
I am starting to see that the construction of some ViewModel objects might get more complicated and has potential downsides if I made my generic ReadServiceInterface return ViewModels only:

(1) Should I plan some design restriction for the ViewModels returned by my CQRS? Like making sure that their construction is almost as fast as a plain DTO?
(2) DTOs by nature are easily serialized and ready to be sent to an external system in a SOA architecture or embedded into a message. Does this mean that using ViewModels will have a negative impact on my architecture?
(3) Which type of ViewModels should I keep outside my Read Services?
(4) Should I expect all ViewModels to be retrieved from Read Services?

In the past I implemented some ViewModels that needed more than one query. In a CQRS I suppose, that is a design smell, since everything they provide, should be in only one query.
I am starting a new project, where I thought that any query will return either aggregate objects or DTOs. Since now ViewModels come into play. I am wondering:

(5) Should I plan that queries within my architecture will yield two type of objects (ViewModels+Aggregates) or three (+DTO)?","View Models (VM) serve a single master: the View. We're usually consider the VM a pretty dumb object so in this regard, there's no technical difference between a VM and a DTO, only their purpose and semantics are different.
How you build a VM is an implementation detail. Some VM are pre generated and stored in a VM repository.  Others are built in real-time by a service (or a query handler) either by querying the db directly or querying other repos/services then assembling the results. There's no right or wrong and no rules about how to do it. It comes down to preference.
In CQRS the important part is separation of commands from queries i.e more than one model. There's no rule about how many queries you should do or if you should return a view model or dto. As long as you have at least one read model dedicated for queries, it's CQRS.
Don't let technicalities complicate your design. Proper design is more about high level structure and not low level implementation. Use CQRS because having a read model simplifies your app, not for other reasons. Aim for simplification and clean code, not for rigid rules that dictate a 'how to' recipe.",https://stackoverflow.com/questions/21408226/using-viewmodels-instead-dtos-as-the-result-of-a-cqrs-query,1,1
Storing C# GetHashCode() in DB is Unreliable [duplicate],"Possible Duplicate:
How do I create a HashCode in .net (c#) for a string that is safe to store in a database? 

I'm planning to store hundreds of thousands of URLs in my database.  Each row in my UrlInfo table will be immutable with the URL itself being the logical primary key. Since URL's can be fairly lengthy, I've decided to hash the URL as a quick way to find possible matches when adding new rows.  The hash isn't my true key, just a way to find possible matches quickly. In addition I'm using a RegEx pattern per domain that distills the essence of the URL into something that can be compared against other URL's.  I'm storing the results of the RegEx also as a hash and I'm not concerned if it yields possible duplicates.
Everything has been going great until I learned that C#'s string.GetHashCode() method, which I've been using to hash things, is not guaranteed to be unique across implementations of .Net. I noticed this when I tried moving my hash function from ASP.Net to SQLServer CLR code.  The web app uses .Net 4.0 and SQLServer 2008 R2, I've learned, uses .Net 3.5. They yielded separate hash results for the same string so now I need to get away from using string.GetHashCode() because I don't want to have to worry about this changing when I upgrade my app to to future versions of .Net.
So, questions:

Does my architecture smell since I'm storing a hash in my db?  Any better ways?  Obviously Microsoft doesn't want me to store hash results!
Can someone recommend a good C# replacement algorithm for hashing strings?  I saw Jon's here but not exactly sure how to modify to work for strings (loop through each char using ascii codes?).
Are there any string compression algorithms that would be better than using a hash algorithm?

Thanks
AWESOME RESPONSES BY MANY.  THANK YOU VERY MUCH!!!","You could always use an MD5 hash instead, which is relatively fast:
public string GetUrlHash(string url) {

    byte[] hash = MD5.Create().ComputeHash(Encoding.UTF8.GetBytes(url));

    StringBuilder sb = new StringBuilder();
    for (int i = 0; i < hash.Length; i++) {
        sb.Append(hash[i].ToString(""X2""));
    }

    return sb.ToString();
}

Call it like so:
Console.WriteLine(this.GetUrlHash(""http://stackoverflow.com/questions/5355003/storing-c-gethashcode-in-db-is-unreliable""));

And get:
> 777BED7F83C66DAC111977067B4B4385

This should be fairly reliable from an uniqueness standpoint. MD5 is insecure nowadays for password applications but you don't have that problem here.
The only problem is using a string like this as a primary key on a table might be problematic, performance-wise.
The other thing you could do is use the URL shortener approach: use your database's sequence generation feature, and convert the value (make sure you use the equivalent of LONG or BIGINT!) to something like Base36, which gives you a nice, concise string.",https://stackoverflow.com/questions/5355003/storing-c-sharp-gethashcode-in-db-is-unreliable,5,1
Proper structuring of Lucene.Net usage in an ASP.NET MVC site,"I'm building an ASP.NET MVC site where I plan to use Lucene.Net. I've envisioned a way to structure the usage of Lucene, but not sure whether my planned architecture is OK and efficient.

My Plan:

On Application_Start event in Global.asax: I check for the existence of the index on the file system - if it doesn't exist, I create it and fill it with documents extracted it from the database.
When new content is submitted: I create an IndexWriter, fill up a document, write to the index, and finally dispose of the IndexWriter. IndexWriters are not reused, as I can't imagine a good way to do that in an ASP.NET MVC application.
When content is edited: I repeat the same process as when new content is submitted, except that I first delete the old content and then add the edits.
When a user searches for content: I check HttpRuntime.Cache to see if a user has already searched for this term in the last 5 minutes - if they have, I return those results; otherwise, I create an IndexReader, build and run a query, put the results in HttpRuntime.Cache, return them to the user, and finally dispose of the IndexReader. Once again, IndexReaders aren't reused.


My Questions:

Is that a good structure - how can I improve it?
Are there any performance/efficiency problems I should be aware of?
Also, is not reusing the IndexReaders and IndexWriters a huge code smell?","The answer to all three of your questions is the same: reuse your readers (and possibly your writers). You can use a singleton pattern to do this (i.e. declare your reader/writer as public static). Lucene's FAQ tells you the same thing: share your readers, because the first query is reaaalllyyyy slow. Lucene handles all the locking for you, so there is really no reason why you shouldn't have a shared reader.
It's probably easiest to just keep your writer around and (using the NRT model) get the readers from that. If it's rare that you are writing to the index, or if you don't have a huge need for speed, then it's probably OK to open your writer each time instead. That is what I do.
Edit: added a code sample:
public static IndexWriter writer = new IndexWriter(myDir);

public JsonResult SearchForStuff(string query)
{
    IndexReader reader = writer.GetReader();
    IndexSearcher search = new IndexSearcher(reader);
    // do the search
}",https://stackoverflow.com/questions/3480974/proper-structuring-of-lucene-net-usage-in-an-asp-net-mvc-site,2,1
SharpArchitecture - FluentNHibernate Schema Generation?,"I'm trying out SharpArchitecture and want to have FluentNHibernate generate my database schema for my MVC WebSite. 
I'm a bit lost on where to do this. I can do it by adding the SchemaUpdate thingy in the global.asax.cs-file right after NHibernateInitializer.Instance().InitializeNHibernateOnce(InitializeNHibernateSession); in ""Application_beginrequest"". (If I place it before that call, SharpArch throws an exception).
This doesn't seems right and it smells bad. It feels like I'm missing something basic in the Sharp Architecture that allows for automatic schema generation to my DB (MSSQL2005). Or am I not? If not, please fill me in on best practices for schema generation with fluent nhibernate and Sharp Architecture.
Thanks in advance!
Edit: I might add that I'm looking on the Northwind sample project in SharpArch, but want to make FNHb generate the schema instead.","You don't want to do it in Application_BeginRequest.
To auto-gen the DDL, what you should do is do it in your TDD classes.  Create a special class that you can manually call when you need to generate your DDL for your development database.  
Something like:
private static void CreateDatabaseFromFluentNHibernateMappings()
{
    var mappingAssemblies = RepositoryTestsHelper.GetMappingAssemblies();
    SchemaExport schema = new SchemaExport(NHibernateSession.Init(new SimpleSessionStorage(), mappingAssemblies, NHIBERNATE_CFG_XML));
    schema.Execute(true, true, false);
}

This will generate and execute the DDL based on your mappings to the database you specify in your NHibernate config file (in the NHIBERNATE_CFG_XML).  The database, albeit empty, should already exist.
You can also create another method in your class that can update the schema of the development database as you develop in case you have added new entities, properties, etc.
private static void UpdateExistingDatabaseFromFluentNHibernateMappings()
{
    var mappingAssemblies = RepositoryTestsHelper.GetMappingAssemblies();
    SchemaUpdate schema = new SchemaUpdate(NHibernateSession.Init(new SimpleSessionStorage(), mappingAssemblies, NHIBERNATE_CFG_XML));
    schema.Execute(true, true);
}

This will update an existing database with the changes you have made in FNH without destroying the existing database. Very useful, especially when you might have test data already in the database.
And finally, You can use NDbUnit to preload a database based on test data defined in XML in your project and under SCM.  Great when you have a team working on the same database and you want to preload it with data, thus everyone starts with the same blank slate.
Using NDbUnit:
private static void LoadTheTestDataintoDb()
{
    const string connectionstring = // your connection string to your db
    NDbUnit.Core.INDbUnitTest sqlDb = new NDbUnit.Core.SqlClient.SqlDbUnitTest(connectionstring);
    sqlDb.ReadXmlSchema(/* your XML schema file defining your database (XSD) */);
    sqlDb.ReadXml(/* Your XML file that has your test data in it (XML) */);
    // Delete all from existing db and then load test data allowing for identity inserts
    sqlDb.PerformDbOperation(NDbUnit.Core.DbOperationFlag.CleanInsertIdentity);
}

This requires you to use NDbUnit.  Thanks to Stephen Bohlen for it!  
I hope this helps; I wrote this kinda quickly, so if I confused you, let me know.",https://stackoverflow.com/questions/2024265/sharparchitecture-fluentnhibernate-schema-generation,1,1
"How do I wrap Func<dynamic, MyClass> property","This is simplified setup - I have API (I have n o control on the API), which exposes a Func property like this:
public Func<dynamic, MyClass> FuncProperty { get; set; }

Usually it's used like this:
api.FuncProperty = s =>
   {
      do1();
      do2();
      return new MyClass(); //simplified
   }

Similar code is used all over the place (of course the content in {} is different), and I want to add common functionality to all these, I'd like to create a ""wrapper"" method, which I can use like this:
api.FuncProperty = MyWrapperMethod( 
   s =>
   {
      do1();
      do2();
      return new MyClass();
   });

I know I can edit all these calls to look like:
api.FuncProperty = s =>
  {
     DoMyCommonFunctionality();
     //... as before
  }

But if my common functionality is something like:
using(var disposable = SetSomeState())
{
   //the stuff which previously was in the lambda
}

then, using the latter approach is kind of ugly.
That's why even if it's only for learning purposes, how should my wrapper's method signature look like? And how should I use it?","If I understand you right, it should also return a Func<dynamic, MyClass>, like this:
public static Func<dynamic, MyClass> MyWrapperMethod(Func<dynamic, MyClass> func)
{
    // Validation if you want
    return d =>
    {
        using(var disposable = SetSomeState())
        {
            return func(d);
        }
    };
}

That's an example with the using statment you wanted.
Note that calling MyWrapperMethod does not call the delegate you pass into it. Instead, it returns a delegate which, when called, will call the delegate you passed. This sort of deferred execution can be confusing, but I believe it's what you want here.",https://stackoverflow.com/questions/7167632/how-do-i-wrap-funcdynamic-myclass-property/7167670,3,1
TypeScript How to access object defined in other class,"I have a project that uses Aurelia framework. I want to make global\static object that should be accessed across couple files. But when I try to access it from a different file it says that my object is undefined. Here is what it looks like:
FirstFile.ts
export function showA() {
    console.log(""Changed a to "" + a);
}
export var a = 3;

export class FirstFile {
    public ModifyA() {
        a = 7;
        showA();
    }

It says that a  = 7. Then I use it in other file like this.
SecondFile.ts
import FirstFile = require(""src/FirstFile"");
export class SecondFile {
    showA_again() {
        FirstFile.showA();
}

I execute showA_again() in my view file called SecondFile.html
<button click.trigger=""showA_again()"" class=""au-target"">Button</button>

When I click button, I see in console that variable ""a"" is still 3.
Is there any way to store variables between files?","I'd recommend you to inject FirstFile into SecondFile. Now your code has a smell of bad architecture.
To answer your question: probably you are looking for static (playground sample)
export class FirstFile {

    static showA = function() {
        console.log(""Changed a to "" + FirstFile.a);
    }

    static a = 3;

    public ModifyA() {
        FirstFile.a = 7;
        FirstFile.showA();
    }
}

export class SecondFile {
    showA_again() {
        FirstFile.showA();
    }
}",https://stackoverflow.com/questions/31720825/typescript-how-to-access-object-defined-in-other-class/31724489,1,1
Low latency communication between machines using a shared file,"We would like to (continuously) write to a log file on one linux machine, and continuously read from it on another machine.  We're looking for low latency updates (this is for real-time plotting) and easy setup.  What the best way to achieve this?  
I have tinkered with nfs, sshfs, and tail -f, but updates only come in at 1Hz.  I assume these filesystems don't support the select syscall, and so top polls at it's internal rate.","This smells like a bad architecture choice.
Is it for RPC? If so, why don't you just create a webservice?
If it's just for transfering data, try memcached.",https://stackoverflow.com/questions/12041371/low-latency-communication-between-machines-using-a-shared-file/12041478,1,1
SOLID - are the Single Responsibility Principle and the Open/Closed Principle mutually exclusive?,"The Single Responsibility Principle states that:

A class should have one, and only one, reason to change.

The Open/Closed Principle states that:

You should be able to extend a classes behavior, without modifying it.

How can a developer respect both principles if a class should have only one reason to change, but should not be modified?
Example
The factory pattern is a good example here of something that has a single responsibility, but could violate the open/closed principle:
public abstract class Product
{
}

public class FooProduct : Product
{
}

public class BarProduct : Product
{
}

public class ProductFactory
{
    public Product GetProduct(string type)
    {
        switch(type)
        {
            case ""foo"":
                return new FooProduct();
            case ""bar"":
                return new BarProduct();
            default:
                throw new ArgumentException(...);
        }
    }
}

What happens when I need to add ZenProduct to the factory at a later stage?

Surely this violates the open/closed principle?
How can we prevent this violation?","This feels like a discussion of the semantics of 'extend a classes behaviour'. Adding the new type to the factory is modifying existing behaviour, it's not extending behaviour, because we haven't changed the one thing the factory does. We may need to extend the factory but we have not extended it's behaviour. Extending behaviour means introducing new behaviour and would be more along the lines of an event each time an instance of a type is created or authorising the caller of the factory - both these examples extend (introduce new) behaviour.

A class should have one, and only one, reason to change.

The example in the question is a factory for creating Product instances and the only valid reason for it to change is to change something about the Product instances it creates, such as adding a new ZenProduct.

You should be able to extend a classes behavior, without modifying it.

A really simple way to achieve this is through the use of a Decorator

The decorator pattern is often useful for adhering to the Single Responsibility Principle, as it allows functionality to be divided between classes with unique areas of concern.

public interface IProductFactory
{
    Product GetProduct(string type);
}

public class ProductFactory : IProductFactory
{
    public Product GetProduct(string type)
    {
        \\ find and return the type
    }
}

public class ProductFactoryAuth : IProductFactory
{
    IProductFactory decorated;
    public ProductFactoryAuth(IProductFactory decorated)
    {
        this.decorated = decorated;
    }

    public Product GetProduct(string type)
    {
        \\ authenticate the caller
        return this.decorated.GetProduct(type);
    }
}

The decorator pattern is a powerful pattern when applying the SOLID principles. In the above example we've added authentication to the ProductFactory without changing the ProductFactory.",https://stackoverflow.com/questions/49197359/solid-are-the-single-responsibility-principle-and-the-open-closed-principle-mu/49199005,4,1
"MySQL - Count all rows in name specific tables, which are older than 1 day","This is the statement I am using to get the count of all rows from tables where name of the table includes word 'devices'.
SELECT SUM(TABLE_ROWS) FROM INFORMATION_SCHEMA.TABLES 
  WHERE TABLE_NAME like '%devices%'

And above works wonderfully. However I need to get the count of only those rows which had changes during the last 24 hours. 
This should be easy to do, because each '%devices%' table has a column named dateofinstall where I store the unix timestamp (epoch).
So naturally I wanted to use the following statement: 
SELECT SUM(TABLE_ROWS) FROM INFORMATION_SCHEMA.TABLES 
   WHERE TABLE_NAME like '%devices%' 
     and dateofinstall >= unix_timestamp(CURRENT_TIMESTAMP - INTERVAL 1 DAY)

However this doesn't seem to work!
Any other way of getting the count of all rows in name specific tables, which are older than 1 day?
--
FOUND A CORRECT ANSWER (thanks to user: Alma Do):
This builds a new query, which upon execution returns count within 24 hours.
SELECT 
  CONCAT('SELECT SUM(rowscount) FROM (', 
         GROUP_CONCAT(sqlcount SEPARATOR ' UNION ALL '), 
         ') as initunion') 
FROM 
  (SELECT 
    CONCAT('SELECT COUNT(1) AS rowscount FROM `',
    TABLE_SCHEMA,
    '`.`',
    TABLE_NAME,
    '` WHERE dateofinstall>= unix_timestamp(CURRENT_TIMESTAMP - INTERVAL 1 DAY)') as sqlcount 
   FROM 
     INFORMATION_SCHEMA.TABLES 
   WHERE 
     TABLE_NAME like '%devices%') as initcount;","You can not do that directly. That is because - even if TABLE_ROWS holds data that relied to actual table data, it can not be used with conditions in any case and INFORMATION_SCHEMA itself does not contains any table data, only metadata.
However, there's a way to do this with building SQL query by another SQL query. It will be like:
SELECT 
  CONCAT('SELECT SUM(rowscount) FROM (', 
         GROUP_CONCAT(sqlcount SEPARATOR ' UNION ALL '), 
         ') as initunion') 
FROM 
  (SELECT 
    CONCAT('SELECT COUNT(1) AS rowscount FROM `',
    TABLE_SCHEMA,
    '`.`',
    TABLE_NAME,
    '` WHERE dateofinstall<NOW()-INTERVAL 24 HOUR') as sqlcount 
   FROM 
     INFORMATION_SCHEMA.TABLES 
   WHERE 
     TABLE_NAME like '%devices%') as initcount;

-the resulting string will be valid SQL and you will be able to execute that via prepared statements. For example:
SET group_concat_max_len = 32000;
SET @sql = (SELECT 
  CONCAT('SELECT SUM(rowscount) FROM (', 
         GROUP_CONCAT(sqlcount SEPARATOR ' UNION ALL '), 
         ') as initunion') 
FROM 
  (SELECT 
    CONCAT('SELECT COUNT(1) AS rowscount FROM `',
    TABLE_SCHEMA,
    '`.`',
    TABLE_NAME,
    '` WHERE dateofinstall<NOW()-INTERVAL 24 HOUR') as sqlcount 
   FROM 
     INFORMATION_SCHEMA.TABLES 
   WHERE 
     TABLE_NAME like '%devices%') as initcount);
PREPARE stmt FROM @sql;
EXECUTE stmt;

-note, that GROUP_CONCAT() has restriction for length of returned string, so you may want to adjust group_concat_max_len for your session.
Also, please, note, that in common case building one query by another query is an architecture smell (because of unpredictable SQL length, at least), but in your case this is acceptable since you will not have too many tables, obviously.",https://stackoverflow.com/questions/20146479/mysql-count-all-rows-in-name-specific-tables-which-are-older-than-1-day/20147063,1,1
writing a closure in coffeescript with using __o_,"I am trying to set the property of my Backbone view class using a HTML select in Coffeescript and what I've realised is that I need to inject this(@) into the callback/handler via a javascript closure to maintain scope. But I can't seem to figure out how to do this as using ""do"" just executes the function on page load
dropdown = $('<select />') .on 'change', do(myView = @) -> myVieW.prop = @.value

thanks for your help!","The do notation does compile to an IIFE, yous still need the actual handler function:
dropdown = $('<select />') .on 'change', do(myView = @) -> (e) -> myVieW.prop = @.value

# or wrap the whole handler assignment:
do(myView = @) ->
  dropdown = $('<select />') .on 'change', (e) -> myVieW.prop = @.value

Or you just use lexical this:
dropdown = $('<select />') .on 'change', (e) => @.prop = e.target.value

dropdown = $('<select />') .on 'change', (e) => @.prop = dropdown.val()",https://stackoverflow.com/questions/23294877/writing-a-closure-in-coffeescript-with-using-do/23295391,2,1
Is MEF a Service locator?,"I'm trying to design the architecture for a new LOB MVVM project utilising Caliburn Micro and nHibernate and am now at the point of looking into DI and IOC. 
A lot of the examples for bootstrapping Caliburn Micro use MEF as the DI\IOC mechanism. 
What I'm struggling with is that MEF seems to by reasonably popular but the idea of the Mef [Imports] annotations smells to me like another flavour of a Service Locator?
Am I missing something about MEF whereby almost all the examples I've seen are not using it correctly or have I completely not understood something about how it's used whereby it side steps the whole service locator issues?","I suppose if you were to just new up parts that had property imports and try to use them, then you could run into some of the same problems described here: Service Locator is an Anti-Pattern
But in practice you get your parts from the container, and if you use [Import] without the additional allowDefault property, then the part is required and the container will blow up on you if you ask for the part doing the import.  It will blow up at runtime, yes, but unlike a run of the mill service-locator, its fairly straightforward to do static analysis of your MEF container using a test framework.  I've written about it a couple times here and here.
It hasn't been a problem for me in practice, for a couple of reasons:

I get my parts from the container.
I use composition tests.
I'm writing applications, not framework code.",https://stackoverflow.com/questions/11148913/is-mef-a-service-locator,2,1
RX Subjects - are they to be avoided?,"I've had a mini-discussion on the topic in another thread, and would like to have people's input on the ""bad"" sides of subjects.
People who frequent the RX forum know that E.Meijer does not like Subjects. While I have a deepest respect to RX creator's opinion, I have been using Subjects quite extensively in multiple projects for a couple of years and haven't had any architectural problem or a bug because of them.
The only ""pitfall"" with Subjects I can name is that they are not ""reusable"" - after you have completed an observable on a Subject, you need to re-instantiate it, before new subscribers can receive events from it. 
""Code smell"" and ""Don't like them"" need to be supported by ""pragmatic"" examples - can you bring to our attention possible situations when using a Subject can lead to a bug or a problem? Or maybe you think they are easy and harmless altogether - then try to define an area where they are to be used.","Erik Meijer is thinking in a purely functional way - Subjects are the mutable variables of Rx. So, in general usage he's right - using Subjects is sometimes a way to cop out of Thinking Functionally, and if you use them too much, you're trying to row upstream. 
However! Subject are extremely useful when you're interfacing with the non-Functional world of .NET. Wrapping an event or callback method? Subjects are great for that. Trying to put an Rx ""interface"" onto some existing code? Use a Subject!",https://stackoverflow.com/questions/9299813/rx-subjects-are-they-to-be-avoided,4,1
Is the singleton pattern still bad when we talk about it in a framework context?,"Using the singleton pattern is usually a code smell.
However, it seems that using ""singletons"" in a framework such as Laravel is a popular solution to a lot of problems.
It's also worth noting that singletons in Laravel aren't really the same thing as a singleton class in the following sense:

The class being used as a singleton is unaware of this, there are no
static methods in there, nor is there anything that would prevent you
from creating a new instance of that class manually.
When using Laravel singletons, the container keeps track of the
objects you want to use as singleton and hands these to you upon
request.

Am I correct in my understanding that what is called a singleton in Laravel isn't in fact a singleton, but an implementation inspired by the singleton pattern, and therefore isn't as much as a bad practice as a real singleton because it gets rid of some of the downsides (harder to test, global state...)?
This question is loosely related to this question about design patterns in PHP but it appeared appropriate to me to create a separate question.","Yes, it is still bad. It is still an anti-pattern. 
The fact, that you see many people resorting to this, simply indicate the flaws in any given system. It applies to applications, frameworks and libraries. 
And saying that ""our singletons are different"" is just wrong. And, if you were writing proper unit tests, you would be testing your code in isolation, without having to initialize various parts of a framework.

P.S.
  You seem to have wrong opinion of what ""code smell"" is. Code smell is when you have variables named $a1, $bzzzz, code smell is when your functions have 8 parameters. Anit-pattern is not a code smell. It is a mistake in architecture.",https://stackoverflow.com/questions/39225088/is-the-singleton-pattern-still-bad-when-we-talk-about-it-in-a-framework-context/39278425,1,1
Can I get a PHP object to run code if a method is not found?,"I have a master class, DBAPI which contains all the interaction with the database. It's not singleton per se, but is designed to only be instantiated once as $DBAPI.
When I alter the database, I obviously have to add functions to DBAPI to let the site use the new functionality, however, since there are a lot of different actions that can be taken, instead of including everything in a single massive class file, I've split them out by functionality/permission level as traits, and the DBAPI class file is dynamically created by adding traits flagged based off of permission level (read only, read-write etc.). Since the only time the file needs to be created is when new traits are added, I only create the class file if it doesn't exist for that specific user permission level, otherwise I use the already generated file. (If there's a better way to do this I'm all ears).
The issue I'm running into now is that if I add some functions in a new trait, the previously generated classes are obviously not aware of it, and I don't find out about that until I try to use the function in the code somewhere and it fails. It's pointless to write wrappers around every single function call made to check if it is a function first- is there some way to get the DBAPI class to do some action if code attempts to access a method it can't find? 
for example, if code calls some function $DBAPI->newfunction() $DBAPI handles the exception itself, running some code that will attempt to update itself, which will cause newfunction() to run if it can be found.","(N. B. This architecture has a really bad code smell.  I'm sure there's a better way to do this.)
PHP classes can implement the __call magic method that is used when there is no matching method name.
function __call( $name, $arguments ) {
    // Code to run...
}",https://stackoverflow.com/questions/23147010/can-i-get-a-php-object-to-run-code-if-a-method-is-not-found/23147051,1,1
Extend AudioContext in TypeScript,"I would like to extend (in OOP terms) the built-in AudioContext class in TypeScript:
class LiveAudioContext extends AudioContext {
    constructor() {
        super();
    }

    public setPlaybackRate(rate: number) {
        console.log(`Set rate to ${rate}.`);
    }
}

This is valid TypeScript, accepted by both the compiler and IntelliSense. At runtime however, I get a TypeError, for which Chrome adds the following error message as well:
var liveCtx = new LiveAudioContext();


Failed to construct 'AudioContext': Please use the 'new' operator, this DOM object constructor cannot be called as a function.

Fair enough, I know how it would be possible to solve the issue in the compiled Javascript output. However, having to manually fix an error caused by the compiler after each build is just unsuitable for production use.
How could I accomplish this extension correctly, so that it is functional in the compiled output as well, and off-the-shelf?

Edit: of course, creating a wrapper class that effectively redefines every single method and property works, but I find it to be smelling -- more like, stinking, and badly -- from an architectural point of view:
class LiveAudioContext {
    private internalContext: AudioContext;

    public createBuffer() {
        return this.internalContext.createBuffer.apply(this.internalContext, arguments);
    }

    public createBufferSource() {
        return this.internalContext.createBufferSource.apply(this.internalContext, arguments);
    }

    ...
}","I believe that the following question provides a good solution: How to handle warnings for proprietary/custom properties of built-in objects in TypeScript
Essentially you extend the existing interface, and apply your new methods directly on the browser's objects.",https://stackoverflow.com/questions/36766688/extend-audiocontext-in-typescript,1,1
KISS & design patterns [closed],"I'm presented with a need to rewrite an old legacy desktop application. It is a smallish non-Java desktop program that still supports the daily tasks of several internal user communities.  
The language in which the application is both antiquated and no longer supported. I'm a junior developer, and I need to rewrite it. In order to avoid the app rewrite sinkhole, I'm planning on starting out using the existing database & data structures (there are some significant limitations, but as painful as refactoring will be, this approach will get the initial work done more quickly, and avoid a migration, both of which are key to success).
My challenge is that I'm very conflicted about the concept of Keep It Simple. I understand that it is talking about functionality, not design. But as I look to writing this app, it seems like a tremendous amount of time could be spend chasing down design patterns (I'm really struggling with dependency injection in particular) when sticking with good (but non-""Group of Four"") design could get the job done dramatically faster and simpler.
This app will grow and live for a long time, but it will never become a 4 million line enterprise behemoth, and its objects aren't going to be used by another app (yes, I know, but what if....YAGNI!). 
The question
Does KISS ever apply to architecture & design? Can the ""refactor it later"" rule be extended so far as to say, for example, ""we'll come back around to dependency injection later"" or is the project dooming itself if it doesn't bake in all the so-called critical framework support right away?
I want to do it ""right""....but it also has to get done. If I make it too complex to finish, it'll be a failure regardless of design.","I'd say KISS certainly applies to architecture and design.
Over-architecture is a common problem in my experience, and there's a code smell that relates:
Contrived complexity

forced usage of overly complicated design patterns where simpler
  design would suffice.

If the use of a more advanced design pattern, paradigm, or architecture isn't appropriate for the scale of your project, don't force it.
You have to weigh the potential costs for the architecture against the potential savings... but also consider what the additional time savings will be for implementing it sooner rather than later.",https://stackoverflow.com/questions/13480484/kiss-design-patterns/13480743,2,1
How to make a property protected AND internal in C#?,"Here is my shortened abstract class:
abstract class Report {

    protected internal abstract string[] Headers { get; protected set; }
}

Here is a derived class:
class OnlineStatusReport : Report {

    static string[] headers = new string[] {
        ""Time"",
        ""Message""
    }

    protected internal override string[] Headers {
        get { return headers; }
        protected set { headers = value; }
    }

    internal OnlineStatusReport() {
        Headers = headers;
    }
}

The idea is, I want to be able to call Report.Headers from anywhere in the assembly, but only allow it to be set by derived classes. I tried making Headers just internal, but protected does not count as more restrictive than internal.  Is there a way to make Headers internal and its set accessor protected AND internal?
I feel like I'm grossly misusing access modifiers, so any design help would be greatly appreciate.","What's wrong with making the getter public?  If you declare the property as
public string[] Headers { get; protected set; }

it meets all of the criteria you want:  all members of the assembly can get the property, and only derived classes can set it.  Sure, classes outside the assembly can get the property too.  So?
If you genuinely need to expose the property within your assembly but not publicly, another way to do it is to create a different property:
protected string[] Headers { get; set; }
internal string[] I_Headers { get { return Headers; } }

Sure, it's ugly decorating the name with that I_ prefix.  But it's kind of a weird design.  Doing some kind of name mangling on the internal property is a way of reminding yourself (or other developers) that the property they're using is unorthodox.  Also, if you later decide that mixing accessibility like this is not really the right solution to your problem, you'll know which properties to fix.",https://stackoverflow.com/questions/941104/how-to-make-a-property-protected-and-internal-in-c/941147,7,1
"OSGi, Java Modularity and Jigsaw","So as of yesterday morning I hadn't a clue as to what OSGi even was. OSGi was just some buzzword that I kept seeing cropping up over and over again, and so I finally set aside some time to brush up on it.
It actually seems like pretty cool stuff, so I'd like to start off by stating (for the record) that I'm not anti-OSGi in any respect, nor is this is some ""OSGi-bashing"" question.
At the end of the day, it seems that OSGi has - essentially - addressed JSR 277 on Java Modularity, which recognized that there are shortcomings with the JAR file specification that can lead to namespace resolution and classloading issues in certain corner cases. OSGi also does a lot of other really cool stuff, but from what I can ascertain, that's its biggest draw (or one of them).
To me - as a fairly new (a few years now) Java EE developer, it is absolutely mind-boggling that we are in the year 2011 and currently living in the era of Java 7, and that these classloading issues are still present; particularly in enterprise environments where one app server could have hundreds of JARs on it, with many of them depending on different versions of one another and all running (more or less) concurrently.
My question:
As interested as I am in OSGi, and as much as I want to start learning about it to see where/if it could be of use to my projects, I just don't have the time to sit down and learn something that large, at least now.
So what are non-OSGi developers to do when these problems arise? What Java (Oracle/Sun/JCP) solutions currently exist, if any? Why was Jigsaw cut from J7? How sure is the community that Jigsaw will get implemented next year in J8? Is it possible to get Jigsaw for your project even though its not a part of the Java platform yet?
I guess what I'm asking here is a combination of panic, intrigue and a facepalm. Now that I finally understand what OSGi is, I just don't ""get"" how something like Jigsaw has taken 20+ years to come to fruition, and then how that could have been canned from a release. It just seems fundamental. 
And, as a developer, I am also curious as to what my solutions are, sans OSGi.
Also, Note: I know this isn't a ""pure programming""-type question, but before some of you get your noses bent out of shape, I wanted to state (again, for the record) that I deliberately put this question on SO. That's because I have nothing but the utmost respect for my fellow SOers and I'm looking for an architectural-level answer from some of the ""Gods of IT"" that I see lurking around here every day.
But, for those of you who absolutely insist that a SO question be backed with some code segment:
int x = 9;

(Thanks to anybody who can weigh-in on this OSGi/Jigsaw/classloader/namespace/JAR hell stuff!)","First understand that Jigsaw's primary use case is to modularise the JRE itself. As a secondary goal it will offer a module system that may be used by other Java libraries and applications.
My position is that something like Jigsaw is probably necessary for the JRE only, but that it will create far more problems than it claims to solve if used by other Java libraries or apps.
The JRE is a very difficult and special case. It is over 12 years old and is a frightful mess, riddled with dependency cycles and nonsensical dependencies. At the same time is is used by approximately 9 million developers and probably billions of running systems. Therefore you absolutely cannot refactor the JRE if that refactoring creates breaking changes.
OSGi is a module system that helps you (or even forces you to) create software that is modular. You cannot simply sprinkle modularity on top of an existing non-modular codebase. Making a non-modular codebase into a modular one inevitably requires some refactoring: moving classes into the correct package, replacing direct instantiation with the use of decoupled services, and so on.
This makes it hard to apply OSGi directly to the JRE codebase, yet we still have a requirement to split the JRE into separate pieces or ""modules"" so that cut-down versions of the JRE can be delivered.
I therefore regard Jigsaw as a kind of ""extreme measure"" to keep the JRE code alive while splitting it up. It does not help code to become more modular, and I'm convinced that it will actually increase the maintenance required to evolve any library or application that uses it.
Finally: OSGi exists whereas Jigsaw does not exist yet and may never exist. The OSGi community has 12 years of experience in developing modular applications. If you are seriously interested in developing modular applications, OSGi is the only game in town.",https://stackoverflow.com/questions/7498540/osgi-java-modularity-and-jigsaw/7499297,3,1
Is there an access modifier that limits to a solution?,"In my .NET solution, I have two projects: one main project and a project for running tests against the main project. In my project, I have several methods that I'd like to keep ""private"", but would also like to run tests for. Is there an access method that could limit these functions to just inside of my solution?","You are looking for the InternalsVisibleTo attribute.
This attributes lets you specify other assemblies that should have access to types and methods that are internal to your assembly. So, in your main project AssemblyInfo.cs file (or any other source file), you can specify that your test project is a 'friend assembly' and should have access to the internals of your main project:
[assembly:InternalsVisibleTo(""MainProject.Tests"")]

On a side note, as pointed out by Alexei, if your MainProject is signed with a strong name key, any 'friend' assembly must also be signed. This is explained here
Although, as mentioned in another comment. Best practice is to test your assembly by using its public API.",https://stackoverflow.com/questions/34138823/is-there-an-access-modifier-that-limits-to-a-solution/34138970,4,1
"How does Spring 3+ autowire beans, which use each other?","For example, I have 
@Service
public class UserSerice {
  @Autowired
  private HouseService houseService;
}

and
@Service
public class HouseService {
  @Autowired
  private UserSerice userService;
}

How will Spring autowire this?
And is this a good practice to configure beans this way?","Since it's not a constructor injection, spring can safely instantiate both objects and then satisfy their dependencies. Architecture-wise such case is so called 'code smell'. It's the sign that something is wrong in the composition. Maybe you need to move logic, maybe you need to introduce third class, it depends.",https://stackoverflow.com/questions/13739056/how-does-spring-3-autowire-beans-which-use-each-other/13739655,3,1
What is the best way to query data from multilpe tables and databases?,"I have 5 databases which represent different regions of the country. In each database, there are a few hundred tables, each with 10,000-2,000,000 transaction records. Each table is a representation of a customer in the respective region. Each of these tables has the same schema. 
I want to query all tables as if they were one table. The only way I can think of doing it is creating a view that unions all tables, and then just running my queries against that. However, the customer tables will change all the time (as we gain and lose customers), so I'd have to change the query for my view to include new tables (or remove ones that are no longer used). 
Is there a better way?
EDIT
In response to the comments, (I also posted this as a response to an answer):
In most cases, I won't be removing any tables, they will remain for historic purposes. As I posted in comment to one response, the idea was to reduce the time it takes a smaller customers (one with only 10,000 records) to query their own history. There are about 1000 customers with an average of 1,000,000 rows (and growing) a piece. If I were to add all records to one table, I'd have nearly a billion records in that table. I also thought I was planning for the future, in that when we get say 5000 customers, we don't have one giant table holding all transaction records (this may be an error in my thinking). So then, is it better not to divide the records as I have done? Should I mash it all into one table? Will indexing on customer Id's prevent delays in querying data for smaller customers?","I think your design may be broken. Why not use one single table with a region and a customer column?
If I were you, I would consider refactoring to one single table, and if necessary (for reverse compatibility for example), I would use views to provide the same info as in the previous tables.

Edit to answer OP comments to this post :
One table with 10 000 000 000 rows in it will do just fine, provided you use proper indexing. Database servers are built to cope with this kind of volume.
Performance is definitely not a valid reason to split one such table into thousands of smaller ones !",https://stackoverflow.com/questions/752123/what-is-the-best-way-to-query-data-from-multilpe-tables-and-databases/752148,5,1
Company architecture has us defining same Enum in three places?,"Our company architecture is touted as being loosely coupled and and providing great separation of concerns. But, I'm wondering if we're ""doing it right"".
For example, let's say we have a Car object in our business / client layer. This Car has a ""CarEnum"" on it. When we are ready to save the Car to our database, we will map the Car to a CarDTO, which also has a CarEnum with the exact same options that are in the business layer Enum. When our DTO gets to the server code (in another solution), the DTO is mapped to yet another model with yet another CarEnum with the exact same definition as the business and DTO Enum.
This feels wrong.
I didn't expect ""good architecture"" to involve defining the same Enum in three places. I thought code was supposed to be DRY. Is this good architecture or is this just forcing weird cargo cult programming on the entire company? What is a better alternative architecture?","Having the ""same"" enum defined in 3 places is definitely an architecture\design smell. 
It's common to have an assembly for ""shared types"". This typically contains things like: interfaces, DTOs, Exceptions and enums that make sense across layers. It importantly contains no behavior\logic as such. This assembly can then be shared by your different layers without fear of coupling.",https://stackoverflow.com/questions/6917470/company-architecture-has-us-defining-same-enum-in-three-places/6917582,3,1
What are some open source tools to check a software architecture's robustness?,"I think it is a tough question but I want to verify the architecture of my software is robust enough.
I've plans to execute these tools on my code:

Gendarme
FxCop
StyleCop
Visual Studio's code metrics
Visual Studio's code analysis

But I want some tools to check (among others):

the relations between assembly's' dependencies
too strong coupling between my objects
and so forth.

In a word, I want open source tools to highlight any architectural glitches of my project.
I understand the best tool is an experienced architect, but even the best carpenter needs a good hammer ;)","The built-in Visual Studio tools are good, if you have the advanced editions.
You can also look at nDepend, which is a tool to do static analysis and point out areas of your system that have a high number of dependencies or coupling, among many other features.  nDepend is a commercial product, but has a free academic license and is free to use if you are working on open-source development -- so this may or may not meet your criteria.
Be aware that all of these tools have a learning curve, and you are probably not going to have a ""silver bullet"" that tells you exactly what to change on your system, but they can be really useful to improve performance and maintainability, and reduce risk.",https://stackoverflow.com/questions/7768994/what-are-some-open-source-tools-to-check-a-software-architectures-robustness/7773557,3,1
TypeScript How to access object defined in other class,"I have a project that uses Aurelia framework. I want to make global\static object that should be accessed across couple files. But when I try to access it from a different file it says that my object is undefined. Here is what it looks like:
FirstFile.ts
export function showA() {
    console.log(""Changed a to "" + a);
}
export var a = 3;

export class FirstFile {
    public ModifyA() {
        a = 7;
        showA();
    }

It says that a  = 7. Then I use it in other file like this.
SecondFile.ts
import FirstFile = require(""src/FirstFile"");
export class SecondFile {
    showA_again() {
        FirstFile.showA();
}

I execute showA_again() in my view file called SecondFile.html
<button click.trigger=""showA_again()"" class=""au-target"">Button</button>

When I click button, I see in console that variable ""a"" is still 3.
Is there any way to store variables between files?","I'd recommend you to inject FirstFile into SecondFile. Now your code has a smell of bad architecture.
To answer your question: probably you are looking for static (playground sample)
export class FirstFile {

    static showA = function() {
        console.log(""Changed a to "" + FirstFile.a);
    }

    static a = 3;

    public ModifyA() {
        FirstFile.a = 7;
        FirstFile.showA();
    }
}

export class SecondFile {
    showA_again() {
        FirstFile.showA();
    }
}",https://stackoverflow.com/questions/31720825/typescript-how-to-access-object-defined-in-other-class/31724489,1,1
Can I have circular dependencies in Composer?,"I'm writing a package A that is required by some other package B which I'm not publishing for now. At some point A will be changed to use itself B. Chances are they should be both in the same package then, but I'd prefer keeping the two things separate, just for cleanness' sake, and, more importantly, because B is only a dev dependency to A. 
package A requires-dev B
package B requires A

I'm curious if this is possible. I'm also curious if it's the same for:
package A requires B
package B requires A

...and...
package A requires B
package B requires C
package C requires A

... or more complex cases. What problems will I encounter?
Thank you.","A wider, not php-specific answer here: circular dependencies are never a good idea. 
You see, you ""separate"" things into different packages/modules/you-name-it in order to give them a useful structure. To create a ""model"" that helps you dealing with the complexity of your code. 
In other words: you want to define an architecture. And circular dependencies are most often seen as ""bad smell"" in designs. 
Thus you shouldn't ask ""would it work?"", but ""is there a better way to handle this?""",https://stackoverflow.com/questions/41778805/can-i-have-circular-dependencies-in-composer/41779968,1,1
Boundaries intercommunication restrictions Java EE,"Applying the Boundary-Control-Entity (BCE) pattern in Java EE:
@Stateless //1st boundary
public class A {}

@Stateless //2nd boundary
public class B {}

Until now, all it's ok, now, let's supose that for some reason I need use some services exposed by B on A. So, A now looks like:
@Stateless 
public class A {
    @Inject
    B b;
    //... call some B's methods
}

But, according to the BCE pattern stands for 

Control elements can communicate with each of the other two kinds, but
  entities and boundary elements should not communicate directly.

Obviously for JPA Entities they need to communicate each other (otherwise, ""JOINs"" won't be possible). Then, I end with some questions related:
1) Why communication between boundaries is forbidden?
2) Under Java EE, we can use @Remote interfaces, Will this still violates the statement?:
@Stateless 
public class A {
    @Inject
    RemoteB b; //now uses a remote dependency
}

@Stateless
@Remote(RemoteB.class)//implements a remote interface
public class B {}

3) How Java EE solves the pattern.","First of all, my advice is to use architecture as a guideline on how to structure your application, but never as a law - so always adapt it to your needs and do what is sensible, easy and fits to your situation.
The idea behind the boundary is, that it serves as the only externally visible contract to your business logic behind it, which may change and whose details are hidden. And it's reasonable to keep the dependence on other boundaries as low as possible - yet controls can make use of and can be called by many boundaries as it is needed.
Adam Bien, one of the Java EE gurus, emphasizes in his workshops and talks one this pattern, as he explains in this example. Another good article is this one.",https://stackoverflow.com/questions/24665136/boundaries-intercommunication-restrictions-java-ee/25838692,2,1
Is setting properties on the Window object considered bad practice?,"I'm writing a quite complex JavaScript application that has an MVC architecture that I'm implementing using Prototype's Class support and the Module pattern. The application uses AJAX and the Observer pattern. I create my controller instance when the DOM has loaded, pass it a view and some models created from JSON data and away it goes. 
However, I've found that I have to set my controller instance as a property on the Window object__.e. declare it without using var__ecause I have an AJAX success callback that refreshes the view object owned by the controller and at this point in the code my nice little MVC world is not in scope.
I investigated passing in the view object as a parameter to the function containing the AJAX code, but this got really messy and would have led to some horrible violations of the MVC pattern, such as coupling the model and the view. It was horrendous.
Is doing things like storing my controller instance directly on Window considered bad form? It smells a bit like using a global variable to me, but I can't see any way around it.","Setting properties on the window object is equivalent to creating global variables. That is, sometimes doing it is inevitable, but you should try to keep it to a bare minimum, as it ends up polluting the global namespace.
In your case, creating a single property is not so bad. If you want to be extra careful about it, you can explicitly create a namespace for any stuff you need global access to:
// In init:
var mynamespace = {};

. . .

// Once the controller is available:
var namespace = window.mynamespace;
namespace.controller = controller;
namespace.foo = bar; // Set other stuff here as well.",https://stackoverflow.com/questions/3679634/is-setting-properties-on-the-window-object-considered-bad-practice,3,1
What is Component-Driven Development?,"Component-Driven Development term is starting to get used widely, esp. in connection with Inversion of Control.

What is it?
What problems does it solve?
When is it appropriate and when not?","What is it?

I think the definition in your answer covers this question well.  Although, I question why the definition includes that a component needs to explicitly define its dependencies.  A canonical example of a component is an ActiveX control - do they need to explicitly define their dependencies?

What problems does it solve?

Management of complexity.  It seeks to address that by allowing you to only ever think about the implementation of the component.  One should only need to author components, one should not to have to think about how to combine or manage them.  That is done by some framework or infrastructure external to the component, and unimportant to the component author.

When is it appropriate and when not?

Not necessarily appropriate in a trival or throw-away application. The bad smell in a component architecture, is if you are spending time on thinking or working on the infrastructure to manage and combine components, rather than the components themselves.",https://stackoverflow.com/questions/933723/what-is-component-driven-development/960839,7,1
Can you have multiple delegate methods for the same function,"For example, could you have two delegats for MKMapViewDelegate and have them both implement - mapView:sender:viewForAnnotation:annotation?","The MKMapView can only have one delegate at a time. Of course you can switch the delegate when you need to, or you could have the delegate call the other object that you intend to use as a delegate.
But I think this smells of a bad architecture. Ask yourself: Why do you even want more than one delegate? You very probably don't.",https://stackoverflow.com/questions/10596741/can-you-have-multiple-delegate-methods-for-the-same-function/10596819,1,1
Importing modules to a package,"Is there anyway I can import modules to a package level?
for example, consider the following package:
- conf
  - __init__.py
  - general.py

Now I have another package:
- conf2
  - __init__.py

I would like to be able to use import conf2.general. That means, somehow importing the general.py module to conf2 package.

EDIT:
I have added the following import to conf2/__init__.py:
from conf import general
Now, I can use from conf2 import general and it works fine. However, what I would like to achieve is import conf2.general. Is that possible?","Contents of conf2/__init__.py:
from conf import general

to demonstrate the extra info from the edit, also: 
import conf.general

To demonstrate:
$ find .
.
./conf
./conf/__init__.py
./conf/general.py
./conf2
./conf2/__init__.py

$ python
Python 2.6.1 (r261:67515, Aug  2 2010, 20:10:18) 
[GCC 4.2.1 (Apple Inc. build 5646)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import conf2
>>> conf2.general
<module 'conf.general' from 'conf/general.pyc'>

again, to answer the edit:
>>> conf2.conf.general
<module 'conf.general' from 'conf/general.pyc'>",https://stackoverflow.com/questions/8778021/importing-modules-to-a-package/8778226,2,1
Swift IOS keep view controller running in background after pop,"My app consists of two views. The first one is a GMSMapView and the second one is used to connect to a Bluetooth device that sends coordinates. 
After the Bluetooth device is connected, I use a delegate to send the information back to the map view and move a marker around. To transition between views I was previously using segues, this didn't stop the Bluetooth view controller and the data made its way like I wished to the map view.
I ran into the problem of my map view being reinitiated so I decided to use a navigation controller. Now I use a push segue to get to my second view, and pop to come back to the same instance of the first one. Great, that worked! The issue I have now is that popping the second view seems to stop it completely from running in the background like it used to. Is there a way to keep it running in the background like it did before?
What I'm currently using to pop the second view is 
self.navigationController?.popViewControllerAnimated(true)

Any idea would be appreciated! Thanks!","A popped view controller does not ""stop running"". It is returned to you, and if you don't retain it, it is completely destroyed.
If you don't want that to happen, retain it when it is returned. You are currently ignoring the returned view controller:
 self.navigationController?.popViewControllerAnimated(true)

Instead, keep a reference to it:
self.mySecondViewController = 
    self.navigationController?.popViewControllerAnimated(true)

Be warned, however, that this is a very unusual architecture. You will not be able to use the storyboard segue to push again, because it will push a different copy. It would be better to abandon your navigation controller architecture entirely, as it is completely unsuited to the idea of a view controller persisting after it is popped. If you want an architecture where two view controllers persist simultaneously, you would be better off using a UITabBarController _ or, even better, reorganize your app completely. The notion that you need the view controller to persist after being popped is a ""bad smell"": it means that you have put the functionality in the wrong place. Put the functionality in a place that does persist, rather than forcing the view controller to persist in some artificial way.",https://stackoverflow.com/questions/43260955/swift-ios-keep-view-controller-running-in-background-after-pop/43261268,1,1
wrapper to template class inherited by another class,"template <class CollectionItem>
class Collection
{
    void A();
    // Many other utility functions
}

class ICollection
{
   virtual void B() = 0;
}


class Base : public Collection<BaseItem>, public IBase
{
    virtual void B();
}

Is there any way of offering Collection functions via ICollection interface without wrapping all the functions in Base class? ICollection : public Collection<CollectionItem> is not an option.
Bounty Update:
OK, so the original idea was to have Interface to all Collection classes. Before we continue, every CollectionItem also has Interface, let's call it ICollectionItem and ICollection only knows about ICollectionItem.
So what I did was create another template class as Interface to Collection template class - ICollection (pure virtual) accepting ICollectionItem(s). Collection class inherits this interface.
Every Collection class (inheriting Collection<CollectionItem> class) would also inherit it's Interface Collection class. That Interface then virtual inherits ICollection<ICollectionItem>. I'll just post the code :)
Here is the code:
template <class ICollectionItem>
class ICollection
{
public:
    virtual const ICollectionItem* At(const int idx) = 0;
};

template <class CollectionItem, class ICollectionItem>
class Collection
    : public ICollection,
      public virtual ICollection<ICollectionItem>    // Weak point
{
private:
    List<CollectionItem*> fContainer;

public:
    Collection(void) {}
    virtual ~Collection() {}

    virtual const ICollectionItem* At(const int idx);  // Casting GetAt result
    virtual const TCollectionItem& GetAt(const int idx) const 

    virtual ListIterator<TCollectionItem> >* GetIterator(void) const;
    virtual ListIterator<ICollectionItem> >* Iterator(void) const;  // Weak point
}

Example usage:
class IBaseItem
{
public:
    virtual int Number() = 0;
{

class BaseItem
    : public IBaseItem
{
public:
    virtual int Number();
    void SetNumber(int value);
}

class IBase
    : public virtual ICollection<IBaseItem>
{
public:
    virtual IBaseItem* ItemByName(String name) = 0;
    virtual ~IBase() {}
}

class Base
    : public Collection<BaseItem, IBaseItem>,
      public IBase
{
public:
    BaseItem* GetItemByName(String name);
    virtual IBaseItem* ItemByName(String name);
}

Weak points: 
 First is at using virtual inheritance ... lots written about it, not much to talk about, or is it?
 Unable to access Iterator using ICollection interface. See ListIterator function, only first one can be implemented, the second one would require some kind of new List of IBaseItem. I decided to live with that and just use for loop.

Even tho I somehow managed to get what I wanted (With wrapping and casting), I would still like to hear an second opinion. I don't like using virtual inheritance, specially in such delicate situations - using Collections for application Base creation.","I can not see any other solution than calling some Collection method in Base implementation of IBase virtual methods.
class Base : public Collection<BaseItem>, public IBase
{
    virtual void B()
    {
        A();
    }
}",https://stackoverflow.com/questions/17087761/wrapper-to-template-class-inherited-by-another-class/19235162,7,1
Implement ActiveRecord for Webservice client,"I'm writing a webservice client with C#/MVC 4 that communicates with a REST-Webservice, using JSON.net on the client side.
Everything is working fine so far, but I want to improve the architecture to make the handling more fluid.
I wrote a connector class and initialize it like this:
var conn = new MyConnector(""admin"", ""admin"", ""http://localhost:9000"");

Then I have a POCO class like this:
public class MyRecord
{
    [JsonProperty(""record_id"")]
    public string RecordId;
    ...

I'm saving changes by performing a PUT request to the Webservice. It looks like:
var updated = conn.UpdateRecord(""MyRecordId"", new  NameValueCollection{{""title"",""new_title""}});

What I want to do is to implement it more like ActiveRecord:
var myRecord = conn.GetRecord(""myRecordId);
myRecord.title = ""Foo"";
myRecord.save();

That means that the MyRecord-class must be aware of the connection handler. That seems bad design to me, because MyRecord is basically a plain object. Another choice would be to pass the connection handler to the save-method, but that smells, too.
Any suggestions for improving it?","If you want a ""plain record"" you should not implement the activerecord pattern. That is a contradiction. It sounds to me like you are more interested in the repository pattern.
As references have a look at:
Is Repository pattern as same as Active Record pattern?
or for a broader perspective of your options:
http://msdn.microsoft.com/en-us/magazine/dd569757.aspx",https://stackoverflow.com/questions/11613748/implement-activerecord-for-webservice-client,1,1
php isset do you need it in a form check?,"I trying to understand if a isset is required during form processing when i check $_REQUEST[""input_name""] if no value is passed it doesn't cry about it and php doesn't throw out an error if you are trying to access a array item which doesn't exist....i can use if($_REQUEST[""input_name""])..
what about ""empty"" even in those cases i can use if()
THnks","if($_REQUEST[""input_name""])

will throw a notice (error) if ""input_name"" doesn't exist, so isset() is recommended.",https://stackoverflow.com/questions/818326/php-isset-do-you-need-it-in-a-form-check/818332,5,1
Bypassing (disabling) Delphi's reference counting for interfaces,"For one particular issue in the architecture of an application I'm working on, interfaces seem to be a nice solution. Specifically, some ""business objects"" depend on a bunch of settings that are pulled from the database in the actual app. Letting those business objects ask for an interface (through Inversion of Control), and letting a central TDatabaseSettings object implement those interfaces, allows for better isolation, and thus for much easier unit testing.
However, in Delphi, interfaces seem to come with an, in this case, unpleasant bonus: reference counting. This means that if I do something like this:
type
IMySettings = interface
    function getMySetting: String;
end;

TDatabaseSettings = class(..., IMySettings)
    //...
end;

TMyBusinessObject = class(TInterfacedObject, IMySettings)
    property Settings: IMySettings read FSettings write FSettings;
end;

var
  DatabaseSettings: TDatabaseSettings; 
    // global object (normally placed in a controller somewhere)

//Now, in some function...
O := TMyBusinessObject.Create;
O.Settings := DatabaseSettings; 
// ... do something with O
O.Free;

On the last line (O.Free), my global DatabaseSettings object is now also freed, since the last interface reference to it (which was contained in O) is lost!
One solution would be to store the 'global' DatabaseSettings object with an interface; another solution would be to override the reference counting mechanism for the TDatabaseSettings class, so I can continue to manage the DatabaseSettings as a normal object (which is much more consistent with the rest of the app).
So, in summary, my question is: how do I disable the interface reference counting mechanism for a particular class?
I've been able to find some info that suggests overriding the IInterface methods _AddRef and _Release for the class (TDatabaseSettings in the example); has anyone ever done that?
Or would you say I shouldn't do this (confusing? just a bad idea?), and find a different solution to the architectural problem?
Thanks a lot!","Ok, you can bypass it, but the question is if you really want that.
If you want to use interfaces, you better use them completely. So as you have experienced it, you get problems if you mix class and interface variables.
var
  // DatabaseSettings: TDatabaseSettings; 
  DatabaseSettings : IMySettings;

//Now, in some function...
O := TMyBusinessObject.Create;
O.Settings := DatabaseSettings; 
// ... do something with O
O.Free;

You now have a second reference to the interface and losing the first will not free the object.
It as also possible to keep both the class and the object:
var
  DatabaseSettings: TDatabaseSettings; 
  DatabaseSettingsInt : IMySettings;

Be sure to set the interface right after the object has been created.
If you really want to disable reference counting, you just have to create a new descendant of TObject that implements IInterface. I have tested the example below in D2009 and it works:
// Query Interface can stay the same because it does not depend on reference counting.
function TMyInterfacedObject.QueryInterface(const IID: TGUID; out Obj): HResult;
begin
  if GetInterface(IID, Obj) then
    Result := 0
  else
    Result := E_NOINTERFACE;
end;

constructor TMyInterfacedObject.Create;
begin
  FRefCount := 1;
end;

procedure TMyInterfacedObject.FreeRef;
begin
  if Self = nil then
    Exit;
  if InterlockedDecrement(FRefCount) = 0 then
    Destroy;    
end;

function TMyInterfacedObject._AddRef: Integer;
begin
  Result := InterlockedIncrement(FRefCount);
end;

function TMyInterfacedObject._Release: Integer;
begin
  Result := InterlockedDecrement(FRefCount);
  if Result = 0 then
    Destroy;
end;

FreeRef just lowers the refcount just like _Release. You can use it where you normally use Free.",https://stackoverflow.com/questions/769329/bypassing-disabling-delphis-reference-counting-for-interfaces/769504,6,1
Is holding static reference to multiple objects in abstract class OK?,"Is there anything significantly bad in terms of performance, memory consumption, design etc about below pattern ?
public abstract class A {
    public static final D d = new D();
    public static final C c = new C();
    ......... and many more member of class B, C, and D; like hundreds
}

public class B extends A {
}

public class C extends B {
}

public class D extends C {
}","Is there anything significantly bad in terms of performance, memory consumption, design etc about below pattern ?


performance - not really.
memory - no, if you (1) have predictable bound on how many internal objects will be created (and you are ok with memory they will occupy), (2) have a good idea of lifespan of these objects, (3) you are confident that your abstract class wont be leaked. Otherwise, the answer is maybe.
design - yes. What you are trying to do smells badly from design point of view. It is really hard to tell what you are actually trying to achieve, but the chance that you can design your architecture better is rather high.",https://stackoverflow.com/questions/46226075/is-holding-static-reference-to-multiple-objects-in-abstract-class-ok/46226314,2,1
Cyclic dependency between Type A and B,"I have two classes:
class A {

    final B b;

    A(final B b) {
    this.b = b;
    }

    void doIt() {
    b.doSomething();
    }

    void doSomething() {
    // TODO Auto-generated method stub
    }
}

and 
class B {

    final A a;

    B(final A a) {
    this.a = a;
    }

    void doIt() {
    a.doSomething();
    }

    void doSomething() {
    // TODO Auto-generated method stub
    }
}

There is no way to instantiate any of them.
I could use a setter, e.g:
class B {

    A a;

    B() {

    }

    void setA(final A a) {
    this.a = a;
    }

    void doIt() {
    a.doSomething();
    }

    void doSomething() {
    // TODO Auto-generated method stub
    }
}

But here I need checking for a being null in doIt() and handling this case. This is not the end of the world, but maybe 
there is a more clever way to do it?
Or maybe this is even an anti pattern in general and something is wrong with the architecture in the first place?
Root of the problem:
I have a database from which I load entities. I cache them once loaded and use this cache to establish bi-directional relationships between the types. This is necessary, since when I load multiple instances of A, they should have (in this case) all the same instance of B. Therefore B needs to be instantiated before and re-used. But B has also a relationship to all these as, therefore the cyclic dependency.
So in more short, The cyclic dependencies are caused by bi-directional relationships in the database. I use to avoid them where possible but besides of the trouble described here, there is no 'real-world' problem with bi-di relationships, in fact it is a very natural thing.
So maybe the question should be How to properly map bi-di-relationships from a relational database into the oop world?
More concrete example:
To establish bi-di relationships, I load instances from a cache, so that I have the same instance when the entity ID is the same:
interface Cache<T>

interface CacheA extends Cache<A>

interface CacheB extends Cache<B>

class CacheManager {

    final CacheA cacheA;

    final CacheB cacheB;

    CacheManager(final DatabaseAccess databaseAccess) {
    cachA = new CacheA();
    cachB = new CacheB();
    cacheA.setEntityLoader(id -> new SimpleAttachedA(id, databaseAccess, cacheB));
    cacheB.setEntityLoader(id -> new SimpleAttachedB(id, databaseAccess, cacheA));

    }

}

Instances of A will access this cache if the relationship to B is accessed. The same the other way around.
I know that it would be best, if CacheA and CacheB would be the same object, since then I could just create the cache and pass it on to all instances of A and B.
CacheA and CacheB used to be the same, namely Cache. I chose to separate those, so I can use a generic class and remove a lot of duplicate code.
CacheManager cannot implement CacheA and CacheB, if they both extend the same generic interface Cache<T> but with different types of T.
Therefore CacheManager uses composition instead of inheritance. So I end up with two caches, which need to access each other to realize the bi-di relationship of A and B.","Cyclic dependencies are typically a sign of bad design. In some cases you cannot prevent cyclic dependencies, but you should always think about another solution. When you have cyclic depedencies, chances are that you will change one of the classes and have to change the other one as well. When your cycle contains more than two classes, this can be a lot of work. Also problems like the one you mentioned, ""I need the other class to instantiate one of them"", arise.
As your classes appear to be dummy classes, I cannot give a really good advise, but some general points nonetheless.
Classes should have high cohesion and low coupling. Meaning, classes should depend on another as less as possible (low coupling), while every class should do one functionality, and all of that functionality (high cohesion).
When you have two classes which depend on one another, this is typically a sign for low cohesion, as part of the functionality is in class A and another part in class B. In this case you should consider merging both classes in one class.
On the other hand when you try to find a class name for that merged class and come up with something like ThisAndThatDoer, you should split them in two classes ThisDoer and ThatDoer, as this is a sign for low cohesion. When you then have your original classes depending on one another again, you could maybe create a new class Executor which connects both classes. But this can quickly become a god class, which is also an anti-pattern. So you should be careful with this.
So all in all, I recommend thinking about your class design and finding a way to remove the depedency at least in one direction. I hope this helps with your problem.",https://stackoverflow.com/questions/45529749/cyclic-dependency-between-type-a-and-b/45530277,4,1
VueJs 2.0 emit event from grand child to his grand parent component,"It seems that Vue.js 2.0 doesn't emit events from a grand child to his grand parent component.
Vue.component('parent', {
  template: '<div>I am the parent - {{ action }} <child @eventtriggered=""performAction""></child></div>',
  data(){
    return {
      action: 'No action'
    }
  },
  methods: {
    performAction() { this.action = 'actionDone' }
  }
})

Vue.component('child', {
  template: '<div>I am the child <grand-child></grand-child></div>'
})

Vue.component('grand-child', {
  template: '<div>I am the grand-child <button @click=""doEvent"">Do Event</button></div>',
  methods: {
    doEvent() { this.$emit('eventtriggered') }
  }
})

new Vue({
  el: '#app'
})

This JsFiddle solves the issue https://jsfiddle.net/y5dvkqbd/4/ , but by emtting two events: 

One from grand child to middle component
Then emitting again from middle component to grand parent

Adding this middle event seems repetitive and unneccessary. Is there a way to emit directly to grand parent that I am not aware of?","The Vue community generally favors using Vuex to solve this kind of issue. Changes are made to Vuex state and the DOM representation just flows from that, eliminating the need for events in many cases.
Barring that, re-emitting would probably be the next best choice, and lastly you might choose to use an event bus as detailed in the other highly voted answer to this question.
The answer below is my original answer to this question and is not an approach I would take now, having more experience with Vue.

This is a case where I might disagree with Vue's design choice and resort to DOM.
In grand-child,
methods: {
    doEvent() { 
        try {
            this.$el.dispatchEvent(new Event(""eventtriggered""));
        } catch (e) {
            // handle IE not supporting Event constructor
            var evt = document.createEvent(""Event"");
            evt.initEvent(""eventtriggered"", true, false);
            this.$el.dispatchEvent(evt);
        }
    }
}

and in parent,
mounted(){
    this.$el.addEventListener(""eventtriggered"", () => this.performAction())
}

Otherwise, yes, you have to re-emit, or use a bus.
Note: I added code in the doEvent method to handle IE; that code could be extracted in a reusable way.",https://stackoverflow.com/questions/51657920/emitting-events-to-a-distant-relative-of-a-component,9,1
Is procedural code ever appropriate for rendering a UI View? [closed],"Background
At work, I ran into some code that used a series of procedural methods to build an HTML View. It felt really wrong to me, and while working on it, I managed to introduce unintended layout changes.
See the pseudo-code below for a more detailed explanation.
Questions

The project was an ASP.NET web application targeting the 4.0 framework. With all the tools available to an ASP.NET developer, is there any reason this pattern would be appropriate.
Ultimately, given no other time priorities, should this code be refactored ? It definitely smells of code fragility.

Caveats
I'm not 100% sure if this is an appropriate question for SO. It falls in the somewhat subjective topic of programming patterns. As a subjective topic, there is the potential for the SO Answers to devolve into a discussion, which as we all know, is not the purpose of SO. I don't think this question qualifies as code-golf.
What I'm looking for is objective evidence that this particular pattern is
either a bad idea for some reason such as it leads to unmaintainable code. Or evidence that this pattern is appropriate for certain situations, bonus points for citing a specific example.
Had I written this code, I would have probably used some sort of well-known templating engine. I'd handle the logic decisions (highlight rows for people who have birthdays) in non-view code. I suppose I'm a proponent of logic-less template engines. 
I apologize for the long code, I tried to keep it terse without sacrificing context. Hopefully, this will be a fun question to answer for some battle-scarred SO veteran.
The Code
About the pseudo-code
The code in this question does not represent any actual programming language.
It is mostly based on JavaScript with HTML terms thrown in there. It should
be easy enough to interpret without significant mental gymnastics. 
page load event handler
protected void Page_Load(object sender, EventArgs e) {

    var data = DataLayer.getData();
    var engine = new DataViewRenderEngine(data);
    var divElement = engine.Render();

    document.body.add(divElement);

}

Definition of DataViewRenderEngine
class DataViewRenderEngine {

    private _dataSource = null;

    public DataViewRenderEngine(dataSource) {
        this._dataSource = dataSource;
    }

    public Element Render() {
        return RenderView(this._dataSource);
    }

    private Element RenderView(data) {

        var containerDiv = RenderContainerDiv(data);            

        var pageHeader = PenderPageHeader(data.Title);
        containerDiv.add(pageHeader)    

        var personTable = renderPersonTable(data.PeopleArray)
        containerDiv.add(personTable);

        return containerDiv;        

    }

    private Element RenderContainerDiv() {
        return new Element({
            tag: 'div',
            class: 'container'
        });
    }

    private Element RenderPageHeader(title) {       
        return new Element({
            tag: 'h1',
            innerHTML: title        
        });     
    }

    private Element RenderPersonTable(peopleArray) {

        var table = new Element('table');

        for(var person in peopleArray) {
            var row = RenderPersonTableRow(person);
            table.add(row); 
        }

        return table;
    }

    private Element RenderTableRow(person) {
        var row = new Element('tr');

        row.add(new Element({
            tag: 'td',
            innerHTML: person.firstName     
        });

        row.add(new Element({
            tag: 'td',
            innerHTML: person.lastName      
        });

        var today = Date.Today();
        if(person.birthday.Month == today.Month &&
           person.birthday.Day   == today.Day) {
            row.BackgroundColor = 'green';
        }

        return row; 
    }

}","Actually it seems like you think that object-oriented programming has no trace of the past: procedural programming is still there.
When you implement methods, their bodies are still procedural. That is, you're calling methods which used to be procedures and functions later fused into a single concept.
Later on, there're good and bad designs. If your coworker's code looks like your pseucode, then it seems that it smells.
A render engine shouldn't be tied to a particular view. Yours seem like a mix of a view, view template and view renderer. Thus, clearly breaks single responsibility principle as it does too many things.
Conclusion: it's just a bad design/architecture. That's all.
Now I'll answer your concrete questions:

The project was an ASP.NET web application targeting the 4.0
  framework. With all the tools available to an ASP.NET developer, is
  there any reason this pattern would be appropriate.

In 2017, probably you shouldn't invest more effort on ASP.NET Web Forms. If they did this way, either adapt yourself to their bad design, or convince them to switch to ASP.NET MVC or even to abandon server-side view programming in favor of a full HTML5 web app on the client-side.

Ultimately, given no other time priorities, should this code be
  refactored ? It definitely smells of code fragility.

This question deserves the same answer as your first one.",https://stackoverflow.com/questions/41771642/is-procedural-code-ever-appropriate-for-rendering-a-ui-view/41772312,1,1
Why not use an IoC container to resolve dependencies for entities/business objects?,"I understand the concept behind DI, but I'm just learning what different IoC containers can do.  It seems that most people advocate using IoC containers to wire up stateless services, but what about using them for stateful objects like entities?
Whether it's right or wrong, I normally stuff my entities with behavior, even if that behavior requires an outside class.  Example:
public class Order : IOrder
{

    private string _ShipAddress;
    private IShipQuoter _ShipQuoter;

    public Order(IOrderData OrderData, IShipQuoter ShipQuoter)
    {
        // OrderData comes from a repository and has the data needed 
        // to construct order
        _ShipAddress = OrderData.ShipAddress;  // etc.
        _ShipQuoter = ShipQuoter;

    }

    private decimal GetShippingRate()
    {
        return _ShipQuoter.GetRate(this);
    }
}

As you can see, the dependencies are Constructor Injected. Now for a couple of questions.

Is it considered bad practice to have your entities depend on outside classes such as the ShipQuoter?  Eliminating these dependencies seems to lead me towards an anemic domain, if I understand the definition correctly.
Is it bad practice to use an IoC container to resolve these dependencies and construct an entity when needed?  Is it possible to do this?

Thanks for any insight.","The first question is the most difficult to answer. Is it bad practice to have Entities depend on outside classes? It's certainly not the most common thing to do.
If, for example, you inject a Repository into your Entities you effectively have an implementation of the Active Record pattern. Some people like this pattern for the convenience it provides, while others (like me) consider it a code smell or anti-pattern because it violates the Single Responsibility Principle (SRP).
You could argue that injecting other dependencies into Entities would pull you in the same direction (away from SRP). On the other hand you are certainly correct that if you don't do this, the pull is towards an Anemic Domain Model.
I struggled with all of this for a long time until I came across Greg Young's (abandonded) paper on DDDD where he explains why the stereotypical n-tier/n-layer architecture will always be CRUDy (and thus rather anemic).
Moving our focus to modeling Domain objects as Commands and Events instead of Nouns seems to enable us to build a proper object-oriented domain model.
The second question is easier to answer. You can always use an Abstract Factory to create instances at run-time. With Castle Windsor you can even use the Typed Factory Facility, relieving you of the burden of implementing the factories manually.",https://stackoverflow.com/questions/4835046/why-not-use-an-ioc-container-to-resolve-dependencies-for-entities-business-objec/4836790,2,1
C# - How to disable all events on WPF form,"I have a lot of ToggleSwitch and Combobox controls in my application, I am setting them up on application startup, which fires all the events associated with those controls.
These events are supposed to fire only when the user interacts with the controls and not when the value is changed programmatically.
Is there a way to disable all the events and reactivate them afterwards?
I didn't find any efficient solution on other posts or on the Internet.","Disabling all events isn't possible because there are many event handlers in the framework that you have no direct control over. The fact you want to do this at all suggests that your design is flawed.
You can detect the difference between user interaction and programmatic change with a bit of plumbing and the diligence to use it everywhere that it matters. Suppose you have a ComboBox, and you want to detect when the user triggers SelectionChanged. This can be done with a flag, set only when you make programmatic changes. i.e.
private bool blockHandlers;

// Wrapped in a method for convenience.
public void SetSelectedIndex(int index)
{
    blockHandlers = true;
    comboBox.SelectedIndex = index;
    blockHandlers = false;
}

private void ComboBox_SelectionChanged(object sender, SelectionChangedEventArgs e)
{
    if (blockHandlers) return;

    // Your event handling code...
}

Using this technique requires that you always either use SetSelectedIndex or set/reset blockHandlers around programmatic changes to ensure event handler(s) observe it and do nothing.",https://stackoverflow.com/questions/48425977/c-sharp-how-to-disable-all-events-on-wpf-form/48449698,2,1
DI Service with No Dependent Services,"I have been working with Ninject to implement an application using dependency injection. I feel like I have a pretty thorough understanding of the concepts and have really liked the loosely coupled and testable architecture that the application has achieved using DI. I am struggling with one specific type of service, however, and am looking for insight into whether I am doing something wrong or if others have ran across the same thing.
Basically, I end up with some services/classes (a pretty small number) that have no other services depending on them. Because of this, the class never gets instantiated even though it is required to since it performs a useful role in the application. As an example, say I have an IMonkeyRepository service and an IMonkeyPopulator service. Assume the IMonkeyPopulator service really has no public API, and its sole responsibility (following the Single Responsibility Principle) is to discover monkeys on the network and populate the IMonkeyRepository with them. This service depends on the IMonkeyRepository and perhaps some other service(s) to handle its interaction with the network (configuration data for ports and addresses, for example). However, the IMonkeyPopulator has no public API, its just an empty interface.
Is this a bad design or some sort of code smell that I'm missing? I could obviously move this functionality into the repository itself but that seems like a violation of SRP to me (the repository has useful access functions, etc., and could actually be populated by multiple services). Some approaches that I've considered or tried but am not happy with are:

Make the service have a single public method, such as Start, that must be called for it to begin work. This has the drawback of needing to determine a somewhat arbitrary spot in the system to make this call.
Bind the service to a constant that I instantiate when the Ninject kernel is created. This requires that I understand that no one is dependent on this service so it must be handled specially, which seems wrong.
Add some members to the service and make a GUI somewhere in my application that reads these values (such as status of the service, etc.). Obviously, having to add a GUI to my application that is only there for this reason is quite silly (although at times useful for debugging, etc.).

Any thoughts or guidance?","You say that the IMonkeyPopulator depends on the IMonkeyRepository, but it seems like that should be the other way around?  It sounds like your IMonkeyRepository depends on, and consequently may need to be injected with, an IMonkeyPopulator.  If you also inject some other service fine, but internally, the IMonkeyRepository could tell the IMonkeyPopulator to ""start"" so that there's actually something in the repository?  I might be misunderstanding the problem though... maybe I shouldn't be monkeying around so much :/",https://stackoverflow.com/questions/14371700/di-service-with-no-dependent-services,2,1
How can I avoid downcasting when passing information through a queue?,"I'm writing a tool which enables a user to interact with a bit of hardware by changing settings and then streaming information.
To do this I have a couple of threads running: EquipmentInterface and DataProcessor which are connected by a Queue.
The EquipmentInterface thread has methods to alter settings on the equipment (Rotate and Refocus for example) and the resulting information (CurrentAngle and CurrentFocalDistance) is added to the Queue. Once the settings are correct there are methods to StartStreaming and StopStreaming and once streaming starts, data from the equipment is packetised and added onto the queue.
All of the information placed on the queue derives from a single BaseMessage class which includes an indication of the message type. I then have derived message types for angles, focal distances, beginning and ending streaming and of course, the data itself.
The DataProcessor listens to the other end of the Queue and depending on the current angle / focal distance, processes the subsequent data.
Now, the thing is, I have a function in the data processor which uses a switch statement to type-check the messages coming in. Those messages are then down-casted to the appropriate type and passed to an appropriate handler. In reality, there's more than just a DataProcessor listening to a single queue, but in fact multiple listeners on multiple queues (some store to disk, some display information on a gui). Every time I add some information I have to create a new BaseMessage derived class, add a new type to that base class and then update the switch statements in each of the consumers to cope with the new message.
Something about this architecture feels wrong to me and I've been reading a lot about down-casting recently. From what I've seen, the general consensus seems to be that what I'm doing is a bad code smell. I've seen a suggestion which use Boost, but they don't look any cleaner than the switch statement to me (maybe I'm missing something?).
So my question is: Should I be trying to avoid the switch-statement / downcasting solution and if so, how?
My implementation is in C++/CLI so either .net or C++ solutions are what I'm after.
Edit - Based on the comments from iammilind and stfaanv, is this the sort of thing you're suggesting:
class QueuedItem
{
public:
    QueuedItem() { }
    virtual ~QueuedItem() { }

};

class Angle : public QueuedItem
{
public:
    Angle() {}
    virtual ~Angle() { }
};

class FocalLength : public QueuedItem
{
public:
    FocalLength() {}
    virtual ~FocalLength() { }
private:

};


class EquipmentHandler
{
protected:
    virtual void ProcessAngle(Angle* angle) {}; 
    virtual void ProcessFocalLength(FocalLength* focalLength) {};   

public:
    void ProcessMessages(QueuedItem* item)
    {
        Angle* pAngle = dynamic_cast<Angle*>(item);
        if( pAngle != NULL )
        {
            ProcessAngle(pAngle);
        }
        FocalLength* pFocalLength = dynamic_cast<FocalLength*>(item);
        if( pFocalLength != NULL )
        {
            ProcessFocalLength(pFocalLength);
        }

    }
};

class MyDataProcessor : public EquipmentHandler
{
protected:
    virtual void ProcessAngle(Angle* angle) override { printf(""Processing Angle""); }
    virtual void ProcessFocalLength(FocalLength* focalLength) override { printf(""Processing FocalLength""); };   
};


int _tmain(int argc, _TCHAR* argv[])
{

    // Equipment interface thread...
    FocalLength* f = new FocalLength();
    QueuedItem* item = f; // This gets stuck onto the queue

    // ...DataProcessor thread (after dequeuing)
    QueuedItem* dequeuedItem = item;

    // Example of a DataProcessor implementation.
    // In reality, this would 
    MyDataProcessor dataProc;
    dataProc.ProcessMessages(dequeuedItem);

    return 0;
}

...and can it be simplified? The ProcessMessages feels a bit clunky but that's the only way I could see to do it without a switch statement and some sort of enumerated message type identifier in the base class.","You could try a visitor design pattern: http://en.wikipedia.org/wiki/Visitor_pattern
Each DataProcessor would inherit from a BaseVisitor class, which defines virtual method for handling each type of Message. Basically these methods are just noop. 
When you define a new message type you add a new virtual method with a noop implementation for this message type in the BaseVisitor. Then if a child DataProcessor class wants to process this message type you override the virtual method in this DataProcessor only. All other DataProcessorremain untouched.
    #include <iostream>


    class FocalLength;
    class Angle;
    class EquipmentVisitor;

    class QueuedItem
    {
    public:
            QueuedItem() { }
            virtual ~QueuedItem() { }

            virtual void AcceptVisitor(EquipmentVisitor& visitor) = 0;
    };

    class EquipmentVisitor
    {
    public:
            virtual ~EquipmentVisitor() {}

            virtual void Visit(FocalLength& item) {}
            virtual void Visit(Angle& item)       {}

            void ProcessMessages(QueuedItem* item)
            {
                    item->AcceptVisitor(*this);
            }
    };

    class Angle : public QueuedItem
    {
    public:
            Angle() {}
            virtual ~Angle() { }

            void AcceptVisitor(EquipmentVisitor& visitor) { visitor.Visit(*this); }
    };

    class FocalLength : public QueuedItem
    {
    public:
            FocalLength() {}
            virtual ~FocalLength() { }

            void AcceptVisitor(EquipmentVisitor& visitor) { visitor.Visit(*this); }
    private:

    };

    class MyDataProcessor : public EquipmentVisitor
    {
    public:
            virtual ~MyDataProcessor() {}

            void Visit(Angle& angle)             { std::cout << ""Processing Angle"" << std::endl; }
            void Visit(FocalLength& focalLength) { std::cout << ""Processing FocalLength"" << std::endl; }
    };


    int main(int argc, char const* argv[])
    {
            // Equipment interface thread...
            FocalLength* f    = new FocalLength();
            QueuedItem*  item = f; // This gets stuck onto the queue

            // ...DataProcessor thread (after dequeuing)
            QueuedItem* dequeuedItem = item;

            // Example of a DataProcessor implementation.
            // In reality, this would
            MyDataProcessor dataProc;
            dataProc.ProcessMessages(dequeuedItem);

            return 0;
    }",https://stackoverflow.com/questions/10101443/how-can-i-avoid-downcasting-when-passing-information-through-a-queue,3,1
Is it a good practice to reference servlet as OSGI component?,"In my project we have few servlets that are osgi components and have properties tied to them. Like 
@Component(label = ""Default Address Servlet"", immediate = true)
@Service(value = Servlet.class)
@Properties(
    @Property(name = ""sling.servlet.resourceTypes"", value = { ""sling/servlet/default"" }),
    @Property(name = ""sling.servlet.selectors"", value = { ""defaultaddress"" }),
    @Property(name = ""sling.servlet.extensions"", value = { ""json"" }),
    @Property(name = ""sling.servlet.methods"", value = { ""POST"" }), 
    @Property(name = ""prop1"", value = { ""value1"" }) })
public class SetDefaultAddressServlet extends SlingAllMethodsServlet {

Now I have a requirement to use this prop1 inside a sling model class during a load of a component. While technically servlets are osgi components, it is ok to reference this servlet as osgi component? Like 
@Model(adaptables = SlingHttpServletRequest.class)
public class Address {

    @Inject
    @Reference
    private SetDefaultAddressServlet service;

Though this technically works right, is it a good approach? Or alternate, I need to create separate OSGi service with the respective property and reference it. What is the advisable approach?","It's a philosophical question, rather than technical because nobody can give you 100% correct answer, but I will put my opinion there.
I prefer applying Hexagonal Architecture to my code (original post on this pattern or another great one) wich brings to your application high maintainability.
One of its main ideas is that dependencies should point inwards - so external world depends on your servlet, servlet depends on your business layer.
Servlet can be treated here as an adapter between external world and your business logic. Sling Model is just the same - it is sitting between your html page and service.
And according to this design pattern, it's a bad idea to reference one adapter from another.
Also, there is a light code smell: what do these classes need this property for? Should the code, which needs the same property be placed in 2 different places, rather than in one small Util class or Service?
Again, it's just my opinion.",https://stackoverflow.com/questions/41767567/is-it-a-good-practice-to-reference-servlet-as-osgi-component/41769710,2,1
Using angular service inside each other [duplicate],"This question already has an answer here:


Problems with circular dependency and OOP in AngularJS

                    3 answers
                



Do you know if it's possible to use angular services inside each other? I tried it but getting error ""Circular dependency found"". For example I have two services A and B. Can I use service A inside service B and service B inside service A ?","No , you cant do it with basic angularjs dependency injector, but you can retrieve any service with $injector
https://docs.angularjs.org/api/auto/service/$injector
so can ""lazy"" load it to any var, and access it in other service.... but you should post some code.
Also it is smell like you are anti patterning something , or you have bad architecture when you have circular dependencies. Its like on the road :
Assume you have 2 Services
Foo Depends on Bar
Bar depends on Foo

Angular will go as this
Foo -> Depends on bar so load Bar -> Instantiate Bar but its depends on Foo so instantiate Foo -> Instantiate Foo but it depends on Bar but wait! Bar is still not instantiated because it waiting for Foo , so its cyclic. 
You should have service
Foo with no dependency
Bar with no dependency

and Third factory 
FooBar which will handle functions which needs the circular dependency.",https://stackoverflow.com/questions/31083051/using-angular-service-inside-each-other/31083496,1,1
"angular 6 post request return correct object, but from service to component it gets undefined","I'm new in Angular, its architecture and typescript. I'm Struggling to achieve this simple thing and i didn't find any tutorial which i could follow. So i will really appreciate a little help with this. 
I have a project or web app where I'm practicing a simple login process. In this case login component has a method to login where the app takes as arguments user and password from textfields and call a user service method which is in charge of doing a post request to a REST service. I've verified that Rest server send as body the answer I expect (in this case, the complete user in case of right user and password values). Login component subscribes to this call and I want it passing the value obtained as User in a property. BUT even when I get a JSON object as response, when service pass the value to login component it becomes undefined.
I know it seems simple but I think I'm misunderstanding the architecture of angular itself or maybe the observables... I don't know, I will appreciate any kind of advice in this.
Let me present you the code:
User class:
export class Usuario {
    // atributos
    nombreDeUsuario: string;
    email: string;
    password: string;
    fechaNacimiento: Date;
    karma: number; /*puntuaci贸n del usuario en la comunidad*/
    isAdmin: boolean; /*flag que establece si es administrador o no*/
    amigos: Usuario[]; /* lista de amigos del usuario */
    bloqueados: Usuario[]; /* lista de usuarios bloqueados */

    public constructor() { }

    public static conversor(convertible: any): Usuario {
        const usuario: Usuario = new Usuario();
        usuario.nombreDeUsuario = convertible.nombreDeUsuario;
        usuario.email = convertible.email;
        usuario.password = convertible.password;
        usuario.fechaNacimiento = convertible.password;
        usuario.karma = convertible.karma;
        usuario.amigos = convertible.amigos;
        usuario.bloqueados = convertible.bloqueados;
        console.log('convertido ' + usuario.toString());
        return usuario;
    }

    public toString(): String {
        return 'Usuario{' +
            'nombreDeUsuario=\'' + this.nombreDeUsuario + '\'' +
            ', email=\'' + this.email + '\'' +
            ', password=\'' + this.password + '\'' +
            ', fechaNacimiento=' + this.fechaNacimiento +
            ', karma=' + this.karma +
            ', isAdmin=' + this.isAdmin +
            ', amigos=' + this.amigos +
            ', bloqueados=' + this.bloqueados +
            '}';
    }
}

login component: 
import { Component, OnInit } from '@angular/core';
import { UsuariosService } from '../../servicios/usuarios.service';
import { FormsModule } from '@angular/forms';
import { Usuario } from '../../modelo/usuario';

@Component({
  selector: 'app-login',
  templateUrl: './login.component.html',
  styleUrls: ['./login.component.css']
})
export class LoginComponent implements OnInit {

  servicioUsuarios: UsuariosService;
  nombreDeUsuario: string;
  password: string;
  usuario: Usuario;

  constructor(servicioUsuarios: UsuariosService) {
    this.servicioUsuarios = servicioUsuarios;
  }

  ngOnInit() {
  }

  iniciarSesion() {
    console.log('se ejecuta');
    console.log(this.nombreDeUsuario);
    console.log(this.password);
    this.servicioUsuarios.inicioDeSesion(this.nombreDeUsuario, this.password)
      .subscribe((usuario) => { this.usuario = <Usuario>usuario; console.error(this.usuario);
      },
      (error) => { console.log(error); });
  }

}

and finally user service:
import { Injectable } from '@angular/core';
import { Http } from '@angular/http';
import { catchError, map } from 'rxjs/operators';
import { Usuario } from '../modelo/usuario';

@Injectable()
export class UsuariosService {

  constructor(public http: Http) { }

  getUsuario(nombreDeUsuario: String) {
    let path: String = 'http://localhost:8080/usuarios/';
    path = path.concat(nombreDeUsuario.valueOf());
    return this.http.get(path.valueOf()).pipe(map(resultado => resultado.json() as Usuario));
  }

  inicioDeSesion(nombreDeUsuario: string, password: string) {
    const peticion: Login = {
      nombreDeUsuario: nombreDeUsuario,
      password: password
    };
    console.log(peticion);
    return this.http.post('http://localhost:8080/login', peticion)
      .pipe(map(resultado => {
        if (resultado == null) {
          throw new Error();
        } else {
          Usuario.conversor(resultado.json() as Usuario);
        }
      },
        error => console.log('Error capturado')
      ),
      catchError(err => err)
      );
  }

}

interface Login {
  nombreDeUsuario: string;
  password: string;
}

I've tried:

Passing to login component directly response.json();
Using a static method in User class to make a conversion of any to a User object.
passing response without .json() directly and passing to the property the values directly from the response (but i got many error messages).

I know this code probably smells of bad design so I would appreciate any advice.","Well, a little bit late but i've resolved this. My problem were two problems to be specific. 
1 - I realized i didn't pass an object to the converter static function in the user class, in fact, REST server was sending the object i want to cast inside a ""_body"" field of the response. When i realized it, i changed it so i could retrieve the json object itself. 
2 - Even with the object located. I wasn't really converting it to json and because of that i was getting undefined in all the fields. I was passing some object that was all strings, even in the name of the fields all the names had the same format ""nameOfField"":""valueOfField"" instead of nameOfField:""valueOfField"". I've resolved this with JSON.parse(response._body). Then, the converter method of the User class could retrieve all the fields without problem.
I hope this would be usefull to someone since i didn't get so much help here.",https://stackoverflow.com/questions/51661895/angular-6-post-request-return-correct-object-but-from-service-to-component-it-g,1,1
Staying RESTful while performing AJAX server-side validations?,"My Rails application has a number of forms. When Joe uses my app, I want each form to provide him with immediate visual feedback as to the validity of his input for each field. One field checks the format of his email address - pretty simple. But another allows him to associate the current resource with a number of other resources, and complex business rules apply. If the form is incomplete or invalid, I want to prevent Joe from moving forward by, for example, disabling the 'submit' button.
I could duplicate the validations that appear in my Rails code by writing JavaScript that does the validation in the browser as well. But this smells bad - any time business rules change, I'll need to update them in two places with two different languages and two sets of tests.
Or I could add a single method to the controller for the resource called 'validate'. It would accept form data in an AJAX request, and return a response that could then be used inside Joe's form to provide real-time validation feedback. Unlike the 'create' action, the 'validate' action would not change the state of the server. The only purpose of 'validate' would be to provide a validation response.
The dilemma is that I don't like adding actions to RESTful controllers in Rails. But I like even less the idea of duplicating validation code in two different contexts.
I noticed this SO question, which touches on this subject. But I'm not interested in a plugin or piece of technology. Nor do I consider this question necessarily Rails-specific. I'm more interested in how best to handle this kind of problem in general in a Web application.
I also notice this SO question, which doesn't include the constraint of maintaining a RESTful architecture.
Given the need to dynamically validate form data with complex business rules in a Web application, and the desirability of maintaining a REST-like server architecture, what is the cleanest, most maintainable way to accomplish both at the same time?","I see no problem in creating a validator ""processing resource"" that can accept an entity and ensure that it passes all validation rules.
You could do this either with a global validator
POST /validator

where the validator will have to identify the passed representation and perform the appropriate rules, or you could create subresources,
POST/foo/validator

As long as these urls are discovered via hypermedia and the complete representation to validate is passed as a body of the request, I see no REST constraints being violated.",https://stackoverflow.com/questions/3710262/staying-restful-while-performing-ajax-server-side-validations,3,1
Overwrite ONE column in select statement with a fixed value without listing all columns,"I have a number of tables with a large number of columns (> 100) in a SQL Server database. In some cases when selecting (using views) I need to replace exactly ONE of the columns with a fixed result value instead of the data from the row(s).
Is there a way to use something like 
select table.*, 'value' as Column1 from table

if Column1 is a column name within the table?
Of course I can list all the columns which are expected as result in the select Statement, replacing the one with a value.
However, this is very inconvinient and having 3 or 4 those views I have to maintain them all if columns are added or removed from the table.","Nope, you have to specify columns in this case. 
And you have much more serious problems if tables are being changed often. This may be a signal of large architectural defects. 
Anyway, listing all columns instead of * is a good practice, because if columns number will change, it may cause cascade errors.",https://stackoverflow.com/questions/30647948/overwrite-one-column-in-select-statement-with-a-fixed-value-without-listing-all/30648054,1,1
Maintaining conceptual integrity of the system during Maintenance,"While starting a new project, we kick start it based on what is ""latest"" and what is ""known"". 
This includes selection of programming languages, frameworks in those languages etc. Quite a lot of time is spent on architectural design and detailed level design in terms of using specific frameworks and design patterns etc.
Things go on fine till we complete the development and push things to production.
Then comes maintenance (Defect fixes and Enhancements). People change, architects and designers moved out.
New set of folks who may not have any historical details of project are maintaining it now. They start comprising things on architecture, design principles etc. to provide quick fixes and adding enhancements.
This trend I'm seeing in many projects I've worked.
How to maintain the ""conceptual integrity"" of the system while doing maintenance?","Start with minimal changes. 
Get into the style of the project. 
Create islands of sanity.
Each commit has to improve the state of the code.

See this excellent screencast for an example of what can be done on legacy code without too much disruption.
Of course this requires management's commitment to a code-quality-first strategy to be able to make those fixes ""right"".",https://stackoverflow.com/questions/536944/maintaining-conceptual-integrity-of-the-system-during-maintenance,2,1
Use a particular CA for a SSL connection,"I'm reading through Support Certificates In Your Applications With The .NET Framework 2.0 trying to determine how to set a CA for a SSL connection.
Around half-way down the article under Validating Certificates, MSDN presents some code:
static void ValidateCert(X509Certificate2 cert)
{
    X509Chain chain = new X509Chain();

    // check entire chain for revocation 
    chain.ChainPolicy.RevocationFlag = X509RevocationFlag.EntireChain;

    // check online and offline revocation lists
    chain.ChainPolicy.RevocationMode = X509RevocationMode.Online | 
                                           X509RevocationMode.Offline;

    // timeout for online revocation list 
    chain.ChainPolicy.UrlRetrievalTimeout = new TimeSpan(0, 0, 30);

    // no exceptions, check all properties
    chain.ChainPolicy.VerificationFlags = X509VerificationFlags.NoFlag;

    // modify time of verification
    chain.ChainPolicy.VerificationTime = new DateTime(1999, 1, 1);

    chain.Build(cert);
    if (chain.ChainStatus.Length != 0)
        Console.WriteLine(chain.ChainStatus[0].Status);
    }

Then later:
// override default certificate policy
ServicePointManager.ServerCertificateValidationCallback =
    new RemoteCertificateValidationCallback(VerifyServerCertificate);

I feel like I'm missing something really obvious. For example, I don't want a callback - I just want to say, ""establish a SSL connection, and here's the one CA to trust"".  But I don't see that in the code above.
X509Chain does not appear to have an add method to add a CA or root of trust. Shouldn't the CA be set before the callback? But I don't see that in the code above.
In Java, it would be done with a TrustManager (or TrustManagerFactory) after loading the particular CA you want to use (for an example, see Use PEM Encoded CA Cert on filesystem directly for HTTPS request?).
Question: How does one set a CA to use for an SSL connection in .Net or C#?","The following code will avoid the Windows certificate stores and validate the chain with a CA on the filesystem.
The name of the function does not matter. Below, VerifyServerCertificate is the same callback as RemoteCertificateValidationCallback in SslStream class. It can also be used for the ServerCertificateValidationCallback in ServicePointManager.
static bool VerifyServerCertificate(object sender, X509Certificate certificate,
    X509Chain chain, SslPolicyErrors sslPolicyErrors)
{
    try
    {
        String CA_FILE = ""ca-cert.der"";
        X509Certificate2 ca = new X509Certificate2(CA_FILE);

        X509Chain chain2 = new X509Chain();
        chain2.ChainPolicy.ExtraStore.Add(ca);

        // Check all properties
        chain2.ChainPolicy.VerificationFlags = X509VerificationFlags.NoFlag;

        // This setup does not have revocation information
        chain2.ChainPolicy.RevocationMode = X509RevocationMode.NoCheck;

        // Build the chain
        chain2.Build(new X509Certificate2(certificate));

        // Are there any failures from building the chain?
        if (chain2.ChainStatus.Length == 0)
            return false;

        // If there is a status, verify the status is NoError
        bool result = chain2.ChainStatus[0].Status == X509ChainStatusFlags.NoError;
        Debug.Assert(result == true);

        return result;
    }
    catch (Exception ex)
    {
        Console.WriteLine(ex);
    }

    return false;
}

I have not figured out how to use this chain (chain2 below) by default such that there's no need for the callback. That is, install it on the ssl socket and the connection will ""just work"". And I have not figured out how install it such that its passed into the callback. That is, I have to build the chain for each invocation of the callback. I think these are architectural defects in .Net, but I might be missing something obvious.",https://stackoverflow.com/questions/22516450/use-a-particular-ca-for-a-ssl-connection/23180787,1,1
Use PEM Encoded CA Cert on filesystem directly for HTTPS request?,"This is similar to Import PEM into Java Key Store. But the question's answers use OpenSSL for conversions and tools to import them into key stores on the file system.
I'm trying to use a well formed X509 certificate as a trust anchor:
static String CA_FILE = ""ca-rsa-cert.pem"";

public static void main(String[] args) throws Exception
{
    KeyStore ks = KeyStore.getInstance(""JKS"");
    ks.load(new FileInputStream(CA_FILE), null);

    TrustManagerFactory tmf = TrustManagerFactory
                .getInstance(TrustManagerFactory.getDefaultAlgorithm());
    tmf.init(ks);

    SSLContext context = SSLContext.getInstance(""TLS"");
    context.init(null, tmf.getTrustManagers(), null);

    // Redirected through hosts file
    URL url = new URL(""https://example.com:8443"");

    HttpsURLConnection connection = (HttpsURLConnection) url
            .openConnection();
    connection.setSSLSocketFactory(context.getSocketFactory());

    ...
}

When I attempt to run the program, I get an error:
$ java TestCert 
Exception in thread ""main"" java.io.IOException: Invalid keystore format
    at sun.security.provider.JavaKeyStore.engineLoad(JavaKeyStore.java:650)
    at sun.security.provider.JavaKeyStore$JKS.engineLoad(JavaKeyStore.java:55)
    at java.security.KeyStore.load(KeyStore.java:1214)
    at TestCert.main(TestCert.java:30)

I also tried KeyStore ks = KeyStore.getInstance(""PEM""); and getInstance(""X509"");, but they did not work either.
I know Java supports PEM and DER encoded certificates because that's what a web server sends to a client. But none of the KeyStoreType's seem to match my needs, so I suspect I'm not using the right APIs for this.
The reasons I want to use them directly and not import them into a long-lived KeyStore are:

There are hundreds of PEM certs to test
The certs are on my filesystem
Using certs from the filesystem matches my workflow
I don't want to to use openssl or keytool
I don't want to perform key store maintenance

How does on take a well formed PEM encoded certificate on the filesystem and use it directly?","I found the answer while trying to do this another way at Set certificate for KeyStore.TrustedCertificateEntry?. Its based on Vit Hnilica's answer at loading a certificate from keystore. I""m going to leave the question with this answer since most Stack Overflow answers start with ""convert with openssl, then use keytool ..."".
String CA_FILE = ...;

FileInputStream fis = new FileInputStream(CA_FILE);
X509Certificate ca = (X509Certificate) CertificateFactory.getInstance(
        ""X.509"").generateCertificate(new BufferedInputStream(fis));

KeyStore ks = KeyStore.getInstance(KeyStore.getDefaultType());
ks.load(null, null);
ks.setCertificateEntry(Integer.toString(1), ca);

TrustManagerFactory tmf = TrustManagerFactory
        .getInstance(TrustManagerFactory.getDefaultAlgorithm());
tmf.init(ks);
...",https://stackoverflow.com/questions/22493997/use-pem-encode-ca-cert-on-filesystem-directly-for-https-request,1,1
How to use certificate callback in SslStream.AuthenticateAsClient method?,"My C#.NET SSL connect works when I import the certificate manually in IE (Tools/Internet Options/Content/Certificates), but how can I load the certificate by code?
Here is my code:
TcpClient client = new TcpClient(ConfigManager.SSLSwitchIP, Convert.ToInt32(ConfigManager.SSLSwitchPort));

SslStream sslStream = new SslStream(
                client.GetStream(),
                false,
                new RemoteCertificateValidationCallback(ValidateServerCertificate),
                null
                );
sslStream.AuthenticateAsClient(""Test"");

The above code works fine if i import my certificate file manually in Internet Explorer. But if i remove my certificate from IE and use the following code instead, i get Authentication exception:
sslStream.AuthenticateAsClient(""Test"", GetX509CertificateCollection(), SslProtocols.Default, false);

and here is the 'GetX509CertificateCollection' method :
public static X509CertificateCollection GetX509CertificateCollection()
        {
            X509Certificate2 certificate1 = new X509Certificate2(""c:\\ssl.txt"");
            X509CertificateCollection collection1 = new X509CertificateCollection();
            collection1.Add(certificate1);
            return collection1;
        }

What should I do to load my certificate dynamically?","To build upon owlstead's answer, here's how I use a single CA certificate and a custom chain in the verification callback to avoid Microsoft's store.
I have not figured out how to use this chain (chain2 below) by default such that there's no need for the callback. That is, install it on the ssl socket and the connection will ""just work"". And I have not figured out how install it such that its passed into the callback. That is, I have to build the chain for each invocation of the callback. I think these are architectural defects in .Net, but I might be missing something obvious.
The name of the function does not matter. Below, VerifyServerCertificate is the same callback as RemoteCertificateValidationCallback. You can also use it for the ServerCertificateValidationCallback in ServicePointManager.
static bool VerifyServerCertificate(object sender, X509Certificate certificate,
    X509Chain chain, SslPolicyErrors sslPolicyErrors)
{
    try
    {
        String CA_FILE = ""ca-cert.der"";
        X509Certificate2 ca = new X509Certificate2(CA_FILE);

        X509Chain chain2 = new X509Chain();
        chain2.ChainPolicy.ExtraStore.Add(ca);

        // Check all properties
        chain2.ChainPolicy.VerificationFlags = X509VerificationFlags.NoFlag;

        // This setup does not have revocation information
        chain2.ChainPolicy.RevocationMode = X509RevocationMode.NoCheck;

        // Build the chain
        chain2.Build(new X509Certificate2(certificate));

        // Are there any failures from building the chain?
        if (chain2.ChainStatus.Length == 0)
            return true;

        // If there is a status, verify the status is NoError
        bool result = chain2.ChainStatus[0].Status == X509ChainStatusFlags.NoError;
        Debug.Assert(result == true);

        return result;
    }
    catch (Exception ex)
    {
        Console.WriteLine(ex);
    }

    return false;
}",https://stackoverflow.com/questions/12653911/how-to-use-certificate-callback-in-sslstream-authenticateasclient-method/22702002,3,1
Reducing defect injection rates in large software development projects,"In most software projects, defects originate from requirements, design, coding and defect corrections. From my experience the majority of defects originate from the coding phase.
I am interested in finding out what practical approaches software developers use to reduce defect injection rates.
I have seen the following appraoches used with varying levels of success and associated cost

code inspections
unit tests
static code analysis tools 
use of programming style
peer programming","In my experience it has been the fault of the process, not developers, that permit defects. See They Write the Right Stuff on how the process affects bugs.
Competitive Testing
Software developers should aspire to prevent testers from finding issues with the software they have written. Testers should be rewarded (does not have to be financial) for finding issues with software.
Sign Off
Put a person in charge of the software who has a vested interest in making sure the software is devoid of issues. The software is not shipped until that person is satisfied.
Requirements
Avoid changing requirements. Get time estimates from developers for how long it will take to implement the requirements. If the time does not match the required delivery schedule, do not hire more developers. Instead, eliminate some features.
Task Switching
Allow developers to complete the task they are working on before assigning them to another. After coming back to a new task, much time is spent getting familiar with where the task was abandoned and what remaining items are required to complete the it. Along the way, certain technical details can be missed.
Metrics
Gather as many possible metrics you can. Lines of code per method, per class, dependency relationships, and others.
Standards
Ensure everyone is adhering to corporate standards, including:

Source code formatting. This can be automated, and is not a discussion.
Naming conventions (variables, database entities, URLs, and such). Use tools when possible, and weekly code reviews to enforce.
Code must compile without warnings. Note and review all exceptions.
Consistent (re)use of APIs, both internally and externally developed.

Independent Review
Hire a third-party to perform code reviews.
Competent Programmers
Hire the best programmers you can afford. Let go of the programmers who shirk corporate standards.
Disseminate Information
Hold review sessions where developers can share (with the entire team) their latest changes to the framework(s). Allow them freedom to deprecate old portions of the code in favour of superior methods.
Task Tracking
Have developers log how long (within brackets of 15 minutes) each task has taken them. This is not to be used to measure performance, and must be stressed that it has no relation to review or salary. It is simply a measure of how long it takes certain technical tasks to be implemented. From there you can see, generally, how much time is being spent on different aspects of the system. This will allow you to change focus, if necessary.
Evaluate the Process
If many issues are still finding their way into the software, consider reevaluating the process with which the software is being developed. Metrics will help pinpoint the areas that need to be addressed.",https://stackoverflow.com/questions/1056206/reducing-defect-injection-rates-in-large-software-development-projects/1056271,6,1
Loadrunner replay http response returns different return_uri than when performed manually from the browser,"I have started using HP Loadrunner as part of a performance testing project.
We are trying to hit an initial URL which redirects to an identity server and then redirects to the web portal login screen.
For now I am trying in parallel to see the message exchange in both loadrunner log and browser log (just to compare the two request response messages). For some reason even though the initial HTTP GET message seems to be the same as the browser one the response coming back from the web portal is not the same. So at the location property when i get the rediction url the path is missing the port number at the redirect_uri point (...../url:portnumber2/.....). that creates an issue with the second redirection as instead of getting me to the proper redirection url it takes me to the error page redirection.
so the message exchange in browser looks like below:
   Request URL:https://urladdress:portnumber1/
   Request Method:GET
   Status Code:302 
   Remote Address:10.33.5.83:4020
   Referrer Policy:no-referrer-when-downgrade
   Response Headers
   content-length:0
   date:Fri, 25 Aug 2017 06:50:47 GMT
   location:/connect/authorize?client_id=...&redirect_uri=https%3A%2F%2Furladdress%3Aportnumber%2F....sign........
   server:.......
   set-cookie:.AspNetCore.OpenIdConnect........
   set-cookie:.AspNetCore.........
   status:302
   x-powered-by:ASP.NET
   x-ua-compatible:IE=Edge,chrome=1
   Request Headers
   :authority:urladdress:portnumber1
   :method:GET
   :path:/
   :scheme:https
   accept:text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8
   accept-encoding:gzip, deflate, br
   accept-language:en-US,en;q=0.8
   upgrade-insecure-requests:1
   user-agent:.......


second redirection

    Request URL:https://urladdress:portnumber2/connect/authorize?......
    Request Method:GET
    Status Code:302 
    Remote Address:ipaddress:portnumber2
    Referrer Policy:no-referrer-when-downgrade
    Response Headers
    content-length:0
    date:Fri, 25 Aug 2017 06:50:47 GMT
    location:https://urladdress:portnumber2/account/login?.......
    server:......
    status:302
    x-powered-by:ASP.NET
    Request Headers
    :authority:urladdress:portnumber2
    :method:GET
    :path:/connect/authorize?......
    :scheme:https
    accept:text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8
    accept-encoding:gzip, deflate, br
    accept-language:en-US,en;q=0.8
    cookie:.AspNetCore.......
    upgrade-insecure-requests:1
    user-agent:..........

and the message exchange in loadrunner looks the same with the only difference of not including the portnumber2 after the end of the return_uri property.
That results in being redirected to an error page instead of the second redirection.
The script we are using is shown below:
  web_set_sockets_option(""SSL_VERSION"", ""TLS1.2"");
web_cleanup_cookies();
web_cache_cleanup();
web_add_auto_header(""Accept-Encoding"",""gzip, deflate, br"");
web_add_auto_header(""Accept-Language"",""en-US,en;q=0.8"");
web_add_auto_header(""Accept"",""text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8"");
web_add_auto_header(""Upgrade-Insecure-Requests"",""1"");
web_add_auto_header(""User-Agent"",""Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.101 Safari/537.36"");

web_set_option(""MaxRedirectionDepth"",""0"",LAST);
web_set_max_html_param_len(""10000000"");

web_reg_save_param_ex(""ParamName=url1"",
                      ""LB=Location: "",
                      ""RB=\r\n"",
                      LAST);

web_url(""urladdress:portnumber1"", 
    ""URL=https://urladdress:portnumber1"", 
    ""Resource=0"", 
    ""RecContentType=text/html"", 
    ""Referer="", 
    ""Mode=HTTP"", 
    LAST);

lr_message(""redirected address 1 = %s "" ,lr_eval_string(""{url1}""));

web_reg_save_param_ex(""ParamName=url2"",
                      ""LB=Location: "",
                      ""RB=\r\n"",
                      LAST);
web_url(""connect"",
        ""URL={url1}"",
        ""Resource=0"", 
        ""RecContentType=text/html"", 
        ""Referer=https://urladdress:portnumber1"", 
        ""Mode=HTTP"",
        LAST);

return 0;

I need to know why the port number on a url is not fetched by the http response message( perhaps there is a configuration I need to create). 
Any other comments that might help would be much appreciated.","Have you considered that having a double redirection, particularly ones which are temporary, are going to extract a large performance hit.  You have two double handshakes in a row.  This is bad magic, independent of the testing tool being used.  It is poor architecture on the part of your designers/developers.  Cite the defect - Go for a max redirect level of one.",https://stackoverflow.com/questions/45876228/loadrunner-replay-http-response-returns-different-return-uri-than-when-performed/45911391,1,1
AWS multiple Lambda in same project,"My team is in the process of creating a project which follows the serverless architecture, we are using AWS Lambda, NodeJS and Serverless framework. 
The application will be a set of services each one will be handled as a separate function.
I found examples combining multiple functions under the same project then using cloud formation to deploy all at once, but with some defects we don't want, like having resources of different modules deployed for each lambda function, 
which will cause some redundancy and if we want to change one file it will not be reflected in all lamda functions as it's local to the hosting lamda function 
https://github.com/serverless/examples/tree/master/aws-node-rest-api-with-dynamodb
My question: 
do you know the best way to organize a project containing multiple functions, each one has it's separate .yaml and configurations with the ability to deploy all of them when needed or specify selective updated functions to be deployed?","I think I found a good way to do this in a way like the one mentioned here : https://serverless.readme.io/docs/project-structure 
I created a service containing some Labmda functions , each one is contained within a separate folder , also I had a lib folder on the root level containing all the common modules that can be used in my Lambda functions .
So my Structure looks like : 
Root ---
      functions----
               function1
               function2
      libs---
      tests--
      resources 
serverless.yml (root level)

and in my yml file I'm pointing to Lamdas with relative paths like : 
functions:
  hello1:
    handler: functions/function1/hello1.hello

Now I can deploy all functions with one Serverless command , or selectively deploy the changes function specificity
and the deployed Lamda will only contain the required code",https://stackoverflow.com/questions/48805296/aws-multiple-lambda-in-same-project,1,1
WebSocket authentication security,"I'm trying to authenticate a client to my secure WebSocket server (wss) for registered member area.
Once a member is connected to the web server, I record, in a database, a unique token (associated to the member) that I displayed in a hidden field on the page initiating the connection to the Web Socket server.
Then the token is sent to the WebSocket server that authenticates the account using the token.
I'm really not a security expert, and I wanted your opinion as to the security of my authentication. 
Are there any risks (except cookie hijacking)? Are there any better way to proceed considering that WebSocket doesn't prescribe any particular way that servers can authenticate clients during the WebSocket handshake.
I use Ratchet WebSocket.","Yes, one option is to use cookies (and TLS to avoid cookie hijacking):
Have the cookie set after ""plain old HTML form based"" login, transmit the cookie to WebSocket server, and use the cookie to authenticate the WebSocket.
Here is a complete example of doing Mozilla Persona based authentication with WebSocket.
You asked about Ratchet. This example is not Ratchet, but it might give you some clues - which is why I think it's ok to point to.",https://stackoverflow.com/questions/22593948/websocket-authentication-security/22594987,2,1
UI with microservices,"I have fornt end Web application written in JSF with Richfaces. Its a kind of dashboard application. We are trying to move this in Angular 2 with Spring Boot rest api. 
I want to write microservices where each functionality would be independent. There are total 10 functionality so i will write 10 different rest services and each one would have its own build process. But i am confused with fornt end part. Should i create separate artifacts or separate build for each UI as well ? Or should i bundle in each respective rest api? how should i take care of front end part in microservices?","The UI shouldn't be bundled with the REST services in a true Microservices based architecture. Because if the UI is bundled with APIs, for every defect fix in the UI, all the APIs need to be rebuilt and deployed. The UI needs to be hosted separately. If the team feels comfortable with Spring Boot, the Angular UI can be bundled in a separate Spring Boot application that doesn't have any API.
Update on 21-Mar-2018
I understand that bundling the Angular GUI with Angular Universal in a pm2 server is a better approach.",https://stackoverflow.com/questions/48699765/ui-with-microservices/48704042,2,1
kSecTrustResultRecoverableTrustFailure when connecting to https with self-signed certificate using NSURLConnection,"I've seen here a few questions but none of them helped me. People resolve issues mostly regenerating server certificates: What is the reason of kSecTrustResultRecoverableTrustFailure?
Suppose I need to make a https connection to server with self-signed certificate. I don't have any internal data from the server such as its private keys. For example the server is https://www.pcwebshop.co.uk/
As far as I understand I can bundle a client certificate into app and use it for verification. Am I right I can obtain a valid client certificate without having any internal data from the server?
I've googled a tutorial here http://www.indelible.org/ink/trusted-ssl-certificates
Here's how I'm obtaining a client certificate
openssl s_client \
    -showcerts -connect ""${HOST}:443"" </dev/null 2>/dev/null | \
openssl x509 -outform DER >""../resources/${HOST}.der""

Here's the code (almost unchanged):
- (BOOL)connection:(NSURLConnection *)connection canAuthenticateAgainstProtectionSpace:(NSURLProtectionSpace *)protectionSpace
{
    return [protectionSpace.authenticationMethod isEqualToString:NSURLAuthenticationMethodServerTrust];
}

- (void)connection:(NSURLConnection *)connection didReceiveAuthenticationChallenge:(NSURLAuthenticationChallenge *)challenge
{
    if ([self shouldTrustProtectionSpace:challenge.protectionSpace]) {
        [challenge.sender useCredential:[NSURLCredential credentialForTrust:challenge.protectionSpace.serverTrust]
             forAuthenticationChallenge:challenge];
    } else {
        [challenge.sender performDefaultHandlingForAuthenticationChallenge:challenge];
    }
}

- (BOOL)shouldTrustProtectionSpace:(NSURLProtectionSpace *)protectionSpace
{
    // load up the bundled certificate
    NSString *certPath = [[NSBundle mainBundle] pathForResource:protectionSpace.host ofType:@""der""];

    if (certPath == nil)
        return NO;

    OSStatus status;
    NSData *certData = [[NSData alloc] initWithContentsOfFile:certPath];
    CFDataRef certDataRef = (__bridge_retained CFDataRef)certData;
    SecCertificateRef cert = SecCertificateCreateWithData(NULL, certDataRef);

    // establish a chain of trust anchored on our bundled certificate
    CFArrayRef certArrayRef = CFArrayCreate(NULL, (void *)&cert, 1, NULL);
    SecTrustRef serverTrust = protectionSpace.serverTrust;
    status = SecTrustSetAnchorCertificates(serverTrust, certArrayRef);
    // status == 0

    // verify that trust
    SecTrustResultType trustResult;
    status = SecTrustEvaluate(serverTrust, &trustResult);
    // status == 0

    CFRelease(certArrayRef);
    CFRelease(cert);
    CFRelease(certDataRef);

    return trustResult == kSecTrustResultUnspecified;
}

trustResult is always kSecTrustResultRecoverableTrustFailure.
What am I doing wrong? Thank you.
UPDATE: Ok, I found out that the reason is ""Server's certificate does not match the URL"".
Is it possible to fix the issue from the client side by ignoring the URL (hostname) of the server's certificate?","Suppose I need to make a https connection to server with self-signed certificate. I don't have any internal data from the server such as its private keys. 

In this case, you need a security diversification strategy. Gutmann covers it in great detail in his book Engineering Security.
The short of it: validate the certificate sensibly the first time you encounter it. You can still use most of the traditional PKI/PKIX tests. Once the certificate passes all tests (other than the ""trusted root path""), you then call it ""Trusted"". This strategy is called Trust On First Use or TOFU.
In subsequent connections, you don't need TOFU again because you already encountered the certificate or public key. In the subsequent connections, you ensure the certificate or public key is continuous (i.e., does not change), the IP is from the same area as previously encountered, etc. If the certificate changes, then be sure its because the self signed is expiring. Be wary of unexpected changes.


Here's the code (almost unchanged):
...
trustResult == trustResult == kSecTrustResultUnspecified


For kSecTrustResultUnspecified, see Technical Q&A QA1360. Essentially, its a recoverable error. The Q&A says to prompt the user. Gutmann (and I) say to use a security diversification strategy as described above.
You need to take the user out of the loop because they will always make a decision that gets them past the Message Boxes as quickly as possibly. It dies not matter if they answer right or wrong - they want to see the dancing bunnies.
Also, the security diversification strategy even applies to kSecTrustResultProceed. Consider: Both Diginotar and Trustwave broke PKI{X}, and Cocoa/CocoaTouch was more than happy to return kSecTrustResultProceed. Its not really Cocoa/CocoaTouch's fault - there are architectural defects incumbent to PKI{X}.


Is it possible to fix the issue from the client side by ignoring the URL (hostname) of the server's certificate?

That kind of defeats the purpose of PKI{X}. If you will accept any host, any public key or any signature, why even bother with PKI{X} in the first place? The whole point of X509 in PKI{X} is to bind a entity or host to a public key using a trusted third party signature (or self signed, in this case).
If you don't care about the binding, just use Anonymous Diffie-Hellman and put an end to the security theater.",https://stackoverflow.com/questions/24229926/ksectrustresultrecoverabletrustfailure-when-connecting-to-https-with-self-signed/24234862,1,1
What is the reason of kSecTrustResultRecoverableTrustFailure?,"I'd like to validate my ssl server certificates with some extra checks. And sometimes I get a 
kSecTrustResultRecoverableTrustFailure 

instead of 
kSecTrustResultProceed or kSecTrustResultUnspecified
It seems to happen if

the certificate is md5 hashed (IOS5)
the server does not present the root and intermediate certificates
the SecTrustSetAnchorCertificatesOnly(trust,YES) 
is set and the anchor certificate is only in the built in anchor certificates
the certificate is expired
?

It depends on the AppleX509TP policy used to evaluate the trust.
My problem is I do not want to trust if the chain fails, but I want to trust if MD5 is used.
Is there a way to find out why the evaluation failed?
As an alternative is there a way to extract the CSSM_ALGID_MD5 from a SecCertificateRef?","It may be a server certificate problem....
Check here, I solved my kSecTrustResultRecoverableTrustFailure problem, adding subjectAltName = DNS:example.com into openssl config file, specifically in server key generation...
If you are not using openssl to generate it, I'm sorry but I can help you.. Anyway if you want to use openssl, here is a good tutorial to generate those keys and sign then with your own root certificate authority.
From this tutorial, I just changed my openssl server config file to:
    
    [ server ]
    basicConstraints = critical,CA:FALSE
    keyUsage = digitalSignature, keyEncipherment, dataEncipherment
    extendedKeyUsage = serverAuth
    nsCertType = server
    subjectAltName = IP:10.0.1.5,DNS:office.totendev.com
    
Hope it helps !
EDITED:
My Server evaluation code:
#pragma mark - SERVER Auth Helper
//Validate server certificate with challenge
+ (BOOL)validateServerWithChallenge:(NSURLAuthenticationChallenge *)challenge {
//Get server trust management object a set anchor objects to validate it
SecTrustSetAnchorCertificates([challenge.protectionSpace serverTrust], (__bridge CFArrayRef)[self allowedCAcertificates]);
//Set to server trust management object to JUST ALLOW those anchor objects assigned to it (ABOVE), and disable apple CA trusts 
SecTrustSetAnchorCertificatesOnly([challenge.protectionSpace serverTrust], YES);
//Try to evalute it
SecTrustResultType evaluateResult = kSecTrustResultInvalid; //evaluate result
OSStatus sanityCheck = SecTrustEvaluate([challenge.protectionSpace serverTrust], &evaluateResult);
//Check for no evaluate error
if (sanityCheck == noErr) {
    //Check for result
    if ([[self class] validateTrustResult:evaluateResult]) { return YES ; }
}
//deny!
return NO ;
}
//Validate SecTrustResulType
+ (BOOL)validateTrustResult:(SecTrustResultType)result {
switch (result) {
    case kSecTrustResultProceed: { TDLog(kLogLevelHandshake,nil,@""kSecTrustResultProceed""); return YES ; }
        break;
    case kSecTrustResultConfirm: { TDLog(kLogLevelHandshake,nil,@""kSecTrustResultConfirm""); return YES ; }
        break;
    case kSecTrustResultUnspecified: { TDLog(kLogLevelHandshake,nil,@""kSecTrustResultUnspecified""); return YES ; }
        break;
    case kSecTrustResultDeny: { TDLog(kLogLevelHandshake,nil,@""kSecTrustResultDeny""); return YES ; }
        break;
    case kSecTrustResultFatalTrustFailure: { TDLog(kLogLevelHandshake,nil,@""kSecTrustResultFatalTrustFailure""); return NO ; }
        break;
    case kSecTrustResultInvalid: { TDLog(kLogLevelHandshake,nil,@""kSecTrustResultInvalid""); return NO ; }
        break;
    case kSecTrustResultOtherError: { TDLog(kLogLevelHandshake,nil,@""kSecTrustResultOtherError""); return NO ; }
        break;
    case kSecTrustResultRecoverableTrustFailure: { TDLog(kLogLevelHandshake,nil,@""kSecTrustResultRecoverableTrustFailure""); return NO ; }
        break;
    default: { TDLog(kLogLevelHandshake,nil,@""unkown certificate evaluate result type! denying...""); return NO ; }
        break;
}

}

Hope now it helps :) !",https://stackoverflow.com/questions/7715426/what-is-the-reason-of-ksectrustresultrecoverabletrustfailure/8937798,1,1
Is it enough to launch Sonar only once a day or week?,"I have been digging quite a bit on Sonar best practices. The consensus seems to be that Sonar would be launched only once a day or week, e.g. during the night. However, what if one uses a CI server such as Jenkins? Jenkins builds on every SVN commit, runs unit tests, deploys to staging environment, runs Selenium tests, etc. The way I understand it, all this additional information is lost if Sonar is launched only once a day/week. Most likely all the team's code problems and failing tests have been resolved on in the afternoon or end of the week. Sonar runs probably on Sunday night or every night. The application is pre-built and tested, and then Sonar analysis is executed based on that information. Most likely all tests pass, no major code problems are left in the repository, and the QA team incorrectly believes that there are no problems since all Sonar reports display green. However, during the day/week it is possible that the project has been a total mess with broken builds etc., but is never displayed in Sonar reports :)
Am I missing something here, or should Sonar actually be executed upon every commit, or at least once an hour?","It all dependes on your need and your team velocity to develop new code, test and integrate new functionalities into the project.
If you have a sprint with a time box of a web, probably that on the weekend, the version that started on monday is stable, and therefore bug free, or only a few bugs. If your sprint time box is a week, i would higly recommend at least once a day, so you can get defects running unit tests and so on, giving a good reality on your project quality.
I would recommend these practices:

Using code coverage like Cobertura, running unit tests at least once a day;
Using code analysis like PMD, checkstyle and so on. If possible, create your own rules that applies to your architecture, generating more value for your project in terms of quality.
Considering your time box for a Sprint, choose a frequency for your builds in sonar and schedule a cron job that runs it only when you really need it(since its time consuming).

These are practices I use in my projects, but you have to look closely to your needs, since the tool (Sonar) is to help you with information about the quality of your project, and therefore, your architecture, your team and your engineering practices.",https://stackoverflow.com/questions/12536199/is-it-enough-to-launch-sonar-only-once-a-day-or-week/12536407,3,1
Restrict violation of architecture - asp.net MVP,"If we had a defined hierarchy in an application. For ex a 3 - tier architecture, how do we restrict subsequent developers from violating the norms?
For ex, in case of MVP (not asp.net MVC) architecture, the presenter should always bind the model and view. This helps in writing proper unit test programs. However, we had instances where people directly imported the model in view and called the functions violating the norms and hence the test cases couldn't be written properly.
Is there a way we can restrict which classes are allowed to inherit from a set of classes? I am looking at various possibilities, including adopting a different design pattern, however a new approach should be worth the code change involved.","I'm afraid this is not possible. We tried to achieve this with the help of attributes and we didn't succeed. You may want to refer to my past post on SO.
The best you can do is keep checking your assemblies with NDepend. NDepend shows you dependancy diagram of assemblies in your project and you can immediately track the violations and take actions reactively.

(source: ndepend.com)",https://stackoverflow.com/questions/2718864/restrict-violation-of-architecture-asp-net-mvp,4,1
Restrict custom attribute so that it can be applied only to Specific types in C#?,"I have a custom attribute which is applied to class properties and the class itself. Now all the classes that must apply my custom attribute are derived from a single base class.
How can I restrict my Custom Attribute so that it can be applied to only those classes, that must derive from my base class? How do I do this?","Darn, I hate it when I prove myself wrong... it works if you define the attribute as a protected nested type of the base-class:
abstract class MyBase {
    [AttributeUsage(AttributeTargets.Property)]
    protected sealed class SpecialAttribute : Attribute {}
}

class ShouldBeValid : MyBase {
    [Special]
    public int Foo { get; set; }
}
class ShouldBeInvalid {
    [Special] // type or namespace not found
    [MyBase.Special] // inaccessible due to protection level
    public int Bar{ get; set; }
}


(original answer)
You cannot do this - at least, not at compile time.
Attributes can be restricted (for exmaple) to ""class"", ""struct"", ""method"", ""field"", etc - but nothing more granular.
Of course, since attributes do nothing by themselves (ignoring PostSharp), they will be inert on other types (and inert on your types unless you add some code to look at the attributes at runtime).
You could write an FxCop rule, but that seems overkill.
I wonder, however, whether an attribute is the best choice:

Now all the classes that must apply my custom attribute are derived from a single base class.

If they must apply your custom attribute, maybe an abstract (or maybe just virtual) property / method would be a better choice?
abstract class MyBase {
    protected abstract string GetSomeMagicValue {get;}
}
class MyActualType : MyBase {
    protected override string GetSomeMagicValue {get {return ""Foo"";}}
}",https://stackoverflow.com/questions/1021190/restrict-custom-attribute-so-that-it-can-be-applied-only-to-specifc-types-in-c,2,1
"Custom HTTP Header, does it violate the RESTful Architecture","I'm currently working on designing a RESTful API, my question is, does it violate the RESTful Architecture if adding custom HTTP Header as part of the requirements?
X-API-Token: MyToken
Appreciate the help, thank you!
Erson","As long as you do not store client context as part your custom header, you're good to go.",https://stackoverflow.com/questions/42522059/custom-http-header-does-it-violate-the-restful-architecture,1,1
Does writing code in viewDidLoad Violates MVC architecture?,"I was working in a client project. I have written lot many view customisation code inside ViewDidLoad. I have models for data store and access.
The project was working fine. They hired a new iOS developer he said the code is not compliant with MVC architecture. The asked the reason why? He said the views are created inside viewDidLoad which is a controller of the Class hence it is not acceptable code. What should we do when its really dynamic views and can not be created using storyboard.","My answer is No
From apple doc

Controller Objects
A controller object acts as an intermediary between one or more of an application__ view objects and one or more of its model objects. Controller objects are thus a conduit through which view objects learn about changes in model objects and vice versa. Controller objects can also perform setup and coordinating tasks for an application and manage the life cycles of other objects.

I think that the controller has the responsibility to manage what the view look like.
In viewDidLoad,it is good to write one-time view customisation code here.
But if you write a lot configuration code to a view. I think it is better to use a subclass of UIView. This makes your code clear and easy to debug and maintain.",https://stackoverflow.com/questions/30367431/does-writing-code-in-viewdidload-violates-mvc-architecture,3,1
Do AOP violate layered architecture for enterprise apps?,"The question(as stated in the title) comes to me as recently i was looking at Spring MVC 3.1 with annotation support and also considering DDD for an upcoming project. In the new Spring any POJO with its business methods can be annotated to act as controller, all the concerns that i would have addressed within a Controller class can be expressed exclusively through the annotations.
So,  technically i can take any class and wire it to act as controller , the java code is free from any controller specific code, hence the java code could deal with things like checking security , starting txn etc. So will such a class belong to Presentation or Application layer ??
Taking that argument even further , we can pull out things like security, txn mgmt and express them through annotations , thus the java code is now that of the domain object. Will that mean we have fused together the 2 layers? Please clarify","You can't take any POJO and make it a controller. The controller's job is get inputs from the browser, call services, prepare the model for the view, and return the view to dispatch to. It's still a controller. Instead of configuring it through XML and method overrides, you configure it through annotations, that's all.
The code is very far from being free from any controller specific code. It still uses ModelAndView, BindingResult, etc.",https://stackoverflow.com/questions/10928056/do-aop-violate-layered-architecture-for-enterprise-apps,2,1
DB design for microservice architecture [closed],"I am planning to use the Microservices architecture for the implementation of our website. I wanted to know if it is right to share databases between services or if it is preferable to have a separate database for each service. In this regard, can I consider having one common database for all services or does it violate the very essence of Microservice architecture ?","Microservices offers decoupling. You must break down your application into independent domains. Each domain can have a DB. In case other MS needs to access data owned by some other microservices, they have to communicate over the network.
In case you feel that there are too many dependent services and the network calls would be too much, then you can define a domain, clustering the dependent services together.
For instance -- Suppose I have an online Test evaluation Service where a manager of a company can post tests and he can view results of all the employees in his department.
My Microservices for this scenario would be:
Initial Design 

User Service: For login and user information.
Test Service: Service to evaluate tests.
Employee: Handles employee details
Company: Handles organization CRUD
Department: Handles department CRUD

After breaking it down, seems like employee, Organization and Department service would be making too much network/API calls as they are tightly dependent on each other. So it's better to cluster them.
Updated design

User Service : For login and user information.
Test Service : Service to evaluate tests
Organization : Handles Company, Employee and Department related operations.

Each service could have it's own DB and it's independently deployable. User and Test Service can use mongoDB or any NoSql DB and Organization service can use RDBMS.
Hope this helps.",https://stackoverflow.com/questions/43426699/db-design-for-microservice-architecture,2,1
Lattix architecture analysis tool,"I came across Lattix for analysing the architecture by investigating the code. It supports C/C++, Java, .Net, etc. One can define certain rules to maintain the defined architecture. The tool can flag any violations at build time.
Has anybody used this or any other similar tool? I see some benefit of this tool in development but not sure if this kind of tool is must to have?","We use both Lattix and NDepends to track dependencies of our assemblies. 
Both tools support static visualizing dependencies between assemblies and classes through a dependency structure matrix (DSM). A DSM gives you the ability to show the architecture of your application. For example if you use layering this should be visible in the DSM. Cyclic dependencies will also be visible in a DSM.    
A nice practical introduction can be found in OOPSLA05-dsm.pdf 
NDepends specifically targets .Net and has Visual Studio Integration. Lattix is also able to create DSM's for other languages. Both support build integration which allows you to create rules in your build to prevent incorrect dependencies.
Just try both tools and see if the generated DSM is what you expected. Check circular dependencies and see if there are invalid dependencies. For example, a user interface which directly uses the data access layer instead of using the business layer.",https://stackoverflow.com/questions/797303/lattix-architecture-analysis-tool,4,1
Entity Framework and 3 layer architecture,"I have a three layer architecture program. The questions are:
1. Data access is the layer of EF?
2. If i want to use an entity generated by EF from Presentation Layer, then i reference the Data Access, but this violates the principles of 3 layered architecture.","Yes EF would be your Data Access Layer.
With EF you can use T4 templates with POCO support, you can then extract these POCO into a seperate dll and this will be reference from all of your layers.",https://stackoverflow.com/questions/6587531/entity-framework-and-3-layer-architecture,5,1
Are django Template tags part of View in MVC architecture?,"according to definition of MVC architecture and responsibilities of View Layer can we say Django template tags are part of View Layer ?
if it is true so we should not access our data model inside of template tags and so this answer violate MVC architecture rules ?","In a sense I would say yes. 
Since your code logic (model) and your view (template) logic are separate it would be very similar, you can debate that URL.py would act as your controller ( the router )
I would check out the below to get a better understanding it's much more detailed. 
Similar Question and Explanation. 
Django vs. Model View Controller",https://stackoverflow.com/questions/24965540/are-django-template-tags-part-of-view-in-mvc-architecture,3,1
"In the flux architecture, who is responsible for sending updates to the server?","So in the flux architecture, data flows as follows:
View -> Action -> Dispatcher -> Store
 ^ <-----------------------------|

So let's say the view is a comment box. When the user submits a comment, an addComment action is triggered, but where should that comment be sent to the server? Should it happen in the action function, before dispatching it, or should the store doing it when receiving the action from the dispatcher?
Both cases seam like a violation of the single responsibility pattern. Or should there be two CommentStores, one LocalCommentStore and a ServerCommentStore that both handle the addComment action?","In your case Action is responsible for both sending a pending action or optimistic update to the store and sending a call to the WebAPI:
View -> Action -> Pending Action or optimistic update  -> Dispatcher -> Store -> emitEvent -> View 
               -> WebUtils.callAPI()

onWebAPISuccess -> Dispatcher -> Store -> emitEvent -> View
onWebAPIFail -> Dispatcher -> Store -> emitEvent -> View",https://stackoverflow.com/questions/26816906/in-the-flux-architecture-who-is-responsible-for-sending-updates-to-the-server,2,1
API Gateway handling Webservices in a Microservice Architecture,"I have an architectural question. We are transforming an old Monolith to a Microservice Architecture. Therefore we have in plan to identify the bounded contexts and make Microservices out of these. 
To keep up with our public API we will have an API Gateway which routes the stuff properly. The internal communication will be done via REST (at the first shot). Unfortunatelly our existing public API is about WebServices most of the time.
If we do transformation from Webservices to REST communication we already need to know stuff of the Domain Objects. Isn't that already a violation of the Microservice Design. In the end that means adding a field in the Microservice A implies also touching the API Gateway. Which I do not like.
Am I wrong here? What is your opinion on this?","Don't see any violations here if you are not going to use your domain entities as input parameters to your internal REST services. Use plain old DTO objects as input parameter and then map them to your domain objects.
Also I wouldn't go with API Gateway solution if I were you. I understand you are trying to make your changes transparent for your API clients but the API Gateway adds a redundant step and it might cause performance problems.
I suggest doing the following:

Extract domain logic into reusable libraries, so they can be used by both old and new API.
Build a new version of your API using the libs from item #1
Make sure all your new clients are using the new API and promote it among the old ones

Yes, it won't be easy to support both API's for some time but you will get rid of that API Gateway in the long run.",https://stackoverflow.com/questions/40447582/api-gateway-handling-webservices-in-a-microservice-architecture,2,1
Calculating Design Violations Technical Debt with SonarQube,"I was wondering if it is possible with the latest version of Sonar or through some  plugin calculate the techical debt of an architectural violation (from a java project) like: ""'X' class should be in com.domain.classes package"" or ""the class 'X' must extend the class 'Y' ""? Or ""The 'X' class must have a 'public static Y someAttribute' attribute""?
If there is no plugin for this, there is how to develop a plugin to automate this? I have read the documentation for the Java API, REST, how to develop a plugin and tried to code it, but have not found a way to do it.","On possible route is to use the PMD code checker. You can add your own custom rules to PMD, and there is a PMD plugin for Sonar.


But in that case, Sonar can calculate the debt (through SQALE or another plugin) for me? In another case, how i can calculate the debt?

I think you misunderstand the ""technical debt"" term.  Here is how Wikipedia describes it:

Technical debt (also known as design debt[citation needed] or code debt) is a neologistic metaphor referring to the eventual consequences of poor software architecture and software development within a codebase. The debt can be thought of as work that needs to be done before a particular job can be considered complete. If the debt is not repaid, then it will keep on accumulating interest, making it hard to implement changes later on. Unaddressed technical debt increases software entropy.

The key thing to note is that technical debt is a metaphor.
So, for example, PMD could tell you how many times your code violates certain rules, but it cannot tell you how much work it would be to fix them, or how much extra work in the future would be incurred because you didn't fix it now.
Basically, you cannot measure technical debt because a true measure of technical debt requires knowing what is going to happen in the future.  Also read this: https://softwareengineering.stackexchange.com/questions/167080/how-is-technical-debt-best-measured-what-metrics-are-most-useful",https://stackoverflow.com/questions/21906671/calculating-design-violations-technical-debt-with-sonarqube,2,1
Violation of single responsibility principle in Iterator from Java core,"Why java.util.Iterator interface has method remove()? 
Certainly sometimes this method is necessary and all have become accustomed to its presence. But in fact the main and only objective of the iterator is just to provide access container elements. And when someone wants to create his own implementation for this interface, and cannot or does not want for any reason to provide the ability to remove an element, then he is forced to throw UnsupportedOperationException. And throwing of that exception usually indicates a not too well thought out architecture or some flaws in design.
Really I don't understand the reasons for such a decision. And I guess it would be more correctly separate a specific subinterface to support the optional method:

Any reasoned versions why remove() is part of the Iterator? Is not this example of a direct violation of the single responsibility principle from SOLID?","In addition to fancy technical answers ... please consider the timeline too. The ""single responsibility principle"" was coined by Robert Martin at some point in the middle/late 90es. 
The Java iterator interface came into existence with Java 1.2; so around 1998.
It is very much possible that the folks at Sun had never heard of this concept while working on the early releases of Java.
Of course, many smart people have the same ideas without reading a book about it ... so a good designer might have implemented ""SRP"" like without knowing about ""SRP"" - but it also requires a high degree of awareness to unveil all the big and small violations of this rule ...",https://stackoverflow.com/questions/33160222/violation-of-single-responsibility-principle-in-iterator-from-java-core,3,1
Single page Web App in Java framework or examples? [closed],"Has anyone seen an example or done the following in Java:
http://duganchen.ca/single-page-web-app-architecture-done-right/
That is a design a single page web app that will work with Google SEO with out massive violation of DRY using Java technologies?
It doesn't seem terrible hard to do this on my own but I was curious (and lazy) to see if someone had already done it with either Spring or JAX-RS.","I have built quite a large ""single-page"" javascript website, that generats all HTML on the client. Server provides JSON only responses. I used Google Closure tools for the following reasons:

Google Closure Templates allows designing templates in high level templating language (named soy) which is compiled either to pure javascript functions to run on the client or java code to run on the server site.
Google Closure Compiler, which allows separating javascript code to modules and provides autmatic dependency injection for uncompiled mode. Good program structure and modularisation is necessary for any project exceeding simple html decoration. This is hard to achieve with frameworks like jQuery or dojo. In advanced compiled mode it transforms your javascript to shorter an more efficient equivalent, eliminates dead code and do dramatic reduction in size, which can shrink the original codebase to few % of the original size.
Google Stylesheets is meta css language which works great with closure compiler.
Google Closure Library is huge and well tested javascript library and with closure compiler, you only take what is needed.

To streamline the development, I'm using plovr, written by Michale Bolin, a former googler, one of the members of the original Closure Compiler Team.
I can recommend reading Michale's book: Closure, the Definitive Guide.
I must but warn, the initial leraning curve might be quite steep, but it is well worth the pain. Google used this tools to write almost all their web projects.
Just one more thing
If you feel really adventurous, and want to peep in to the future, I recomend upgrading the former strategy with Clojure/ClojureScript. For the start, watch this very persuasive talk of Rich Hickey and make sure to check Clojurescript one project.",https://stackoverflow.com/questions/9773758/single-page-web-app-in-java-framework-or-examples,3,1
"Am I violating SOLID principles and n-layer, micro service architectures?","In the following example, the AccountService and ProductService are in an ASP.NET MVC app. The AccountWebAPI and ProductWebAPI are externally hosted API micro services.
1) Can I eliminate the ProductService and orchestrate the retrieving of the orders in the CustomerAccountController itself? This is because I consider the Controller as the Application layer/service which is mentioned in the DDD (Domain Driven Design).
2) Am I violating the n-layer architecture because the ProductService calls the AccountService which is the same layer?
3) Since AccountWebAPI and ProductWebAPI are micro services, do they have to be separated as AccountService and ProductService in the client application (MVC App) also to keep the Separation Of Responsibility? So the ProductService needs to be renamed as ProductAppService and ProductService should interact with ProductWebAPI only like AccountService talks to AccountWebAPI.
public class CustomerAccountController : Controller 
{ 
    IProductService _productService;

    public CustomerAccountController(IProductService productService)
    {
        _productService = productService;
    }

    public IActionResult Index()
    {
        return View();
    }

    public IActionResult Account(int customerId)
    {
        var orders = _productService.GetOrders(customerId);

        return View(orders);
    }
}

public class ProductService 
{ 
    IAccountService _accountService; 
    IProductWebAPI _productWebAPI;

    ProductService(IAccountService accountService, IProductWebAPI productWebAPI)
    {
        _accountService = accountService;
        _productWebAPI = productWebAPI;
    }

    IList<Order> GetOrders(int customerId)
    {
        // Find the International Customer Number for CustomerId
        object customer = _accountService.GetInternationCustomerInfo(customerId);

        // Convert the string type to int
        var modifiedCustomerNumber = Convert.ToInt32(customer.Id);

        // Get the orders
        return _productWebAPI.GetOrders(modifiedCustomerNumber);
    }
}

public class AccountService 
{ 
    IAccountWebService _accountWebAPI;

    CustomerService(IAccountWebService accountWebAPI)
    {
        _accountWebAPI = accountWebAPI;
    }

   object GetInternationCustomerInfo(int customerId) 
   { 
        return accountWebAPI.GetCustomer(customerId) 
   } 
}

UPDATE: I realized that OrderService would be the appropriate service name for orders and not ProductService.
The LAYERS:
VIEW -- CONTROLLER -- SERVICE -- WebAPIs -- DOMAIN -- REPOSITORY
OrderView -- CustomerAccountController -- ProductService (calls AccountService in the same layer) -- ProductWebAPI -- ProductDomain -- ProductRepository","The names AccountService and ProductService imply that you are violating the Single Responsibility Principle, Open Closed Principle and Interface Segregation Principle, which is 60% of the SOLID principles.
The reasoning for this is explained in this article, but in short:

The Single Responsibility Principle is violated, because the methods in each class are not highly cohesive. The only thing that relates those methods is the fact that they belong to the same concept or entity.
The design violates the Open/Closed Principle, because almost every time [a method] is added to the system, an existing interface and its implementations need to be changed. Every interface has at least two implementations: one real implementation and one test implementation.
The Interface Segregation Principle is violated, because the interfaces [such as IProductService] are wide (have many methods) and consumers of those interfaces are forced to depend on methods that they don__ use. 

The solution is to give each use case its own class. This design is explained in detail here and here.
I would even say that having Web API controllers with the same structure leads to the same kind of SOLID violation. In fact, if you apply the design given by the articles, you can completely remove all your Web API controllers, and replace them with a single piece of infrastructure logic that will be able to pass messages around. Such design is described here (the article mainly talks about WCF, but its applicable to Web API as well and a working example of Web API can be seen in the example project that the article links to).",https://stackoverflow.com/questions/33452380/am-i-violating-solid-principles-and-n-layer-micro-service-architectures,2,1
Sharing code and schema between microservices,"If you go for a microservices architecture in your organization, they can share configuration via zookeeper or its equivalent. However, how should the various services share a common db schema? common constants? and common utilities?
One way would be to place all the microservices in the same code repository, but that would contradict the decoupling that comes with microservices...
Another way would be to have each microservice be completely independent, however that would cause code duplication and data duplication in the separate databases each microservice would have to hold.
Yet another way would be to implement functional microservices with no context\state, but that's usually not realistic and would push the architecture to having a central hub that maintains the context\state and a lot of traffic from\to it.
What would be a scalable, efficient practical and hopefully beautiful way to share code and schema between microservices?","Regarding common code, the best practice it to use a packaging system. So if you use Java, then use maven, if you use Ruby then Gems, if python then pypi etc.
Ideally a packaging system adds little friction so you may have a (say, git) repository for a common lib (or several common-libs for different topics) and publish their artifacts through an artifact repository (e.g. private maven/gems/pypi). Then at the microservice you add dependency on the required libs.So code reuse is easy.
In some cases packaging systems do add some friction (maven for one) so one might prefer using a single git repo for everything and a multi-module project setup. That isn't as clean as the first approach but works as well and not too bad.
Other options are to use git submodule (less desired) or git subtree (better) in order to include the source code in a single ""parent"" repository.
Regarding schema - if you want to play by the book, then each microservice has its own database. They don't touch each other's data. This is a very modular approach which at first seems to add some friction to your process but eventually I think you'll thank me. It will allow fast iteration over your microservices, for example you might want to replace one database implementation with another database implementation for one specific service. Imagine doing this when all your services use the same database! Good luck with that... But if each single service uses it's own database the service abstracts the database correctly (e.g. it does not accept SQL queries as API calls for example ;-)) then changing mysql to Cassandra suddenly become feasible.
There are other upsides to having completely isolated databases, for example load and scaling, finding out bottlenecks, management etc.
So in short - common code (utilities, constants etc) - use a packaging system or some source code linkage such as git-tree
Database - you don't touch mine, I don't touch yours. That's the better way around this.
HTH, Ran.",https://stackoverflow.com/questions/25600580/sharing-code-and-schema-between-microservices,4,1
VS2010 Validate Architecture combines all layer diagrams?,"This is my first attempt at seriously using the Visual Studio Modelling Project type, and the Layer Diagram in particular.
I would like to use the diagram to validate (the lack of) some dependencies in my architecture. I've created a Layer Diagram of how I thought things should look like and used the Validate Architecture feature. It came up with several violations, most of which were known by me already.
I added the dependencies that shouldn't be there to document the current situation: validation passed as expected.
Now I've created a copy of that diagram in the same project, where I intended to draw out the desired new structure. I removed the dependencies and right clicked the diagram to ""Validate Architecture"" again. To my surprise no violations were reported. Then I removed the dependencies in the original copy and validated my new copy again: the violations were back.
It appears, that Visual Studio uses all the layer diagrams in the project together, to come with a complete picture of the dependencies. Is my assumption correct? Is there some way to have diagrams considered seperately, without creating a second modelling project?","It appears this is how the modelling project is intended.
When validating architecture using the command line, the name of the model is not part of the syntax. Only the name of the project is provided, so it makes sense that all the diagrams are taken into account.
As a workaround, I have created a second modelling project. One contains all the diagrams of the current situation, and one contains several diagrams of how things should be.",https://stackoverflow.com/questions/6227120/vs2010-validate-architecture-combines-all-layer-diagrams,1,1
Is calling service from controller a violation of MVC?,"I have seen many code references online they use services directly in the Controller.
public class PersonController : Controller
{
   public ActionResult Index(int personId)
   {
       Person personDataModel = null;
       using(var service = new Personservice())
       {
          personDataModel = service.GetPerson(personId);
       }

       var personVM = MapPersonDataModelToViewModel(personDataModel);

       return View(""Index"",personVM);
   }
}

As per MSDN, Controllers are the components that handle user interaction, work with the model, and ultimately select a view to render that displays UI.
Where is the mention of Service? 
I know people talking about Service layer pattern or SOA architecture. But still is this a violation. A violation for convenience?
For some reason, if I want to do away with services. Then I'll end up changing the controller. And have to build my model. Instead I'm suppose to change only the Model.
I feel Model should have the intelligence to serve the request from Controller either by calling external service or by itself.
public class PersonController : Controller
{
   public ActionResult Index(int personId)
   {
       var personVM = PersonModel.GetPerson(personId);

       return View(""Index"",personVM);
   }
}

public class PersonModel
{
   public PersonVM GetPerson(int personId)
   {
      Person personDataModel = null;
       //Can either do this
       using(var service = new Personservice())
       {
          personDataModel = service.GetPerson(personId);
       }

       //Or can do this
       personDataModel = GetPersonDataModel(personId);

      var personVM = MapPersonDataModelToViewModel(personDataModel);

      return personVM;
   }
}

I know PersonModel needs re-factoring but that's not the point.
UPDATE:
When I mean Model, I'm not referring to MODEL(class with properties) that we pass to View. I mean the classes in the folder Model.
As per MSDN, Models: Model objects are the parts of the application that implement the logic for the application's data domain. Often, model objects retrieve and store model state in a database.
I assume from above that class objects in the folder Model should have the logic to conduct CRUD operations.
Am I missing something. Please share your valuable thoughts.","No. In fact, it's advocated as a way to keep controllers nice and slim.",https://stackoverflow.com/questions/29973045/is-calling-service-from-controller-a-violation-of-mvc,3,1
common services in layered architecture,"I read in many books that in a layered architecture a layer should only use the services provided by the layers below it. The commonly used layers are in an enterprise application are:

Presentation
Business
Persistence

This means services at business layer (that contain business logic) should access only access the services provided by the persistence layer. 
I have a MessageService that sends messages to the users. Whenever there is a significant change in the state of an object, all the associated users must be notified about the change. This means that the Service at business layer that identified the change must send use MessageService to send messages. But messageService is itself at the business layer, hence no other service from the same layer should access it. 
So how can we use MessageService without violating the architecture of the code?","Presentation Layer(or the Top Layer) does not mean only UI, it can be anything that consumes the services in the system. ie, a Scheduled Job can be at the top layer(may be you don't want to name it as presentation layer).In your case I feel MessageService should be at top level as it consumes other services in the system. For example if you write a web service it should be above the service layer but you may be want it to be named differently.",https://stackoverflow.com/questions/7472815/common-services-in-layered-architecture,1,1
How would a platform or core code design look like in a microservices architecture? [closed],"In a monolithic architecture, you would have a core/platform code on top of which a bunch of services or business domains would be built. Some examples being, db abstraction, external service abstraction etc. 
In case of micro services, would the platform code be written as a module, which would be imported as a dependent module in each of the micro services or, does this violate the construct of the architecture because of tight coupling between the module and the common (core/platform) code and thereby going back to the issues related multiple deployments, code bugs, vendor lock ins etc?","Microservices in the current context they are being referred to today are intended to be stand alone, a service with no dependence on another service (Data stores as well, dedicated to the microservice).
It is a good idea even if not going the microservices route to divide your application into smaller modules/services/packages etc.. This will allow for more maintainable code.",https://stackoverflow.com/questions/27752201/how-would-a-platform-or-core-code-design-look-like-in-a-microservices-architectu,1,1
Dependency from Gateway to Framework in Clean Architecture,"Let's imagine I would want to implement an ASP.NET application based on Uncle Bobs Clean Architecture. As far as I understood it:

Asp.Net itself would be in the framework circle
An Asp.Net controller would be in the gateways/interface adapters layer
my business logic would be in the usecases/entities layer

The dependency rule says only dependencies from outer circles to inner circles are allowed.
As i understood it the dependency rule is not just about control flow but about code level dependencies in general.
BUT: in order to have an Asp.Net controller in the ""gateways"" circle it would have to derive from Asp.Net Controller class.
Question: wouldn't that violate the dependency rule as this introduces a compile time dependency from ""gateways"" circle to ""frameworks"" circle?
Update: I have discussed this question in more detail in my recent blog post https://plainionist.github.io/Implementing-Clean-Architecture-AspNet/","Yes, it breaks the rules.
Framework vendors don__ care about it though, on the contrary, they strive from having our applications vendor locked to their frameworks.
Therefore we should choose our tech stack, including framework, according to the project requirements. If it is a requirement that we create a prototype quickly, we need to choose a framework that helps us do RAD. If the requirements tell us that the business concept is established and the application will live for a long time, we need to choose a framework that allows us to keep our application decoupled from the framework and other tools, so that we can easily update and/or swap tools, including the framework.
For example Symfony allows us to have controllers coupled or decoupled to the framework. When it comes to ORMs we also have that problem, where Propel forces us to have entities extending the Propel entities, while Doctrine allows us to have entities completely unaware of the ORM.",https://stackoverflow.com/questions/48589192/dependency-from-gateway-to-framework-in-clean-architecture,1,1
Code Map Visual Studio not showing circular references,"I have this code that has a circular reference between Service1 and Service 2 and I am using VS2015 Code Map to find circular references but they do not seem to be showing up
I have selected Layout > Analyzers > Circular References Analyzer and according to the legend on the right it should be highlighted in red square boxes...
    public interface IService1
        {
            void Dosometing1();
            void Donothing();
        }

        public class Service1 : IService1
        {
            private readonly IService2 _service2;

            public Service1(IService2 service2)
            {
                _service2 = service2;
            }

            public void Dosometing1(){}
            public void Donothing()
            {
             _service2.Dosometing2();   
            }
        }

        public interface IService2
        {
            void Dosometing2();
        }

        public class Service2 : IService2
        {
            readonly IService1 _service1;

            public Service2(IService1 service1)
            {
                _service1 = service1;
            }

            public void Dosometing2()
            {
                _service1.Donothing();

            }
        }","There are also no circular references in that graph anywhere. All the links flow from left to right and none from right to left. The circular references analyzer highlights cycles in the visible graph. 
Here's an example of a simple circular reference at method level:
    class TestMe
    {
        public void A()
        {
            B();
        }

        public void B()
        {
            A();
        }
    }

And the code map: 

Please let me know if you were expecting something else. It would also help if you could describe what sort of architectural violation you are trying to catch :)",https://stackoverflow.com/questions/33113741/code-map-visual-studio-not-showing-circular-references/33741448,1,1
Can a service-layer object __ervice_ a DAO-layer object?,"Can a service-layer object ""service"" a DAO-layer object? Or is does this violate the integrity of a properly layered architecture?
e.g.,
class MyService {

}

class MyDao {
     private MyService myService;
}","I wouldn't do it.  DAOs ought to deal with databases; services manage other services and DAOs to fulfill their use cases.  It'd be better to have the service call the DAO and any other services that are required.
You risk a circular dependency with this arrangement.",https://stackoverflow.com/questions/12885418/can-a-service-layer-object-service-a-dao-layer-object,2,1
Adding System.Web reference to Business Logic Layer in n-layer Architecture,"I'm using TableProfileProvider to use ASP.NET profile system in an n-layer architecture.
The UI layer is a web application so I have to expose the profilecommon class to be able to use profiles.
Here's a simplified schema of my architecture:
UI: ASP.NET Web Application.
BusinessEntities: Pure POCO Classes. Persistence Igronace.
BLL: Business logic layer.
DAL: Data Access Layer.  
The Profilecommon definition is:  
 public class ProfileCommon : ProfileBase
 {
    public virtual ProfileCommon GetProfile(string username)
    {
        return (ProfileCommon)ProfileBase.Create(username);
    }

    public virtual string FirstName
    {
        get
        {
            return (string)base.GetPropertyValue(""FirstName"");
        }
        set
        {
            base.SetPropertyValue(""FirstName"", value);
        }
    }
 }  

In a simple design architecture where everything is defined in the web application project, I'd access the profilecommon as follows:
ProfileCommon strongleyTypedProfile = (ProfileCommon)this.Context.Profile;  
I'd like to be able to access the Profile Common from my Business Logic Layer, So I moved the ProfileCommon definition to my BusinessEntities Library (Had to add reference to System.Web assembly in BusinessEntities library) and defined the new ProfileBLL Class:  
public class ProfileInfo
{
    public ProfileInfo(ProfileCommon profile)
    {
        this.Profile = profile;
    }

    public ProfileCommon Profile { get; set; }

    public string GetFullName()
    {
        return this.Profile.FirstName + "" "" + this.Profile.LastName;
    }
}  

Now I can access profile common from UI like this:  
var profileInfo = new BLL.ProfileInfo((ProfileCommon)this.Context.Profile);
txtFullName.text = profileInfo.GetFullName();

Now, is referencing System.Web in Business Layer/BusinessEntities Library violates the n-layer architecture disciplines? If so, What would you suggest in order to achieve this?","You can break the dependency on ProfileBase by implementing a interface instead. Lets say
public interface IProfile
{
    string FirstName { get; set; }
    string LastName { get; set; }

    IProfile GetProfile(string username);
}

public class ProfileCommon : ProfileBase, IProfile
 {
    public virtual IProfile GetProfile(string username)
    {
        return (ProfileCommon)ProfileBase.Create(username);
    }

    public virtual string FirstName
    {
        get
        {
            return (string)base.GetPropertyValue(""FirstName"");
        }
        set
        {
            base.SetPropertyValue(""FirstName"", value);
        }
    }
 }

public class ProfileInfo
{
    public ProfileInfo(IProfile profile)
    {
        this.Profile = profile;
    }

    public IProfile Profile { get; set; }

    public string GetFullName()
    {
        return this.Profile.FirstName + "" "" + this.Profile.LastName;
    }
} 

Now you don't have any dependency on System.Web.dll in your Business Logic but still have the freedom to implement the IProfile interface in your webapplication using the ProfileBase",https://stackoverflow.com/questions/4799586/adding-system-web-reference-to-business-logic-layer-in-n-layer-architecture,2,1
"Should I pass doctrine Registry or specific repositories, entity managers to DI container in symfony2?","what is the best practice for injecting repositories, entity managers to a service?
I figured I can do it in at least three ways.
1. Injecting repositories, managers through the constructor
This way the service can be covered with tests quite easily. All you need is to pass mocked dependencies to the constructor and you're ready.
class TestService
{
    public function __construct(MyEntityRepository $my, AnotherEntityRepository $another, EntityManager $manager)
    {
        $this->my = $my;
        $this->another = $another;
        $this->manager = $manager;
    }

    public function doSomething()
    {
        $item = $this->my->find(<...>);
        <..>
        $this->manager->persist($item);
        $this->manager->flush();
    }
}

2. Passing just the EntityManager
This is a bit more difficult to test if you need like 4 repositories from the same manager. I figured this way you have to mock manager's getRepository calls.
class TestService
{
    public function __construct(EntityManager $manager)
    {
        $this->manager = $manager;
    }

    public function doSomething()
    {
        $item = $this->manager->getRepository('my')->find(<...>);
        <..>
        $this->manager->persist($item);
        $this->manager->flush();
    }
}

3. Passing the whole registry
This way you don't get circular reference exception with doctrine event subscribers, but it's harder to mock everything.
Also this is the only way sensiolabs insights doesn't give me an architectural violation for injecting EntityManager. 
class TestService
{
    public function __construct(RegistryInterface $registry)
    {
        $this->doctrine = $registry;
    }

    public function doSomething()
    {
        $item = $this->registry->getManager()->getRepository('my')->find(<...>);
        <..>
        $this->registry->getManager()->persist($item);
        $this->registry->getManager()->flush();
    }
}

What is the best practice to do this and why?","I always try to inject my services as specific as possible.
Which means I always inject repositories since that is easier when writing tests. Otherwise you have to mock the registry and or manager too.",https://stackoverflow.com/questions/20638802/should-i-pass-doctrine-registry-or-specific-repositories-entity-managers-to-di,2,1
Architectural best practices - where to put non-REST actions?,"I'm making my first real Rails app, and am learning about REST architecture and how it fits into Rails apps. My controllers don't really seem to map logically to an individual resource, so it's tough for me to implement strict REST. For instance, I have a ""catalog"", ""checkout"" and ""admin"" controller, not a ""products"", ""categories"", ""orders"" and ""users"" controller, although those are the main resources being used in those controllers. Each of the controllers does more than just REST actions on a resource however, all of them pull from multiple models and service multiple views. 
Am I missing a design/architectural practice that would make REST work better here? Namespaced controllers perhaps, with non-REST actions in the top-level controller only, and REST actions in the sub-controllers? Seems like I would run into some DRY violations there...","If this is the way you want to structure your app, forget about REST and just map the routes you need. If you don't actually want to manage resources why bother?",https://stackoverflow.com/questions/1153492/architectural-best-practices-where-to-put-non-rest-actions,4,1
"Syntax error versus compiler error in Visual Studio, or red wavy underline versus blue wavy underline","What is the difference between a ""syntax error"" and a ""compiler error"" as Visual Studio sees it?  Or, put another way, why are some ""compile-time"" errors underlined with red wavy lines and some with blue wavy lines?  Here is an example:

The red underlined error above has this description:

No overload for method 'ValidateFilteredRecipient' takes 6 arguments

The blue underlined error has this description:

'ValidateBuild': cannot declare instance members in a static class

It's not clear to me what the distinguishing characteristics of the two errors are.
I thought finding the answer would be a piece of cake: I'd just google it and the first result would be an MSDN page expounding this topic fully; however, very surprisingly, this was not the case.  I started off by trying to google the colors (since I didn't yet know red meant ""syntax error"" and blue meant ""compiler error""):
visual studio red underline vs. blue underline
No help there.  Then I tried these searches:
visual studio error underline color meanings
visual studio underline color meanings
I could see that this was going nowhere, so I googled a bit more and figured out where the colors were set in VS: Tools > Options > Environment > Fonts and Colors.  By the way, the inability to search the huge list here is extremely annoying, but I figured out that ""syntax errors"" have the red wavy underline, and ""compiler errors"" have the blue wavy underline.

So what do those mean?  Back to Google:
visual studio compiler error vs. syntax error
Nothing relevant.  Here's the closest I've found, from MSDN:
Fonts and Colors, Environment, Options Dialog Box
This page has these entries:

Compiler Error -- Blue squiggles in the editor indicating a compiler error. 

and

Syntax Error -- Parse errors.

Not at all helpful -- emphasis on ""at all"".  I have always thought I knew what a ""syntax error"" was, at least, and Wikipedia agrees:

a syntax error is an error in the syntax of a sequence of characters
  or tokens that is intended to be written in a particular programming
  language.

Also, here is what what it has for syntax:

the syntax of a computer language is the set of rules that defines the
  combinations of symbols that are considered to be a correctly
  structured document or fragment in that language.

So, going back to my ""syntax error"" from above:

No overload for method 'ValidateFilteredRecipient' takes 6 arguments

How is that a syntax error per the definition I've included here?  Actually, in my opinion, the ""compiler error"" I got more-closely meets this definition of a syntax error:

'ValidateBuild': cannot declare instance members in a static class

Can someone please help me figure this out?","The key difference between the syntax error and compilation error is when Visual Studio detects it.
Syntax error is detected and highlighted as you code. You do not have to build the code to get these errors.
However, Compilation errors are complex that Editor cannot detect as you code. You will have to run it through a compiler (do a build) to identify them. So the chances are one might end up continue coding without compiling and will not be able to detect them until he builds.
Basically, Syntax errors are a subset of compilation errors. If you use text editor like notepad to write your code, you will never see syntax error. All will be logged as Compilation errors when you run the code through compiler.
In regards to highlighting them differently, I think it is a visual representation for the Developer to understand what he could have caught this while writing the code.
As mentioned by everyone, at the end of day, you need to fix both to make you code run.
This is true for any programming language and the editor used along with that language.",https://stackoverflow.com/questions/40550668/syntax-error-versus-compiler-error-in-visual-studio-or-red-wavy-underline-versu/40550812,2,1
How to implement IDependencyResolver in an N-tier layer application?,"I have a three-layered application. Each of parts have dependencies from another part of my solution. I used to implement IDependencyResolver inside of MVC project. This is a wrong way, because it leads to violating the architectural rules of layer separation. My MVC project had reference to DAL layer. It's a bad coding practices. I know that I can create a separate class library project. It will has references to any other projects of my solution. It will resolve all dependencies. I heard that it not the best way. There is a better way. Each projects of my solution should resolve it owns dependencies. I don't understand how to put it all together. So I found this useful article: Dependency Injection Best Practices in an N-tier Modular Application but it seems too difficult and complex. Is there another way? I have a similar structure of solution. UserRepository returns a requsted user. 
public interface IUserRepository
    {
        IEnumerable<UserEntity> GetAll();
    }

    public class UserRepository : IUserRepository
    {
        public IEnumerable<UserEntity> GetAll()
        {
            // some code
        }
    }

UserService can has a few different dependencies. 
public interface IUserService
    {
        IEnumerable<UserModel> GetAll();
    }

    public class UserService : IUserService
    {
        private readonly IUserRepository userRepository;
        private readonly ISecondRepository secondRepository;
        private readonly IThirdRepository thirdRepository;

        public UserService(IUserRepository userRepository, ISecondRepository secondRepository, IThirdRepository thirdRepository)
        {
            this.userRepository = userRepository;
            this.secondRepository = secondRepository;
            this.thirdRepository = thirdRepository;
        }

        public IEnumerable<UserModel> GetAll()
        {
            // some code
        }
    }

And finally UserController constructor might has a lot of different dependencies. 
My question is about what is the right and the simplest way to resolve these dependencies avoiding violating the architectural rules?","Like you said you can create additional layer which will compose dependencies, this is called composition root Check this SO question.
In your case the composition root is the MVC project. From my point of view referencing the DAL is not that bad practice. There is line should be drawn when we are speaking of dependencies. For me when we talk about dependencies is not that important the dll reference but the type(interface or concrete) that is used. As you know DI is about depending on abstraction where it has actual value. So your code is still ok, as long as you depend only on the interfaces in the DAL. 
In the practices i have used additional project as composition root when the dll dependencies became too complex.
About your question about how layers should resolve their dependencies. I am not sure if this is what you meant, but in DDD there is practice your BLL to define the infrastructure interfaces( like repositories) in the layer it self. This way the dependency graph is reveres. Now the infrastructure layer (DAL) you only define concrete implementation of the interfaces provided from the BLL, and again in the composition root everything is wired.
The first approach where the infrastructure layer defines the interface and the implementation has the advantage to be dependency free, which is siutable for reusage across different projects. But keep in mind when working in big domain this sometimes may lead to unmaintanable code.
The second approach being DDD the most important thing is the domain, so everything works for the domain. In my experience having well structured layers around the domain is the best option. This apporach makes your code more explicit about the problems you solve for the domain.
I don't know if i have express my self corectly.
As final note i will suggest you the DDD approach if using this just as a learning experiance. Learning something new is the best option.
I am not the most experiance person in the field.
I am programmer for about 3 years, and have worked mostly on middle sized projects, so my opinion is not solid ;]",https://stackoverflow.com/questions/42274352/how-to-implement-idependencyresolver-in-an-n-tier-layer-application,2,1
Why does including an action verb in the URI in a REST implementation violate the protocol?,"I'm finding it necessary to understand why including action verbs in the URI violates the REST protocol for URI syntax? When I read the following article, I sense that too many people are making too much noise about verbs, and that they should be making more noise about content types:
RestWiki: Minimum Methods
In a perfect world, client browsers would all support GET, POST, PUT and DELETE for request operations. However, only GET and POST are supported, which means we're stuck trying to identify operations that should be PUT and DELETE by using common action verbs in the URL like view, create, edit, and delete.
How does this violate the spirit of the REST architectural principles, and what is the roadblock you experience by putting something like ""delete"" into your URL instead of using ""deletion""?","The only valid reason for the guidance around URIs is to encourage proper usage of the REST verbs.  If a request performs an action that is consistent with the client's expectation as per the HTTP standards then it really does not matter what the url contains.
Naming urls based on nouns makes it natural to create behaviour that is consistent with the intended purpose of GET, PUT, POST and DELETE. 
When you put verbs in the URL it can become very confusing because often the http verb will have contradictory behaviour to the one in the URL.  REST rules say you must respect the HTTP verb, but usually the url is more descriptive so it can be misleading.
The fact that browsers only support a subset of HTTP verbs is not really relevant because even when you do have full access to all the HTTP verbs, you still need to be able to model other verbs, like print, close, confirm, cancel.
You are absolutely right that people need to focus way more on content types than URL structure when talking about REST implementations.
Making your URLs refer to nouns is not a REST constraint, it is about encouraging people to fall into the pit of success.",https://stackoverflow.com/questions/2173721/why-does-including-an-action-verb-in-the-uri-in-a-rest-implementation-violate-th,2,1
accessing devise current_user within model,"hi i am trying to access current_user within a model for the purpose of creating an element on the fly with find_or_create_by.
the following is the method within my model
def opponent_name=(name)
self.opponent = Opponent.find_or_create_by_name_and_team_id(name,current_user.team_id) if name.present?
end

but the error i am getting is 
NameError in EventsController#create

undefined local variable or method `current_user' for #<Event:0x007fb575e92000>","current_user is not accessible from within model files in Rails, only controllers, views and helpers.
What you should do is to pass the current_user.team_id to the opponent_name method like this:
def opponent_name=(name, current_user_team_id)
  self.opponent = Opponent.find_or_create_by_name_and_team_id(name,current_user.team_id) if name.present?
end",https://stackoverflow.com/questions/15680541/accessing-devise-current-user-within-model/18707759,4,1
Custom PMD rules for finding out method usages,"Recently, we were trying to write a PMD rule to spot all occurances of Spring JDBC template's query* methods. Looking at some sample AST xml code, I wrote the following innocuous XPATH expression. 
//PrimaryPrefix[Name[starts-with(@Image,'jdbcTemplate.query')]]
But very soon, we realized that this is not adequate. If someone writes ""this.jdbcTemplate.queryForObject"" then ""this"" becomes the ""Primary Prefix"" and ""jdbcTemplate"" becomes the ""Suffix"". Also the variable name of the JDBCTemplate object instance could be anything. 
I thought it would be fairly easy to construct a XPATH expression to find out the occurance of a particular Class method call - anywhere in the code, but looking at the AST tree, I am just not able to figure it out. Is a XPATH really possible, or we have to write Java code?",I would suggest using the Sonar architectural rules engine to find this kind of violation.,https://stackoverflow.com/questions/11827058/custom-pmd-rules-for-finding-out-method-usages/11835521,1,1
How to get WSDL for REST Service in Java,"Hi I am working REST Services and I just want to know how can I provide the web descriptor for REST Services. It will be great if their is some API that can directly produce the WSDL for REST Service.
I am using Spring STS for development is their anything inbuilt in Spring STS. 
I have few more questions for the same: 

Is it feasible to provide the descriptor for REST Service.
Providing the descriptor is not the violation of REST or HATEOS architecture style.
What are the different ways to achieve this.","REST services shouldn't use WSDLs. WSDLs are used for SOAP Web services, not for the REST design style.
Instead of using WSDLs, REST APIs should be discoverable.",https://stackoverflow.com/questions/23117879/how-to-get-wsdl-for-rest-service-in-java,2,1
Violation of Law of Demeter in Interactors/Use Cases/Application Services,"Lately I have been looking through a bunch of ideas for software architecture. What I see is that a lot of them converge into having Use Cases/Interactors (Clean Architecture) or Application Services (DDD) as an entry point to our applications.
I really like the idea, but something has been bothering me.
Both ways the user injects the repository, which you use to fetch the domain entities and perform an action on it. Like this:
class Interactor
  def initialize(repository)
    @repository = repository
  end

  def call(entity_id)
    entity = @repository.find(entity_id)
    entity.do_something
  end
end

If I want to test this in isolation, I need to return a mock from @repository.find, which already is a mock. That is not good and tells me that I am violating the Law of Demeter (which is the case).
Also, this is too procedural, which bothers me.
Is there a better way to do this?","That is not good and tells me that I am violating the Law of Demeter (which is the case).

How so?
The Law of Demeter is summarized and paraphrased in a variety of ways, but for simplicity let's just look at the small list on Wikipedia:


Each unit should have only limited knowledge about other units: only units ""closely"" related to the current unit.
Each unit should only talk to its friends; don't talk to strangers.
Only talk to your immediate friends.


So in what way does the code shown violate this law?  Interactor has knowledge of exactly two things, a particular repository and a particular entity.  Its knowledge of the repository is a technical concern, and it uses that interaction to get an instance of an entity.  And its knowledge of the entity is a business concern, and it uses that interaction to perform a business operation.
On any domain graph or chart or whiteboard drawing, Interactor would be expected to have a direct connection to both of these units.  And indeed it does.  It doesn't reach into those units to have knowledge of their neighbors, it just has knowledge of its own neighbors and interacts only with them.
So... What exactly is the problem?
As a purely technical concern you may be able to inject the entity directly instead of injecting the repository.  There isn't enough information about the system represented in the question to really list any pros and cons there.  But that's not a question of the Law of Demeter, that's just an implementation concern of dependency management.
Unit testing requires that you create two mocks, sure.  But this unit interacts with two units.  So that's a given, really.",https://stackoverflow.com/questions/35636802/violation-of-law-of-demeter-in-interactors-use-cases-application-services,1,1
Can I use messaging system's CorrelationId as a primary key and can it be used for further communcation,"Here is my system architecture of data flow for creating a record.
Clients ====> RabbitMQ ====> (RESTful service, also listens RabbitMq)
However, the clients can also directly access the RESTful service by REST endpoints for fetching a record.
In order to uniquely identify a record, the client sends the ""CorrelationId"" (an UUID/GUID). Since clients generate GUIDs, can clients use the same GUID as a resource identifier to access resources from the RESTful service?
Example: http://MyApi/Resource/GUID
I know, GUID is 32 character long for an identifier. It's bit strange, apart from that do you see any RESTful design pattern violation? 
Please suggest me the recommended approach.
Thanks,
Pandiarajan","There is no issue with allowing a GUID as a unique identifier for a resource. REST says nothing about what a URI should look like. There are some tradeoffs (less human-readable, more secure), but nothing strongly compelling in most cases. So sure, if it's the right thing for you to do, do it.",https://stackoverflow.com/questions/41318972/can-i-use-messaging-systems-correlationid-as-a-primary-key-and-can-it-be-used-f,1,1
MySQL procedure: change between EXISTS and NOT EXISTS by evaluating an input param?,"I'm working on a MySQL procedure.  I'm doing something like this:
IF aninputparam = 1 THEN
    SELECT blah 
    FROM blah
    JOIN etc etc
    WHERE EXISTS (SELECT mysubquery)
ELSE
    SELECT allthesamestuff
    FROM allthesametables
    JOIN allthesamejoins
    WHERE NOT EXISTS (SELECT samesubquery)
END IF

That works, but it offends my artistic sensibilities and is contrary to DRY principles, not to mention an example of the cut-and-paste architectural antipattern.  Can I do something like
@x = concat(
    'SELECT blah FROM blah JOIN etc WHERE ', 
    IF(aninputparam = 1, 'EXISTS', 'NOT EXISTS'), 
    ' (SELECT mysubquery)'
 );
PREPARE stmt1 FROM @x; 
EXECUTE stmt1; 
DEALLOCATE PREPARE stmt1; 

And if so, are there any disadvantages to preparing the SQL statement dynamically that way?  (And might there be a better way to do it that I don't know about?)","Here are a few reasons why you might opt not to compose a dynamic query:

Writing dynamic queries inside stored procedures makes it more likely that you'll create a SQL injection vulnerability. This does not appear to be the case with your code example, but it's definitely a risk if you start writing dynamic queries on a large scale.
MySQL pre-compiles the execution plan of stored procedures and executes the query from cache. This provides optimal performance. Dynamic queries cannot be cached in this manner as they must be built at run-time for every request. 
It can be more difficult to debug dynamic queries when something goes wrong. You can get syntax errors at runtime that would've otherwise been caught at compile time for a static stored procedure.

It might use more lines of code, but your first example is fine. It's easier to understand than your second example and doesn't have any of the drawbacks I've listed above.
Furthermore, I wouldn't say that your first code example violates DRY principles. The two queries do two different things. However, your second example may well violate the KISS principle.",https://stackoverflow.com/questions/38156455/mysql-procedure-change-between-exists-and-not-exists-by-evaluating-an-input-par,3,1
Where to put business logic in DDD,"I'm trying to figure out the best way to build an easily maintainable and testable architecture. Having gone through several projects, I've seen some pretty bad architectures and I want to avoid making future mistakes on my own projects.
Let's say I'm building a fairly complex three layer application and I want to use DDD. My question is, where should I place my business logic? Some people say it should be placed in services (service layer) and that does make sense. Having a number of services which adhere to Single Responsibility Principle makes sense.
However, some people said that this is an anti pattern and that business logic shouldn't be implemented in the service layer. Why is this?
Let's say we have IAuthenticationService which has a method with bool UsernameAvailable(string username) signature. The method would implement all required logic to check whether the username is available or not.
What is the problem here according to the ""this is an antipattern"" crowd?","A service layer in itself is not an anti-pattern, it is a very reasonable place to put certain elements of your business logic. However, you do need to apply discretion to the design of the service layer, ensuring that you aren't stealing business logic from your domain model and the objects that comprise it.
By doing that you can end up with a true anti-pattern, an anaemic domain model.
This is discussed in-depth by Martin Fowler here.
Your example of an IAuthenticationService isn't perhaps the best for discussing the problem - much of the logic around authentication can be seen as living in a service and not really associated with domain objects. A better example might be if you had some sort of IUserValidationService to validate a user, or even worse a service that does something like process orders - the validation service is stripping logic out of the user object and the order processing service is taking logic away from your order objects, and possibly also from objects representing customers, delivery notices etc...",https://stackoverflow.com/questions/6891019/where-to-put-business-logic-in-ddd,3,1
How to avoid dispatching in the middle of a dispatch,"Within my Flux architected React application I am retrieving data from a store, and would like to create an action to request that information if it does not exist. However I am running into an error where the dispatcher is already dispatching. 
My desired code is something like:
getAll: function(options) {
  options = options || {};
  var key = JSON.stringify(options);
  var ratings = _data.ratings[key];

  if (!ratings) {
    RatingActions.fetchAll(options);
  }

  return ratings || [];
}

However intermittently fails when the dispatcher is already dispatching an action, with the message Invariant Violation: Dispatch.dispatch(...): Cannot dispatch in the middle of a dispatch.. I am often making requests in response to a change in application state (eg date range). My component where I make the request, in response to a change event from the AppStore has the following:
getStateFromStores: function() {
  var dateOptions = {
    startDate: AppStore.getStartISOString(),
    endDate: AppStore.getEndISOString()
  };

  return {
    ratings: RatingStore.getAll(dateOptions),
  };
},

I am aware that event chaining is a Flux antipattern, but I am unsure what architecture is better for retrieving data when it does not yet exist. Currently I am using this terrible hack:
getAll: function(options) {
  options = options || {};
  var key = JSON.stringify(options);
  var ratings = _data.ratings[key];

  if (!ratings) {
    setTimeout(function() {
      if (!RatingActions.dispatcher.isDispatching()) {
        RatingActions.fetchAll(options);
      }
    }, 0);
  }

  return ratings || [];
},

What would be a better architecture, that avoids event chaining or the dispatcher error? Is this really event chaining? I just want to change the data based on the parameters the application has set.
Thanks!","You can use Flux waitFor() function instead of a setTimeout
For example you have 2 stores registered to the same dispatcher and have one store waitFor the other store to process the action first then the one waiting can update after and dispatch the change event. See Flux docs example",https://stackoverflow.com/questions/30357120/how-to-avoid-dispatching-in-the-middle-of-a-dispatch,5,1
When can a design pattern make your software worse? [closed],"When can a design pattern make your software worse?
I have seen a program where they used the facade pattern between the GUI and logic. They considered that no objects may be transported over this, so only primitive types were used which made it difficult to code.","For decades programmers have often spent time making software worse by applying practices they don't understand, rather than learning why that practice is good.  The use of certain tools/keywords/frameworkes can make a programmer feel sophisticated, when they're just screwing things up.  Some common examples:

Throwing in lots of try/catch/finally blocks can make a programmer feel like they have an error handling scheme, when in fact they don't.
Replacing sql with stored procedures can make programmers feel like they have a data access layer that magically gives them efficiency, reuseability, encapsulation, when in fact they don't.
Using classes makes many programmers believe they're engaging in object oriented programming, when in fact they aren't (or are only scratching the surface of OO).

This list could go on forever.  This tendency has always been around and always will be, because we humans always look for shortcuts.  We see this a lot with patterns now because patterns are currently popular.  The root problem is the same:  developers not having a respect for how complicated software development is, and thinking a blindly implemented pattern or practice is a substitute for learning and humility.
The solution?  Difficult to say.  You can't go to wrong though handing a junior developer Code Complete or some such, to help them understand this is all about applying principles to particular situations, and that great developers keep it simple.",https://stackoverflow.com/questions/1820106/when-can-a-design-pattern-make-your-software-worse/2544935,18,1
PHP project with excellent OOP design for studying purposes,"I've recently became interested in proper OOP design in web applications. I think I understand most of the principles and design patterns but sometimes I have problem with putting them into practice.
I use MVC and I think I am able to design controllers and views in OOP way. The problem I face is with models. I'm particularly obsessed with dependency injection and inversion of control in general. It works nice in small examples but I have trouble to design complex multi layered models - I'm thinking about various ways to inject dependencies into lower layers etc.
So I decided to look for some projects made by more experienced OOP programmers which I could study. I'm looking for PHP web application, preferably made with MVC architectural pattern. Also I don't mind if it has anemic model (which is usually considered to be antipattern but in heavy data based applications I'm often forced to use anemic models).
Thanks a lot.
EDIT: I'm not looking for a framework but for a complete application. Frameworks usually have not a lot of to do with model architecture.","Magento Commerce has complex multi layered models (www.magentocommerce.com) which you may pick up a trick or two from.
But my (personal) experience with Magento was frustrating: I was hacking with it around version 0.8-1.1 and it appeared over engineered and poorly documented. Trying to figure out how it actually worked was complex: you'd end up with tons files open in your editor, most of them called Abstract.php. Hopefully things have improved a long way since then.",https://stackoverflow.com/questions/3613084/php-project-with-excellent-oop-design-for-studying-purposes,7,1
"Master data management, the right approach","I am in need of developing a windows form, which is suppossed to perform CRUD operations on a number of 5-10 unrelated tables (say Categories, Partners, Locations etc). 
The no of columns, data elements and their types will be different for each table. There is an architecture astronaut push, to make this as flexible as possible. Somebody suggested to  make the user interface is completely configurable by saving the meta data information (yeah including SQL queries data type references, UI elements etc) in a configuration table and generating the UI on-the fly from the configuration.
I have a strong feeling that this is gonna be a Inner Platform Effect antipattern and will end up as a maintenance nightmare. Can someone please suggest whether building something on a production system is advisable or not? 
The client do not have any specific mention that it needs to be 'configurable'. But the astronaut push is because of the likelihood that a couple of new data tables may come up in the near future. Isn't it better to apply KISS/YAGNI principle here rather than building an inner platform? Can someone please enlighten??
Thanks in advance.","To me it sounds like your concerns about the Inner Platform Effect are valid.  If I understand your problem correctly,  I believe I would build the code to manage the tables that you do have using solid design.  Then when new tables are added in the future adding them to the system while following that same design pattern will be less overall work that creating an elaborate Inner Platform.  My choice would definitely be KISS/YAGNI.",https://stackoverflow.com/questions/5246612/master-data-management-the-right-approach,1,1
Some doubts related this Swing MVC implementation. Opening a database connection should be a Controller task?,"I am following a Java tutorial related to the implementation of the observer pattern (using Swing) and I have some doubts. My doubts are not related to the observer pattern but about the architecture of this tutorial application (that is based on something like an MVC logic)
So it contains an Application class that contains the main() method that is the application entry point:
public class Application {

    public static void main(String[] args) {
        SwingUtilities.invokeLater(new Runnable() {

            @Override
            public void run() {
                runApp();
            }

        });
    }

    public static void runApp() {
        Model model = new Model();
        View view = new View(model);

        Controller controller = new Controller(view, model);

        view.setLoginListener(controller);
    }

}

As you can see here it is declared a Model object (for the data), a Controller object that actually simply perform the operation related to the click on a button defined into my view, this is the View code:
public class Controller implements LoginListener {
    private View view;
    private Model model;

    public Controller(View view, Model model) {
        this.view = view;
        this.model = model;
    }

    @Override
    public void loginPerformed(LoginFormEvent event) {
        System.out.println(""Login event received: "" + event.getName() + ""; "" + event.getPassword());

    }


}

and the View object that implement a login form (username and password) with the JButton okButton (its click is handled from the previous Controller object):
public class View extends JFrame implements ActionListener {

    private Model model;
    private JButton okButton;
    private JTextField nameField;
    private JPasswordField passField;
    private JPasswordField repeatPassField;

    private LoginListener loginListener;

    public View(Model model) {
        super(""MVC Demo"");

        this.model = model;

        nameField = new JTextField(10);
        passField = new JPasswordField(10);
        repeatPassField = new JPasswordField(10);
        okButton = new JButton(""Create user"");

        setLayout(new GridBagLayout());

        GridBagConstraints gc = new GridBagConstraints();
        gc.anchor = GridBagConstraints.LAST_LINE_END;
        gc.gridx = 1;
        gc.gridy = 1;
        gc.weightx = 1;
        gc.weighty = 1;
        gc.insets = new Insets(100, 0, 0, 10);
        gc.fill = GridBagConstraints.NONE;

        add(new JLabel(""Name: ""), gc);

        gc.anchor = GridBagConstraints.LAST_LINE_START;
        gc.gridx = 2;
        gc.gridy = 1;
        gc.weightx = 1;
        gc.weighty = 1;
        gc.insets = new Insets(100, 0, 0, 0);
        gc.fill = GridBagConstraints.NONE;

        add(nameField, gc);

        gc.anchor = GridBagConstraints.LINE_END;
        gc.gridx = 1;
        gc.gridy = 2;
        gc.weightx = 1;
        gc.weighty = 1;
        gc.insets = new Insets(0, 0, 0, 10);
        gc.fill = GridBagConstraints.NONE;

        add(new JLabel(""Password: ""), gc);

        gc.anchor = GridBagConstraints.LINE_START;
        gc.gridx = 2;
        gc.gridy = 2;
        gc.weightx = 1;
        gc.weighty = 1;
        gc.insets = new Insets(0, 0, 0, 0);
        gc.fill = GridBagConstraints.NONE;

        add(passField, gc);

        gc.anchor = GridBagConstraints.LINE_END;
        gc.gridx = 1;
        gc.gridy = 3;
        gc.weightx = 1;
        gc.weighty = 1;
        gc.insets = new Insets(0, 0, 0, 10);
        gc.fill = GridBagConstraints.NONE;

        add(new JLabel(""Repeat password: ""), gc);

        gc.anchor = GridBagConstraints.LINE_START;
        gc.gridx = 2;
        gc.gridy = 3;
        gc.weightx = 1;
        gc.weighty = 1;
        gc.insets = new Insets(0, 0, 0, 0);
        gc.fill = GridBagConstraints.NONE;

        add(repeatPassField, gc);

        gc.anchor = GridBagConstraints.FIRST_LINE_START;
        gc.gridx = 2;
        gc.gridy = 4;
        gc.weightx = 1;
        gc.weighty = 100;
        gc.fill = GridBagConstraints.NONE;

        add(okButton, gc);

        okButton.addActionListener(this);

        // Database db = new Database();
        // Database db = Database.getInstance();

        addWindowListener(new WindowAdapter() {

            @Override
            public void windowOpened(WindowEvent e) {
                try {
                    Database.getInstance().connect();
                } catch (Exception e1) {
                    JOptionPane.showMessageDialog(View.this, ""Unable to connect to database."",
                            ""Error"", JOptionPane.WARNING_MESSAGE);
                    e1.printStackTrace();
                }
            }

            @Override
            public void windowClosing(WindowEvent e) {
                Database.getInstance().disconnect();
            }

        });

        setSize(600, 500);
        setDefaultCloseOperation(EXIT_ON_CLOSE);
        setVisible(true);
    }

    @Override
    public void actionPerformed(ActionEvent e) {

        String password = new String(passField.getPassword());
        String repeat = new String(repeatPassField.getPassword());

        if (password.equals(repeat)) {
            String name = nameField.getText();

            fireLoginEvent(new LoginFormEvent(name, password));
        } else {
            JOptionPane.showMessageDialog(this, ""Passwords do not match."",
                    ""Error"", JOptionPane.WARNING_MESSAGE);
        }
    }

    public void setLoginListener(LoginListener loginListener) {
        this.loginListener = loginListener;
    }

    public void fireLoginEvent(LoginFormEvent event) {
        if (loginListener != null) {
            loginListener.loginPerformed(event);
        }
    }

}

It seems that it follow the classic MVC logic but if you look into the View class you find this code:
addWindowListener(new WindowAdapter() {

        @Override
        public void windowOpened(WindowEvent e) {
            try {
                Database.getInstance().connect();
            } catch (Exception e1) {
                JOptionPane.showMessageDialog(View.this, ""Unable to connect to database."",
                        ""Error"", JOptionPane.WARNING_MESSAGE);
                e1.printStackTrace();
            }
        }

        @Override
        public void windowClosing(WindowEvent e) {
            Database.getInstance().disconnect();
        }

    });

I am not so into Swing and I don't exactly know what is the WindowAdapter object (what is it?) but it seems to me that in this code is add a listener that handle the event related to the main windows opening. 
So when the main window of my application is opened the View obtain a new connection to the database, when it is closed the connection is colsed.
My doubt is: but is this an antipattern? Opening the connection to the database is not a responsability of the Controller?
Tnx","So, MVC doesn't actually explicitly address persistence.  There are really two schools of thought here.  Bake it into the model, so that the model becomes ""self persisting"".  Or do it in the controller.
Opening it in the view layer is absolutely not accepted practice.  There is a definite mixing of view/controller logic here.",https://stackoverflow.com/questions/30800062/some-doubts-related-this-swing-mvc-implementation-opening-a-database-connection,1,1
"caliburn.micro, Bootstrapper and CompositionRoot","I'm trying to understand what CompositionRoot is about.
Right up to now I never found a deep description of what it is about,
only short statements of what shall not be done.
Is the Bootstrapper that comes along when leveraging caliburn.micro already that what is meant ""CompositionRoot""? 
Or is it closer to the servicelocator antipattern, as it can deliver merely anything that is inside the assembly and it's dependencies.
If someone has a good description of CompositionRoot, please share.
I already know the ploeh blog. 
If I see that CompositionRoot leads to better architecture and / or helps me solve problems, I'm still willing to buy the book. But right know there is not enough
information around for me to see what it will help.
update
Let's pretend that all of my ViewModels get an EventAggregator injected (constructor injection). Now I want to dynamically create those ViewModels when they are needed.
I can register types beforehand (in the CompositionRoot), but how would I resolve dependencies later? As far as I understand, the Container should not be touched after the composition root. Definitly I do not want to create all instances before I need them (that would make the application start slow). Is ""Register - Resolve - Release"" meant here?
(that pattern is coined in the ploeh blog, too)","I assume you've seen Mark's article at http://blog.ploeh.dk/2011/07/28/CompositionRoot. 
As it states:

A Composition Root is a (preferably) unique location in an application where modules are composed together.

And that this should be:

As close as possible to the application's entry point.

In the case of Caliburn.Micro, the Bootstrapper class provides a ConfigureContainer method for you to override and compose you modules.
Ideally, it will only be your composition root that has a reference to your IoC container.
Caliburn.Micro will resolve your shell view model (if you use the generic version of the Bootstrapper) via your container.
It does also supply a static IoC class which is an implementation of the Service Locator (anti) pattern if you do need to reference the container outside of your composition root.
Update
If you wish to resolve types via your container at runtime after your composition root (for example if you have complex dependency chains) then use factory types. 
These factory types would also have a reference to your IoC container. You could either:

Pass a reference to your container as a dependency to the factory type
Use the Service Locator pattern in your factories (e.g. the Caliburn.Micro IoC class)
Some IoC containers such as Castle Windsor and (using an extension) Ninject will generate factory types for you based on a factory interface and conventions (this is the nicest option)",https://stackoverflow.com/questions/15763230/caliburn-micro-bootstrapper-and-compositionroot,1,1
"refactor a dal with hibernate,dao,services and unit of work : how to put all togheter","i'm refactoring a jsf web application dal which uses hibernate and the dao pattern, at the moment each data acces class handles its own session and transaction into each method (a session per operation antipattern is it? )
for example :
public void saveEntity(ModelEntity entity) throws Exception {
    String entityName = getCleanClassName(entity.getClass());
    Session session = null;
    Transaction tx = null;
    try {
        session = SessionFactoryUtil.getInstance().openSession();
        tx = session.beginTransaction();
        session.saveOrUpdate(entity);
        session.flush();
        tx.commit();
    } catch (Exception ex) {
        tx.rollback();
                    ...
        throw new Exception(msg, ex);
    } finally {
        session.close();
    }
}

Above classes can be used directly into backing bean actions or through some business logic classes methods (for example create new detached object, set some values and store them trhough DAO objects). Now i've found that some businness logic classes methods need to be executed atomically, for example :
                       //inside some backing bean actionlistener
        DocumentoManager dm = new DocumentoManager();
        PraticaManager pm = new PraticaManager();
        Documento newDoc = new Documento();
        newDoc.setDataArrivo(new java.util.Date                           newDoc.setNote(""..."");
        Allegato newAll = new Allegato();
        newAll.setTitolo(newDoc.getNote());
                       //businness logic methods that should be executed atomically
        dm.creaDocumentoDaAllegato(newDoc,newAll,
            event.getFile().getContents(),
            event.getFile().getFileName(),
            """");
        pm.collegaDocumento(pratica, newDoc);

Finally i've a sessionFactoryUtil class which manages hibernate session (getcurrentSession, openSession, etc ... ).
How can i refactor the above architecture? i was thinking to do the following:

remove the transaction from the DAO objects : i can't put
transactions management into bl classes (xxxManager) , so should i
use a class that just create,commit or rollback the transaction ,
encapsulating the sessionFactoryUtil, and to be used inside backing
beans (say jta userTransaction) ? 
If someone try to use directly the DAO object how do i set the session ? Accessing sessionFactoryUtil into DAOs methods ?
Is it safe to call getCurrentSession inside DAOs?
if i create a class to handle businness logic transactions, how can i be sure it's thread safe ?

I think sessionFactoryUtil is thread safe since it's been written following the pattern suggested on hibernate guide :
private SessionFactoryUtil() {
}

static {
    // Annotation and XML
    // sessionFactory = new
    // AnnotationConfiguration().configure().buildSessionFactory();
    // XML only
    try {
        sessionFactory = new Configuration().configure().buildSessionFactory();
    } catch (Exception e) {
                  ...
    }

}

public static SessionFactory getInstance() {
    try{
        return sessionFactory;
    } catch (Exception ex)
    {
        return null;
    }
}","Transactions should not be part of the DAO.  Those should be handled by a service layer that manages DAOs, model objects, and transactions to fulfill use cases.  The service layer owns and manages the unit of work.
DAOs should be about persistence only.  They need not know anything about sessions.  Let the service give the DAO what it needs to persist an object.",https://stackoverflow.com/questions/7819170/refactor-a-dal-with-hibernate-dao-services-and-unit-of-work-how-to-put-all-tog,1,1
How can you represent inheritance in a database?,"I'm thinking about how to represent a complex structure in a SQL Server database.
Consider an application that needs to store details of a family of objects, which share some attributes, but have many others not common. For example, a commercial insurance package may include liability, motor, property and indemnity cover within the same policy record.
It is trivial to implement this in C#, etc, as you can create a Policy with a collection of Sections, where Section is inherited as required for the various types of cover. However, relational databases don't seem to allow this easily.
I can see that there are two main choices:

Create a Policy table, then a Sections table, with all the fields required, for all possible variations, most of which would be null.  
Create a Policy table and numerous Section tables, one for each kind of cover.

Both of these alternatives seem unsatisfactory, especially as it is necessary to write queries across all Sections, which would involve numerous joins, or numerous null-checks.
What is the best practice for this scenario?","@Bill Karwin describes three inheritance models in his SQL Antipatterns book, when proposing solutions to the SQL Entity-Attribute-Value antipattern. This is a brief overview:
Single Table Inheritance (aka Table Per Hierarchy Inheritance):
Using a single table as in your first option is probably the simplest design. As you mentioned, many attributes that are subtype-specific will have to be given a NULL value on rows where these attributes do not apply. With this model, you would have one policies table, which would look something like this:
+------+---------------------+----------+----------------+------------------+
| id   | date_issued         | type     | vehicle_reg_no | property_address |
+------+---------------------+----------+----------------+------------------+
|    1 | 2010-08-20 12:00:00 | MOTOR    | 01-A-04004     | NULL             |
|    2 | 2010-08-20 13:00:00 | MOTOR    | 02-B-01010     | NULL             |
|    3 | 2010-08-20 14:00:00 | PROPERTY | NULL           | Oxford Street    |
|    4 | 2010-08-20 15:00:00 | MOTOR    | 03-C-02020     | NULL             |
+------+---------------------+----------+----------------+------------------+

\------ COMMON FIELDS -------/          \----- SUBTYPE SPECIFIC FIELDS -----/

Keeping the design simple is a plus, but the main problems with this approach are the following:

When it comes to adding new subtypes, you would have to alter the table to accommodate the attributes that describe these new objects. This can quickly become problematic when you have many subtypes, or if you plan to add subtypes on a regular basis.
The database will not be able to enforce which attributes apply and which don't, since there is no metadata to define which attributes belong to which subtypes. 
You also cannot enforce NOT NULL on attributes of a subtype that should be mandatory. You would have to handle this in your application, which in general is not ideal.

Concrete Table Inheritance:
Another approach to tackle inheritance is to create a new table for each subtype, repeating all the common attributes in each table. For example:
--// Table: policies_motor
+------+---------------------+----------------+
| id   | date_issued         | vehicle_reg_no |
+------+---------------------+----------------+
|    1 | 2010-08-20 12:00:00 | 01-A-04004     |
|    2 | 2010-08-20 13:00:00 | 02-B-01010     |
|    3 | 2010-08-20 15:00:00 | 03-C-02020     |
+------+---------------------+----------------+

--// Table: policies_property    
+------+---------------------+------------------+
| id   | date_issued         | property_address |
+------+---------------------+------------------+
|    1 | 2010-08-20 14:00:00 | Oxford Street    |   
+------+---------------------+------------------+

This design will basically solve the problems identified for the single table method:

Mandatory attributes can now be enforced with NOT NULL.
Adding a new subtype requires adding a new table instead of adding columns to an existing one. 
There is also no risk that an inappropriate attribute is set for a particular subtype, such as the vehicle_reg_no field for a property policy. 
There is no need for the type attribute as in the single table method. The type is now defined by the metadata: the table name.

However this model also comes with a few disadvantages:

The common attributes are mixed with the subtype specific attributes, and there is no easy way to identify them. The database will not know either.
When defining the tables, you would have to repeat the common attributes for each subtype table. That's definitely not DRY.
Searching for all the policies regardless of the subtype becomes difficult, and would require a bunch of UNIONs. 

This is how you would have to query all the policies regardless of the type:
SELECT     date_issued, other_common_fields, 'MOTOR' AS type
FROM       policies_motor
UNION ALL
SELECT     date_issued, other_common_fields, 'PROPERTY' AS type
FROM       policies_property;

Note how adding new subtypes would require the above query to be modified with an additional UNION ALL for each subtype. This can easily lead to bugs in your application if this operation is forgotten. 
Class Table Inheritance (aka Table Per Type Inheritance):
This is the solution that @David mentions in the other answer. You create a single table for your base class, which includes all the common attributes. Then you would create specific tables for each subtype, whose primary key also serves as a foreign key to the base table. Example:
CREATE TABLE policies (
   policy_id          int,
   date_issued        datetime,

   -- // other common attributes ...
);

CREATE TABLE policy_motor (
    policy_id         int,
    vehicle_reg_no    varchar(20),

   -- // other attributes specific to motor insurance ...

   FOREIGN KEY (policy_id) REFERENCES policies (policy_id)
);

CREATE TABLE policy_property (
    policy_id         int,
    property_address  varchar(20),

   -- // other attributes specific to property insurance ...

   FOREIGN KEY (policy_id) REFERENCES policies (policy_id)
);

This solution solves the problems identified in the other two designs:

Mandatory attributes can be enforced with NOT NULL.
Adding a new subtype requires adding a new table instead of adding columns to an existing one. 
No risk that an inappropriate attribute is set for a particular subtype.
No need for the type attribute.
Now the common attributes are not mixed with the subtype specific attributes anymore.
We can stay DRY, finally. There is no need to repeat the common attributes for each subtype table when creating the tables.
Managing an auto incrementing id for the policies becomes easier, because this can be handled by the base table, instead of each subtype table generating them independently.
Searching for all the policies regardless of the subtype now becomes very easy: No UNIONs needed - just a SELECT * FROM policies.

I consider the class table approach as the most suitable in most situations.

The names of these three models come from Martin Fowler's book Patterns of Enterprise Application Architecture.",https://stackoverflow.com/questions/3579079/how-can-you-represent-inheritance-in-a-database/3579462,7,1
How can you represent inheritance in a database?,"I'm thinking about how to represent a complex structure in a SQL Server database.
Consider an application that needs to store details of a family of objects, which share some attributes, but have many others not common. For example, a commercial insurance package may include liability, motor, property and indemnity cover within the same policy record.
It is trivial to implement this in C#, etc, as you can create a Policy with a collection of Sections, where Section is inherited as required for the various types of cover. However, relational databases don't seem to allow this easily.
I can see that there are two main choices:

Create a Policy table, then a Sections table, with all the fields required, for all possible variations, most of which would be null.  
Create a Policy table and numerous Section tables, one for each kind of cover.

Both of these alternatives seem unsatisfactory, especially as it is necessary to write queries across all Sections, which would involve numerous joins, or numerous null-checks.
What is the best practice for this scenario?","@Bill Karwin describes three inheritance models in his SQL Antipatterns book, when proposing solutions to the SQL Entity-Attribute-Value antipattern. This is a brief overview:
Single Table Inheritance (aka Table Per Hierarchy Inheritance):
Using a single table as in your first option is probably the simplest design. As you mentioned, many attributes that are subtype-specific will have to be given a NULL value on rows where these attributes do not apply. With this model, you would have one policies table, which would look something like this:
+------+---------------------+----------+----------------+------------------+
| id   | date_issued         | type     | vehicle_reg_no | property_address |
+------+---------------------+----------+----------------+------------------+
|    1 | 2010-08-20 12:00:00 | MOTOR    | 01-A-04004     | NULL             |
|    2 | 2010-08-20 13:00:00 | MOTOR    | 02-B-01010     | NULL             |
|    3 | 2010-08-20 14:00:00 | PROPERTY | NULL           | Oxford Street    |
|    4 | 2010-08-20 15:00:00 | MOTOR    | 03-C-02020     | NULL             |
+------+---------------------+----------+----------------+------------------+

\------ COMMON FIELDS -------/          \----- SUBTYPE SPECIFIC FIELDS -----/

Keeping the design simple is a plus, but the main problems with this approach are the following:

When it comes to adding new subtypes, you would have to alter the table to accommodate the attributes that describe these new objects. This can quickly become problematic when you have many subtypes, or if you plan to add subtypes on a regular basis.
The database will not be able to enforce which attributes apply and which don't, since there is no metadata to define which attributes belong to which subtypes. 
You also cannot enforce NOT NULL on attributes of a subtype that should be mandatory. You would have to handle this in your application, which in general is not ideal.

Concrete Table Inheritance:
Another approach to tackle inheritance is to create a new table for each subtype, repeating all the common attributes in each table. For example:
--// Table: policies_motor
+------+---------------------+----------------+
| id   | date_issued         | vehicle_reg_no |
+------+---------------------+----------------+
|    1 | 2010-08-20 12:00:00 | 01-A-04004     |
|    2 | 2010-08-20 13:00:00 | 02-B-01010     |
|    3 | 2010-08-20 15:00:00 | 03-C-02020     |
+------+---------------------+----------------+

--// Table: policies_property    
+------+---------------------+------------------+
| id   | date_issued         | property_address |
+------+---------------------+------------------+
|    1 | 2010-08-20 14:00:00 | Oxford Street    |   
+------+---------------------+------------------+

This design will basically solve the problems identified for the single table method:

Mandatory attributes can now be enforced with NOT NULL.
Adding a new subtype requires adding a new table instead of adding columns to an existing one. 
There is also no risk that an inappropriate attribute is set for a particular subtype, such as the vehicle_reg_no field for a property policy. 
There is no need for the type attribute as in the single table method. The type is now defined by the metadata: the table name.

However this model also comes with a few disadvantages:

The common attributes are mixed with the subtype specific attributes, and there is no easy way to identify them. The database will not know either.
When defining the tables, you would have to repeat the common attributes for each subtype table. That's definitely not DRY.
Searching for all the policies regardless of the subtype becomes difficult, and would require a bunch of UNIONs. 

This is how you would have to query all the policies regardless of the type:
SELECT     date_issued, other_common_fields, 'MOTOR' AS type
FROM       policies_motor
UNION ALL
SELECT     date_issued, other_common_fields, 'PROPERTY' AS type
FROM       policies_property;

Note how adding new subtypes would require the above query to be modified with an additional UNION ALL for each subtype. This can easily lead to bugs in your application if this operation is forgotten. 
Class Table Inheritance (aka Table Per Type Inheritance):
This is the solution that @David mentions in the other answer. You create a single table for your base class, which includes all the common attributes. Then you would create specific tables for each subtype, whose primary key also serves as a foreign key to the base table. Example:
CREATE TABLE policies (
   policy_id          int,
   date_issued        datetime,

   -- // other common attributes ...
);

CREATE TABLE policy_motor (
    policy_id         int,
    vehicle_reg_no    varchar(20),

   -- // other attributes specific to motor insurance ...

   FOREIGN KEY (policy_id) REFERENCES policies (policy_id)
);

CREATE TABLE policy_property (
    policy_id         int,
    property_address  varchar(20),

   -- // other attributes specific to property insurance ...

   FOREIGN KEY (policy_id) REFERENCES policies (policy_id)
);

This solution solves the problems identified in the other two designs:

Mandatory attributes can be enforced with NOT NULL.
Adding a new subtype requires adding a new table instead of adding columns to an existing one. 
No risk that an inappropriate attribute is set for a particular subtype.
No need for the type attribute.
Now the common attributes are not mixed with the subtype specific attributes anymore.
We can stay DRY, finally. There is no need to repeat the common attributes for each subtype table when creating the tables.
Managing an auto incrementing id for the policies becomes easier, because this can be handled by the base table, instead of each subtype table generating them independently.
Searching for all the policies regardless of the subtype now becomes very easy: No UNIONs needed - just a SELECT * FROM policies.

I consider the class table approach as the most suitable in most situations.

The names of these three models come from Martin Fowler's book Patterns of Enterprise Application Architecture.",https://stackoverflow.com/questions/3579079/how-can-you-represent-inheritance-in-a-database/3579139,7,1
Replacing static variables in .net web application,"I don't have a problem, but I'm trying to prevent a problem from occurring in the future and would like a little help figuring out the proper way to approach this. I have a vendor who will be posting XML to a webpage that I have created. This would happen potentially every 2 minutes. The web page will then read the XML posted and insert into a Database. 
Serializing the XML and inserting is not good enough so I have to do a little bit of parsing before I send the data to the database. I created a static class that looks like the following.
using System;
using System.Collections.Generic;
using System.Linq;
using System.Web;

/// <summary>
/// Summary description for Accounting
/// </summary>
public static class Accounting
{
    public static String currentFile = """";
    public static List<string> PurchaseOrder = new List<string>();
    public static List<string> item = new List<string>();
    public static List<string> unitPrice = new List<string>();
    public static List<string> shippingCharge = new List<string>();
    public static List<string> handlingCharge = new List<string>();
    public static List<string> discountAmount = new List<string>();
    public static List<string> UOM = new List<string>();
    public static List<string> invoiceNumber = new List<string>();
    public static List<string> supplierNumber = new List<string>(); //{ get; set; }
    public static List<string> supplierInvoiceNo = new List<string>();
    public static List<string> account = new List<string>();
    public static List<string> fund = new List<string>();
    public static List<string> org = new List<string>();
    public static List<string> prog = new List<string>();
    public static List<string> activity = new List<string>();
    public static List<string> location = new List<string>();
    public static List<string> distributionType = new List<string>();
    public static List<string> distributionValue = new List<string>();
    public static List<int> sequence = new List<int>();
    public static List<string> quantity = new List<string>();
    public static List<string> dueDate = new List<string>();
}

This class is static because I need to operate on the variables from different methods. 
Now here is where things get tricky, I keep reading that if I use static variable in a .NET web application I will end up sharing the variable across sessions. This could potentially be a nightmare for me. so I looked into the singleton pattern and I have also found this Static variables in web applications solution throught stackover flow.
   I read that I should do something like this
public class SingletonPerRequest
{
    public static SingletonPerRequest Current
    {
        get
        {
            return (HttpContext.Current.Items[""SingletonPerRequest""] ??
                (HttpContext.Current.Items[""SingletonPerRequest""] = 
                new SingletonPerRequest())) as SingletonPerRequest;

        }
    }
}

My problem is that I don't know how can I convert my class to basically do what the above is doing. In other words how do I set class to follow this singleton request pattern. Forgive my stupidity if this does not make sense. My ultimate goal would be to have every request to the page have its own session of accounting variables and not get shared across session. 
Any help would be greatly appreciated.","Here is your class:
using System;
using System.Collections.Generic;
using System.Linq;
using System.Web;

public class Accounting
{
    public static Accounting Current
    {
        get
        {
            return (HttpContext.Current.Items[""Accounting""] ??
                (HttpContext.Current.Items[""Accounting""] = 
                new Accounting())) as Accounting;

        }
    }

    public String currentFile = """";
    public List<string> PurchaseOrder = new List<string>();
    public List<string> item = new List<string>();
    public List<string> unitPrice = new List<string>();
    public List<string> shippingCharge = new List<string>();
    public List<string> handlingCharge = new List<string>();
    public List<string> discountAmount = new List<string>();
    public List<string> UOM = new List<string>();
    public List<string> invoiceNumber = new List<string>();
    public List<string> supplierNumber = new List<string>(); //{ get; set; }
    public List<string> supplierInvoiceNo = new List<string>();
    public List<string> account = new List<string>();
    public List<string> fund = new List<string>();
    public List<string> org = new List<string>();
    public List<string> prog = new List<string>();
    public List<string> activity = new List<string>();
    public List<string> location = new List<string>();
    public List<string> distributionType = new List<string>();
    public List<string> distributionValue = new List<string>();
    public List<int> sequence = new List<int>();
    public List<string> quantity = new List<string>();
    public List<string> dueDate = new List<string>();
}

You can use the singleton like that: Accounting::Current.quantity
Note:

Using a static class is an antipattern in OOP. A static class is like a namespaced collection of method. You cannot instantiate objects that preventing you from using inheritance, interfaces, low coupling, dependency injection, ... Always use a singleton instead. This will help you to make a better architecture. You also should take a look at SOA (Service Oriented Architecture) in OOP and then in HTTP (REST webservices).",https://stackoverflow.com/questions/27345258/replacing-static-variables-in-net-web-application/27345483,2,1
Practice your micro-architectural skills [closed],"I've googled a lot and can't find what I look for. 
I look for some architecture practice. I mean there are a lot of books about Design Patterns, but I want something like analysis of common mistakes in architecture of EE applications. All I've found - antipatterns like string concatenation or something else that can be found with help of FindBug or Sonar.
How I figure it out:

Book with next steps: task definition, wrong decision, why it is bad, right decision.
Educational resources. I heard there are such resources for testers. Some applications are opened for testing and each who want to learn testing can test it; and after some period discuss own result with other people or see the percent of bugs he has found.
Maybe other ideas?

Why I think Design Pattern books are not suitable for me:
A developer may know many design patterns from such books, but can be incapable of selecting the correct one for the specific situation. IMHO, this is because these books don't give you any practice, and fail to educate the reader as to which design pattern(s) should be applied to any situation. Those books just get you a ready solution. 
EDIT:
There aren't any answers any more. So I want to expand my question: 
I believe, no I'm certain that exist courses dedicated to improve architecture skills, show the common mistakes in designing of web applications and so on. Also I know that there are a lot of conferences linked with this subject.
Advice me where should I look for them, please.","Holub on Patterns is a fresh and interesting perspective on design patterns.
Lots of code. Lots of pro and cons, very hands-on and practical. I learn something new every time I re-read it.
It has been my go-to book as the ""next step after GoF and Head-First Design Patterns"". I love it, and it has been very well recieved by the ones who have tried it.",https://stackoverflow.com/questions/20002524/practice-your-micro-architectural-skills,10,1
Gather info from xhtml in java: parser + visitor?,"I have to write a piece of code that loads a remote web page, search for the links, visit those pages and gather some info from certain tags...
How would you do this? Is the visitor pattern of any help here? If so, how could I use it?
Thanks","Some comments/suggestions

Not sure if the visitor patter is a good fit over here. A typical scenario for the visitor pattern is when the operation algorithm differs depending on the object on which the algorithm is applied. 
The crude way to solve this is to embed the algorithm in the concerned Object itself but this amounts to mixing data and operation (against the spirit of Separation of Concern) 
Visitor pattern helps us here to separate the the algorithm from the data on which its applied. 
Please check out an example for better understanding of visitor pattern. 

In your case

Objects are the web page, Links and Operations are  Visit, parse, extract information. 
The same set of operations are applied on all the web pages and links. 
So here the operation algorithm is not changing for different web pages and links and hence the visitor pattern is not suitable.
Technically you can still use visitor pattern, but that's not what it is for.  

For you problem,

I think its not very complicated design problem. Some patterns might seem to solve the problem like Command Pattern ( Commands: extractLinkFromPage, visitLinkAndParseTags), but IMO, it will be overkill for this simple problem. 
I would suggest a simple way of hosting the logic in a utility class and using the same from your calling program,


 class WebUtility{
 public List<String> parseLinks(String remotePageAddress){
 //Parse links
 }   
 public TageInfo extractTageInfo(String pageURL){
 //Extract the Tag information 
 }
 }


Here the TagInfo class will be a pojo as per your requirement. 
This class is stateless and can be used as singleton. Optionally you can make the constructor private and method static. 
Once you have this, you can invoke parseLinks to get the links and then loop through the list of links to get the tag information from each link by invoking extractTageInfo method.",https://stackoverflow.com/questions/16717209/gather-info-from-xhtml-in-java-parser-visitor/16718848,1,1
Long delegation chains in C++,"This is definitely subjective, but I'd like to try to avoid it
  becoming argumentative. I think it could be an interesting question if
  people treat it appropriately.

In my several recent projects I used to implement architectures where long delegation chains are a common thing.
Dual delegation chains can be encountered very often:
bool Exists = Env->FileSystem->FileExists( ""foo.txt"" );

And triple delegation is not rare at all:
Env->Renderer->GetCanvas()->TextStr( ... );

Delegation chains of higher order exist but are really scarce.
In above mentioned examples no NULL run-time checks are performed since the objects used are always there and are vital to the functioning of the program and
explicitly constructed when execution starts. Basically I used to split a delegation chain in these cases:
1) I reuse the object obtained through a delegation chain:
{ // make C invisible to the parent scope
   clCanvas* C = Env->Renderer->GetCanvas();
   C->TextStr( ... );
   C->TextStr( ... );
   C->TextStr( ... );
}

2) An intermediate object somewhere in the middle of the delegation chain should be checked for NULL before usage. Eg.
clCanvas* C = Env->Renderer->GetCanvas();

if ( C ) C->TextStr( ... );

I used to fight the case (2) by providing proxy objects so that a method can be invoked on non-NULL object leading to an empty result.
My questions are: 

Is either of cases (1) or (2) a pattern or an antipattern?
Is there a better way to deal with long delegation chains in C++?

Here are some pros and cons I considered while making my choice:
Pros:

it is very descriptive: it is clear out of 1 line of code where did the object came from
long delegation chains look nice

Cons:

interactive debugging is labored since it is hard to inspect more than one temporary object in the delegation chain

I would like to know other pros and cons of the long delegation chains. Please, present your reasoning and vote based on how well-argued opinion is and not how well you agree with it.","I wouldn't go so far to call either an anti-pattern. However, the first has the disadvantage that your variable C is visible even after it's logically relevant (too gratuitous scoping).
You can get around this by using this syntax:
if (clCanvas* C = Env->Renderer->GetCanvas()) {
  C->TextStr( ... );
  /* some more things with C */
}

This is allowed in C++ (while it's not in C) and allows you to keep proper scope (C is scoped as if it were inside the conditional's block) and check for NULL.
Asserting that something is not NULL is by all means better than getting killed by a SegFault. So I wouldn't recommend simply skipping these checks, unless you're a 100% sure that that pointer can never ever be NULL.

Additionally, you could encapsulate your checks in an extra free function, if you feel particularly dandy:
template <typename T>
T notNULL(T value) {
  assert(value);
  return value;
}

// e.g.
notNULL(notNULL(Env)->Renderer->GetCanvas())->TextStr();",https://stackoverflow.com/questions/11631513/long-delegation-chains-in-c,8,1
Repository pattern - how to correctly handle JOINs and complex queries?,"I have a problem with Repository pattern - how to perform JOIN operations between several repositories. In this project, we use MVC, EF, DDD. I'm aware that this kind of question was here several times, I reference these questions later in this one.
Between generic repository model (IRepository) and specific repository model, I chose specific option, since I consider ORM (in our case EF) as a generic repository pattern itself, so it doesn't make sense to add another generic repository and we'd rather tailor the repository to the domain needs.
The problem is that I have several (~ 10) tables, each with many rows (millions), and I need to perform JOINs, so using IList or IEnumerable isn't viable option.
My understanding (and my perspective) is that IQueryable shouldn't leave the repository (""What happens in DAL, should stay in DAL.""). It would be way simpler to expose IQueryable and use it in LINQ in service, but it strongly violates separation of concerns and undermines role of repositories - in such case, service will be doing the same thing as repository does. To pick a few, these articles back up this perspective (or rather conviction):
To return IQueryable<T> or not return IQueryable<T>
Should I return IEnumerable<T> or IQueryable<T> from my DAL?
http://www.shawnmclean.com/blog/2011/06/iqueryable-vs-ienumerable-in-the-repository-pattern/
http://blog.ploeh.dk/2012/03/26/IQueryableTisTightCoupling/
There are also similar questions and solutions, e.g. How to join Multiple tables using Repository Pattern & Entity Framework? that suggest .Include(), but this is not an option for heavy loaded tables and joins across many tables - with each JOIN, we use subselects to limit what's actually joined).
This question (the answer and comments) - How can I query cross tables with Repository Pattern? - basically suggests task-based differentiation: creating one Repository for queries with JOINS, and ""regular"" Repositories for manipulation with each entity.
I see we have these options:

Exposing IQueryable and performing JOIN complex queries in services; I sincerely feel it's anti-pattern, I don't like that.
Don't use Repository for these ~ 10 tables and perform queries in services; some articles suggested that using EF is enough (e.g. Is it okay to bypass the repository pattern for complex queries?), I don't concur with that.
Use task-based differentiation, don't limit repositories 1:1 repo:entity (I'm in favor of this option)
Something completely different?

So - what would you suggest? Again and again, thank you.","means leaking persistence into application - antipattern, spaghetti-code.
how it differs from (1) exactly? Still the same issue.
a bit closer...
Use Query Object pattern. Encapsulate your complex query in a task-based object that resides alongside repositories. It can return DTOs optimized for view rather than domain objects.
Relying heavily on QO will lead you to an architecture called CQRS - Command-Query Responsibility Segregation.

One more thing. There is not 1:1 match for entity:repo. Only Aggregates should have a repository, not every single entity.",https://stackoverflow.com/questions/21257555/repository-pattern-how-to-correctly-handle-joins-and-complex-queries/21258130,1,1
To return IQueryable<T> or not return IQueryable<T>,"I have a repository class that wraps my LINQ to SQL Data Context. The repository class is a business line class that contains all the data tier logic (and caching and such).
Here's my v1 of my repo interface.
public interface ILocationRepository
{
    IList<Location> FindAll();
    IList<Location> FindForState(State state);
    IList<Location> FindForPostCode(string postCode);
}

But to handle paging for FindAll, I'm debating whether or not to expose IQueryable<ILocation> instead of IList to simplify the interface for circumstances such as paging.
What are the pros and cons to exposing IQueryable from the data repo?
Any help is very much appreciated.","The pros; composability:

callers can add filters
callers can add paging
callers can add sorting
etc

The cons; non-testability:

Your repository is no longer properly unit testable; you can't rely on a: it working, b: what it does;


the caller could add a non-translatable function (i.e. no TSQL mapping; breaks at runtime)
the caller could add a filter/sort that makes it perform like a dog

Since callers expect IQueryable<T> to be composable, it rules out non-composable implementations - or it forces you to write your own query provider for them
it means you can't optimize / profile the DAL

For stability, I've taken to not exposing IQueryable<T> or Expression<...> on my repositories. This means I know how the repository behaves, and my upper layers can use mocks without worrying ""does the actual repository support this?"" (forcing integration tests).
I still use IQueryable<T> etc inside the repository - but not over the boundary. I posted some more thoughts on this theme here. It is just as easy to put paging parameters on the repository interface. You can even use extension methods (on the interface) to add optional paging parameters, so that the concrete classes only have 1 method to implement, but there may be 2 or 3 overloads available to the caller.",https://stackoverflow.com/questions/718624/to-return-iqueryablet-or-not-return-iqueryablet,3,1
Should I return IEnumerable<T> or IQueryable<T> from my DAL?,"I know this could be opinion, but I'm looking for best practices.
As I understand, IQueryable<T> implements IEnumerable<T>, so in my DAL, I currently have method signatures like the following:
IEnumerable<Product> GetProducts();
IEnumerable<Product> GetProductsByCategory(int cateogoryId);
Product GetProduct(int productId);

Should I be using IQueryable<T> here?
What are the pros and cons of either approach? 
Note that I am planning on using the Repository pattern so I will have a class like so:
public class ProductRepository {

    DBDataContext db = new DBDataContext(<!-- connection string -->);

    public IEnumerable<Product> GetProductsNew(int daysOld) {
        return db.GetProducts()
          .Where(p => p.AddedDateTime > DateTime.Now.AddDays(-daysOld ));
    }
}

Should I change my IEnumerable<T> to IQueryable<T>? What advantages/disadvantages are there to one or the other?","It depends on what behavior you want.

Returning an IList<T> tells the caller that they've received all of the data they've requested
Returning an IEnumerable<T> tells the caller that they'll need to iterate over the result and it might be lazily loaded.
Returning an IQueryable<T> tells the caller that the result is backed by a Linq provider that can handle certain classes of queries, putting the burden on the caller to form a performant query.

While the latter gives the caller a lot of flexibility (assuming your repository fully supports it), it's the hardest to test and, arguably, the least deterministic.",https://stackoverflow.com/questions/3039262/should-i-return-ienumerablet-or-iqueryablet-from-my-dal,4,1
How to join Multiple tables using Repository Pattern & Entity Framework?,"I need to join multiple tables using repository pattern & Entity Framework (using C#). Is this possible? If so, please let me know how to do the same.","In EF, joining tables is done through the use of Navigation Properties. Basically, EF does it for you. When implementing in your Repositories, may it be Generic or not, you can call the Include method when building your query expression to tell EF to populate the navigation properties for you. 
Let's say we have these POCO class:
public class Dog
{
    public int DogId { get; set; }
    public string Name { get; set; }

    public int OwnerId { get; set;}
    public Owner Owner { get; set; } // the navigation property
}

public class Owner
{
    public int OwnerId { get; set; }
    public string Name { get; set; }

    // another navigation property
    // all the dogs that are related or owned by this specific owner
    public ICollection<Dog> DogList { get; set; } 
    public ICollection<Cat> CatList { get; set; }
}

Here's a sample code snippet using Include:
public virtual IEnumerable<Dog> Retrieve()
{
    var _query = context.Dog.Include(a => a.Owner);
    ...
    ...// rest of your code
}

And for multiple tables you can nest the include method like so:
public virtual IEnumerable<Owner> Retrieve()
{
    // you can nest as many as you want if there are more nav properties
    var _query = context.Owner
        .Include(a => a.DogList)
        .Include(a => a.CatList); 
    ...
    ...// rest of your code
}

Once you include nav properties then that is basically joining those other tables. Just look at the SQL being generated by the query. Hope this helps!",https://stackoverflow.com/questions/20623022/how-to-join-multiple-tables-using-repository-pattern-entity-framework,1,1
How can I query cross tables with Repository Pattern?,"In my asp.net mvc 3 application, I'm using the repository pattern.
I have 3 entities, Company, Country, City. Each of them has their own repository. Company entity has FoundedCountry and FoundedCity foreign keys.
Now in a view, I want to show the company details. In this view I want to view Company details as well as, FoundedCountry name and FoundedCity name. In my opinion I have to handle this with a kind of JOIN query. But I'm stuck at how to achieve this in repository pattern. How can I handle this JOIN in repository pattern?
Thank you.","The repository should have a task-based interface. This means that ORM's, joins etc are inside the repository. The app just sees an interface whtch returns an object that it can use.
This means you don't create a repository around a table (it pretty much defeats the purpose). In your scenario I suggest you have (at least) 2 repositories: one will handle everything related to updating the model and the other will serve only reads (queries).
This means the query repository will return only the data you want (it basically returns view model bits). Of course, the actual tables and joins are an implementation detail of the repository.",https://stackoverflow.com/questions/10288695/how-can-i-query-cross-tables-with-repository-pattern,2,1
Is it okay to bypass the repository pattern for complex queries?,"This is my understanding about DDD at the moment:

The strict repository pattern should only implement get(), delete() and create(), and maybe variants of get() where one can search or to retrieve an entire collection
It is common for each aggregate root to have one repository

(from research, I know those are not universally accept norms)
The question here is how to implement complex queries which involves many aggregate roots. For example, we have two aggregate roots - product and user. If I am doing a page which list what products a user have bought, then I have a query which stretch across both the user aggregate and the product aggregate.
How should this query be implemented? 

What I am doing now is actually to have a repository for this query and queries with related functionality (some will disagree and say the repository is not a query layer). 
Use only the repository for product and user, grab all records and do everything in memory (this sounds wrong)
Have the query (LINQ or SQL) to be inside the service, not using the repository associated with the aggregates at all.

Are there some other ways?","The strict repository pattern should only implement get(), delete()
  and create(), and maybe variants of get() where one can search or to
  retrieve an entire collection

Repository interface is part of your domain and should be based on Ubiquitous Language as much as possible. All Repositories are different just like all your Aggregates are different. Strict, generic repositories are CRUD overgeneralization and may reduce code expressiveness. 'Create' method also does not belong to Repository because beginning of object life cycle is usually handled by Factory or Object itself. 'Add' seems like a better name when you want to persist existing object because Repository has a collection semantics.

The question here is how to implement complex queries which involves
  many aggregate roots. For example, we have two aggregate roots -
  product and user. If I am doing a page which list what products a user
  have bought, then I have a query which stretch across both the user
  aggregate and the product aggregate.

In this case you just have to listen to the business requirements, I emphasized the part that I think is most important. Based on that it looks like you need:
Products products = /* get Products repository implementation */;
IList<Product> res = products.BoughtByUser(User user);

The idea of organizing code like that is to match business requirements and ubiquitous language as much as possible. The naming of the repository interfaces is also important, I prefer to have Products or AllProducts instead of ProductsRepository. Phil Cal莽ado has a very good article on this subject, highly recommended.
How should this query be implemented?

There is nothing special about this query, it can be implemented just like all other queries in Products repository. The querying itself is hidden from Domain because Repository implementation belongs to Data Access layer. Data Access can implement any query because it has intimate knowledge of all the Aggregates and their relationships. At this point it would just be a Hibernate or SQL question.",https://stackoverflow.com/questions/7291692/is-it-okay-to-bypass-the-repository-pattern-for-complex-queries,3,1
Problems with circular dependency and OOP in AngularJS,"AngularJS + OOP is kinda sexy feature to use
Hi, I'm successfully using OOP with AngularJs for some time already (first started with  angularjs with oop inheritance in action), the provided approach allows you define your classes as angular services, which you can later extend or inherit from like that: 
Application.factory('AbstractObject', [function () {
    var AbstractObject = Class.extend({
        virtualMethod: function() {
           alert(""Hello world"");
        },
        abstractMethod: function() { // You may omit abstract definitions, but they make your interface more readable
           throw new Error(""Pure abstract call"");
        }
    });

    return AbstractObject; // You return class definition instead of it's instance
}]);

Application.factory('DerivedObject', ['AbstractObject', function (AbstractObject) {
    var DerivedObject = AbstractObject.extend({
        virtualMethod: function() { // Shows two alerts: `Hey!` and `Hello world`
            alert(""Hey!"");

            this._super();
        },
        abstractMethod: function() {
            alert(""Now I'm not abstract"");
        }
    });

    return DerivedObject;
}]);

Plunker: http://plnkr.co/edit/rAtVGAsNYggBhNADMeoT
using the described approach gives you the ability to define classes that beautifully integrate into angular infrastructure. You get all sort of nifty features from two worlds - OOP and AngularJs. Dependency injection is free for your classes, and it makes your classes simple, allows putting a lot of boilerplate controller code into some base class that can be later reused.
However
AngularJs infrastructure blocks previously described approach from spreading it's wings on all 100%. The problem occurs when you try to define recursive class definitions (i.e. recursive aggregation), say you have two class definitions like Blog and Tag
Application.factory('Blog', ['Tag', function (Tag) {
    var Blog = Class.extend({
        tags: function() {
            return this.tags;
        }
    });

    return Blog;
}]);

Application.factory('Tag', ['Blog', function (Blog) {
    var Tag = Class.extend({
        Blogs: function() {
           return this.blogs;
        }
    });

    return Tag;
}]);

It won't work because both Blog and Tag are self-referencing themselves causing circular dependency.
P.S
The last thing, I have found kinda ugly solution that solves my problem in my specific case but doesn't work in general and as I said, it isn't pretty:
Application.factory('BlogNamespace', [function () {
    var Blog = Class.extend({
        tags: function() {
            return this.tags;
        }
    });

    var Tag = Class.extend({
        Blogs: function() {
           return this.blogs;
        }
    });

    return {
        Tag: Tag,
        Blog: Blog
    };
}]);

Question
The above fix won't work because namespaces may also be a subject of circular dependency. This means that it isn't solution to described problem but rather one level deeper problem now.
Any suggestions on how it is possible to solve described problem in general case?","A circular dependency is always the sign of mixing of concerns, which is a really bad thing. Mi拧ko Hevery, one of the authors of AngularJS, explains a nice solution on his awesome blog. In short, you probably have a third service hidden somewhere, which is the only part of your code really needed by the two others.",https://stackoverflow.com/questions/19344214/problems-with-circular-dependency-and-oop-in-angularjs/19368367,3,1
angularjs with oop inheritance in action,"Abstract
I'm working on an application that uses angular as a client side framework, angular currently rocks and I'm really happy using it, though now I find that I use to much copy and paste code that I would like to organize into class hierarchy. For example dialogs share a common set of functionality, they need to be opened, closed, the code that provides typeahead functionality is also a first candidate to inherit from some parent BaseTypeaheadClass, though  one thing I didn't find in angular is a standard way of organising these hierarchies. Both controllers, services, providers use ordinary javascript functions underneath which can be extended by the means of prototype, so my question is:
Question
What is the angular way of organising my class functions, are there any standard mechanisms that will allow to derive one class from another
P.S.
My guesses on the problem:

Define implementation of base classes as services, as a result they will be easily injected into any controller or other services where that specific class will be needed
Define OOP service and provide methods such as define, derive, etc. that will be used to create base / derived classes


Edit
Some time has passed from time when I was initially asking my question. Since then I have come out with approach that I'm successfully using in several projects, that I like very much and want to share with everyone.
Currently angular doesn't provide any constructs for organising class hierarchies and it's a pity since more or less large application can't suffice only Model/View/Controller/... constructs, it has to organise it's code into OOP objects.
I'm working in the field of web-development for quite a long time already and I haven't seen even one enterprise project that was taking advantage of OOP with JavaScript massively. What I seen was huge and nicely organised server side / database side logic + close to infinite javascript spaghetti greased with zoo of frameworks and libraries on client side.
No MVVM, MVP frameworks such as knockout.js, backbone, other... are capable of replacing the OOP as such. If you are not using core principles of oriented programming such as Classes, Objects, Inheritance, Abstraction, Polymorphism you are in deep trouble, what you will end up is a mega long javascript spaghetti.
Regarding Angular I think it is a framework very much different from knockout.js / backbone.js / any other MVV-anything frameworks but according to my practice also it is not a silver bullet capable of replacing OOP. When I'm trying not to use the OOP with Angular I end up with duplicate logic located mostly in controllers. And unfortunately there is no (I have found no) clean and angular-way of beating that problem.
But I have successfully (I think) solved that problem.
I've used compact, zero-dependency lib that just implements John Resig's Simple JavaScript Inheritance (https://github.com/tracker1/core-js/blob/master/js-extensions/040-Class.js). With the help of that library I was able to create / inherit / create abstract methods / override them, in other words do everything that I've accustomed to on server side. 
Here is an example usage:
Application.factory('SomeChildObject', ['$http', 'SomeParentClass', function ($http, SomeParentClass) {
    var SomeChildClass = SomeParentClass.extend({
        init: function() { // Constructor
            this._super.init(123, 231); // call base constructor
        },
        someFunction: function() {
            // Notice that your OOP now knows everything that can be injected into angular service, which is pretty cool :)
            $http({method: 'GET', url: '/someUrl'}).then(function(){
                this._super.someFunction(); // call base function implementation
            });
        }
    });

    // return new SomeChildClass(); // We are not returning instance here!

    return SomeChildClass; // Service is a function definition not an instance of an object
}]);

// So now we can both use this service in angular and have the ability to extend it using the `extend` method call, like so:
Application.controller('MegaController', ['$scope', 'SomeChildClass', function ($scope, SomeChildClass) {
    $scope.someObject = new SomeChildClass();
}]);

OOP + Angular play together very nicely, objects created under angular context can take advantage of dependency injection via services automatically, so you don't have to inject instances into your OOP constructors and this fact makes your OOP hierarchy very slim and free of irrelevant stuff that needs to be (and is) handled by angular.js
So play with this approach and give feedback here with results you gained or problems you encountered,
Another edit
Recently I've faced few problems with original Class.js implementation, as follows:
1) If you will be passing a reference to your instance methods as callbacks to other methods, these methods might work not the way you expect them to work. They will loose reference to this. In such case you will be expecting to see your current object inside this but it will be either top level Window or some other context object depending on how the callback calls your method. It happens due to JavaScript architecture. In order to fight this problem a special ClassMember function is provided which instructs Class to bind your method to object context when it is being created (check Usage below for further guidance).
2) Obviously original Class.js implementation doesn't know anything about angular type of controller method declarations i.e. 
Class.extend('YourClassDisplayName', {
    ctor: function () {
        // Some useful constructor logic
    },
    controller: ['$scope', '$attrs', function ($scope, $attrs) {
        // Do something with $scope and $attrs
    }]
});

Current implementation understands above syntax
3) When using above approach without appropriate handling it would break angular $$annotate'on process so referring to above example it would make impossible to inject $scope and $attrs into into ClassMember method, or overridden method which is using this.base(...) calls. So this is also fixed.
Gotchas:
1) When using this.base(...) within async operation handler (something like $http.get(..., function() { self.base(...); })) please note that this.base(...) call has a limited lifetime and as soon as the method returns this.base(...) stops existing. So you should save reference to base method explicitly if you are planning to call base methods in asynchronous fashion. i.e:
...
var self = this;
var base = this.base;
...
$http.get(..., function () {
    base.call(self, ...); // or base.apply(self, ...), or base() if you don't care about `this`
})

I've resolved all of the above problems (except one gotcha which can not be resolved due to JavaScript architecture) and would like to share with everyone, hope you will benefit from it:
/* Simple JavaScript Inheritance
 * By John Resig http://ejohn.org/
 * MIT Licensed.
 *
 * Inspired by base2 and Prototype

 * Angular adaptations by Denis Yaremov http://github.com/lu4
 * Usage:
 ---------------------------------

   var X = Class.extend('X', {
       ctor: function () {
           this.name = ""I'm X"";
       },

       myOrdinaryMethod: function (x, y, z) {
           console.log([this.name, x, y, z]);
       },

       myClassMemberMethod: ClassMember(function (x, y, z) {
           console.log([this.name, x, y, z]);
       })
   });

   var Y = Class.extend('Y', {
       ctor: function () {
           this.name = ""I'm Y"";
       },

       myOrdinaryMethod: function (x, y, z) {
           console.log([this.name, x, y, z]);
       },

       myClassMemberMethod: ClassMember(function (x, y, z) {
           console.log([this.name, x, y, z]);
       })
   });

   var x = new X();
   var y = new Y();

   x.myClassMemberMethod('a', 'b', 'c'); // [""I'm X"", ""a"", ""b"", ""c""] 
   y.myClassMemberMethod('u', 'v', 'm'); // [""I'm Y"", ""u"", ""v"", ""m""] 

   x.myOrdinaryMethod('a', 'b', 'c'); // [""I'm X"", ""a"", ""b"", ""c""] 
   y.myOrdinaryMethod('u', 'v', 'm'); // [""I'm Y"", ""u"", ""v"", ""m""] 

   y.theirOrdinaryMethod = x.myOrdinaryMethod;
   y.theirClassMemberMethod = x.myClassMemberMethod;

   y.theirOrdinaryMethod('a', 'b', 'c'); // [""I'm Y"", ""a"", ""b"", ""c""] 
   y.theirClassMemberMethod('u', 'v', 'm'); // [""I'm X"", ""u"", ""v"", ""m""]

*/

angular.module('app').factory('ClassMember', function () {
    return function ClassMember(fn) {
        if (this instanceof ClassMember) {
            this.fn = fn;
        } else {
            return new ClassMember(fn);
        }
    };
});

angular.module('app').factory('Class', function (ClassMember) {
    var runtime = { initializing: false },
        fnTest = /xyz/.test(function() { xyz; }) ? /\bbase\b/ : /.*/,
        FN_ARGS = /^function\s*[^\(]*\(\s*([^\)]*)\)/m,
        STRIP_COMMENTS = /((\/\/.*$)|(\/\*[\s\S]*?\*\/))/mg;

    var toString = Object.prototype.toString;

    // The base Class implementation (does nothing)
    function Class() { };

    Class.members = { };

    // Create a new Class that inherits from this class
    Class.extend = function extend(displayName, properties) {
        var array;

        var targetMembers = {};
        var sourceMembers = this.members;

        for (var memberName in sourceMembers) {
            if (sourceMembers.hasOwnProperty(memberName)) {
                targetMembers[memberName] = sourceMembers[memberName];
            }
        }

        var base = this.prototype;

        // Instantiate a base class (but only create the instance,
        // don't run the ctor constructor)
        runtime.initializing = true;
        var prototype = new this();
        runtime.initializing = false;

        // Copy the properties over onto the new prototype
        for (var name in properties) {
            if (properties.hasOwnProperty(name)) {
                // Check if we're overwriting an existing function
                var property = properties[name];

                // Support angular's controller/service/factory declaration notation
                if (toString.call(property) === '[object Array]') {
                    array = property;

                    var item = array[array.length - 1];

                    if (toString.call(item) === '[object Function]' || item instanceof ClassMember) {
                        property = array[array.length - 1];
                    } else {
                        array = null;
                    }
                } else {
                    array = null;
                }

                var isClassMember = property instanceof ClassMember;

                if (isClassMember) {
                    property = property.fn;
                }

                if (typeof property === ""function"") {
                    if (typeof base[name] === ""function"" && fnTest.test(property)) {
                        property = (function (propertyName, fn) {
                            var args = fn.toString().replace(STRIP_COMMENTS, '').match(FN_ARGS)[1];
                            return (new Function('propertyName', 'fn', 'base', 'return function (' + args + ') {\n\
                                    var prevBase = this.base;\n\
                                    var hasBase = ""base"" in this;\n\
\n\
                                    // Add a new .base() method that is the same method\n\
                                    // but on the super-class\n\
\n\
                                    this.base = base[propertyName];\n\
\n\
                                    // The method only need to be bound temporarily, so we\n\
                                    // remove it when we\'re done executing\n\
                                    var ret = fn.call(this' + (!!args ? (', ' + args) : args) + ');\n\
\n\
                                    if (hasBase) {\n\
                                        this.base = prevBase;\n\
                                    } else {\n\
                                        delete this[""base""];\n\
                                    }\n\
                                    return ret;\n\
                                }'))(propertyName, fn, base);
                        })(name, property);
                    }

                    if (isClassMember) {
                        targetMembers[name] = property;
                    } else if (name in targetMembers) {
                        delete targetMembers[name];
                    }

                    if (array) {
                        array[array.length - 1] = property;

                        property = array;
                    }

                    prototype[name] = property;
                } else {
                    prototype[name] = property;
                }
            }
        }

        var membersArray = [];
        for (var i in targetMembers) {
            if (targetMembers.hasOwnProperty(i)) {
                membersArray.push({ name: i, fn: targetMembers[i] });
            }
        }

        // All construction is actually done in the ctor method
        var ChildClass = (new Function(""runtime"", ""members"", ""FN_ARGS"", ""STRIP_COMMENTS"", ""return function "" + (displayName || ""Class"") + ""() {\n\
            if (!runtime.initializing && this.ctor)\n\
            {\n\
                var length = members.length;\n\
                for (var i = 0; i < length; i++)\n\
                {\n\
                    var item = members[i];\n\
                    this[item.name] = (function (me, fn) {\n\
                        var args = fn.toString().replace(STRIP_COMMENTS, '').match(FN_ARGS)[1];\n\
                        return args ? (new Function('me', 'fn', 'return function (' + args + ') { return fn.call(me, ' + args + '); }'))(me, fn) : function () { return fn.call(me); };\n\
                    })(this, item.fn);\n\
\n\
                }\n\
                this.ctor.apply(this, arguments);\n\
            }\n\
        }""))(runtime, membersArray, FN_ARGS, STRIP_COMMENTS);

        ChildClass.members = targetMembers;

        // Populate our constructed prototype object
        ChildClass.prototype = prototype;

        // Enforce the constructor to be what we expect
        ChildClass.prototype.constructor = ChildClass;

        // And make this class extendable
        ChildClass.extend = extend;

        return ChildClass;
    };

    return Class;
});


Another edit
Eventually I've stumbled upon another problem related to original John Resig's implementation in relation to angular, and the problem is related to angular's annotation process (used for dependency injection) which uses Function.prototype.toString() and some Regex'es for the purpose of extracting the names of dependencies. And the problem with original implementation is that it doesn't expect this and so you are not able to declare methods that accept dependencies, so I've tweaked the implementation a little bit to deal with previously described problem and here it is:
/* Simple JavaScript Inheritance
 * By John Resig http://ejohn.org/
 * MIT Licensed.
 *
 * Inspired by base2 and Prototype

 * Angular adaptations by Denis Yaremov http://github.com/lu4
 * Usage:
 ---------------------------------

   var X = Class.extend('X', {
       ctor: function () {
           this.name = ""I'm X"";
       },

       myOrdinaryMethod: function (x, y, z) {
           console.log([this.name, x, y, z]);
       },

       myClassMemberMethod: ClassMember(function (x, y, z) {
           console.log([this.name, x, y, z]);
       })
   });

   var Y = Class.extend('Y', {
       ctor: function () {
           this.name = ""I'm Y"";
       },

       myOrdinaryMethod: function (x, y, z) {
           console.log([this.name, x, y, z]);
       },

       myClassMemberMethod: ClassMember(function (x, y, z) {
           console.log([this.name, x, y, z]);
       })
   });

   var x = new X();
   var y = new Y();

   x.myClassMemberMethod('a', 'b', 'c'); // [""I'm X"", ""a"", ""b"", ""c""] 
   y.myClassMemberMethod('u', 'v', 'm'); // [""I'm Y"", ""u"", ""v"", ""m""] 

   x.myOrdinaryMethod('a', 'b', 'c'); // [""I'm X"", ""a"", ""b"", ""c""] 
   y.myOrdinaryMethod('u', 'v', 'm'); // [""I'm Y"", ""u"", ""v"", ""m""] 

   y.theirOrdinaryMethod = x.myOrdinaryMethod;
   y.theirClassMemberMethod = x.myClassMemberMethod;

   y.theirOrdinaryMethod('a', 'b', 'c'); // [""I'm Y"", ""a"", ""b"", ""c""] 
   y.theirClassMemberMethod('u', 'v', 'm'); // [""I'm X"", ""u"", ""v"", ""m""]

*/


angular.module('homer').factory('Class', function () {
    function ClassMember(fn) {
        if (this instanceof ClassMember) {
            this.fn = fn;
            return this;
        } else {
            return new ClassMember(fn);
        }
    }

    function ClassEvent() {
        if (this instanceof ClassEvent) {
            return this;
        } else {
            return new ClassEvent();
        }
    }

    var runtime = { initializing: false },
        fnTest = /xyz/.test(function () { xyz; }) ? /\bbase\b/ : /.*/,
        fnArgs = /^function\s*[^\(]*\(\s*([^\)]*)\)/m,
        stripComments = /((\/\/.*$)|(\/\*[\s\S]*?\*\/))/mg;

    var toString = Object.prototype.toString;

    // The base Class implementation (does nothing)
    function Class() { };

    Class.events = {};
    Class.members = {};

    // Create a new Class that inherits from this class
    Class.extend = function Extend(displayName, properties) {
        var array;

        var targetEvents = {};
        var sourceEvents = this.events;

        var targetMembers = {};
        var sourceMembers = this.members;

        for (var eventName in sourceEvents) {
            if (sourceEvents.hasOwnProperty(eventName)) {
                targetEvents[eventName] = sourceEvents[eventName];
            }
        }

        for (var memberName in sourceMembers) {
            if (sourceMembers.hasOwnProperty(memberName)) {
                targetMembers[memberName] = sourceMembers[memberName];
            }
        }

        var base = this.prototype;

        // Instantiate a base class (but only create the instance,
        // don't run the ctor constructor)
        runtime.initializing = true;
        var prototype = new this();
        runtime.initializing = false;

        // Copy the properties over onto the new prototype
        for (var name in properties) {
            if (properties.hasOwnProperty(name)) {
                // Check if we're overwriting an existing function
                var property = properties[name];

                // Support angular's controller/service/factory declaration notation
                if (toString.call(property) === '[object Array]') {
                    array = property;

                    var item = array[array.length - 1];

                    if (toString.call(item) === '[object Function]' || item instanceof ClassMember) {
                        property = array[array.length - 1];
                    } else {
                        array = null;
                    }
                } else {
                    array = null;
                }

                var isClassMember = property instanceof ClassMember;

                if (isClassMember) {
                    property = property.fn;
                }

                var isClassEvent = property instanceof ClassEvent;

                if (isClassEvent) {
                    property = (function() {
                        function Subscriber(fn) {
                            Subscriber.listeners.push(fn.bind(this));
                        };

                        Subscriber.listeners = [];
                        Subscriber.fire = function() {
                            var listeners = Subscriber.listeners;

                            for (var i = 0; i < listeners.length; i++) {
                                var result = listeners[i].apply(this, arguments);

                                if (result !== undefined) return result;
                            }

                            return void 0;
                        }

                        return Subscriber;
                    })();
                }

                if (typeof property === ""function"") {
                    if (typeof base[name] === ""function"" && fnTest.test(property)) {
                        property = (function (propertyName, fn) {
                            var args = fn.toString().replace(stripComments, '').match(fnArgs)[1];
                            return (new Function('propertyName', 'fn', 'base', 'return function (' + args + ') {\n\
                                    var prevBase = this.base;\n\
                                    var hasBase = ""base"" in this;\n\
\n\
                                    // Add a new .base() method that is the same method\n\
                                    // but on the super-class\n\
\n\
                                    this.base = base[propertyName];\n\
\n\
                                    // The method only need to be bound temporarily, so we\n\
                                    // remove it when we\'re done executing\n\
                                    var ret = fn.call(this' + (!!args ? (', ' + args) : args) + ');\n\
\n\
                                    if (hasBase) {\n\
                                        this.base = prevBase;\n\
                                    } else {\n\
                                        delete this[""base""];\n\
                                    }\n\
                                    return ret;\n\
                                }'))(propertyName, fn, base);
                        })(name, property);
                    }

                    if (isClassEvent) {
                        targetEvents[name] = property;
                    } else {
                        delete targetEvents[name];
                    }

                    if (isClassMember) {
                        targetMembers[name] = property;
                    } else if (name in targetMembers) {
                        delete targetMembers[name];
                    }

                    if (array) {
                        array[array.length - 1] = property;

                        property = array;
                    }

                    prototype[name] = property;
                } else {
                    prototype[name] = property;
                }
            }
        }

        var eventsArray = [];
        for (var targetEventName in targetEvents) {
            if (targetEvents.hasOwnProperty(targetEventName)) {
                eventsArray.push({ name: targetEventName, fn: targetEvents[targetEventName] });
            }
        }

        var membersArray = [];
        for (var targetMemberName in targetMembers) {
            if (targetMembers.hasOwnProperty(targetMemberName)) {
                membersArray.push({ name: targetMemberName, fn: targetMembers[targetMemberName] });
            }
        }

        // All construction is actually done in the ctor method
        var ChildClass = (new Function(""runtime"", ""events"", ""members"", ""FN_ARGS"", ""STRIP_COMMENTS"", ""return function "" + (displayName || ""Class"") + ""() {\n\
            if (!runtime.initializing && this.ctor)\n\
            {\n\
                var length = members.length;\n\
                var bind = function (me, $$fn$$) {\n\
                    var args = $$fn$$.toString().replace(STRIP_COMMENTS, '').match(FN_ARGS)[1];\n\
                    var result = args ? (new Function('me', '$$fn$$', 'return function (' + args + ') { return $$fn$$.apply(me, arguments); }'))(me, $$fn$$) : function () { return $$fn$$.apply(me, arguments); };\n\
                    return result;\n\
                };\n\
                for (var i = 0; i < length; i++)\n\
                {\n\
                    var item = members[i];\n\
                    var fn = item.fn;\n\
                    var name = item.name;\n\
                    var property = this[name] = bind(this, fn);\n\
                    if (fn.fire) {\n\
                        property.fire = bind(this, fn.fire);\n\
                    }\n\
                    if (fn.listeners) {\n\
                        property.listeners = fn.listeners;\n\
                    }\n\
                }\n\
                \n\
                var length = events.length;\n\
                for (var i = 0; i < length; i++)\n\
                {\n\
                    var item = events[i];\n\
                    var fn = item.fn;\n\
                    var name = item.name;\n\
                    var property = this[name] = bind(this, fn);\n\
                    if (fn.fire) {\n\
                        property.fire = bind(this, fn.fire);\n\
                    }\n\
                    if (fn.listeners) {\n\
                        property.listeners = fn.listeners;\n\
                    }\n\
                }\n\
                this.ctor.apply(this, arguments);\n\
            }\n\
        }""))(runtime, eventsArray, membersArray, fnArgs, stripComments);

        ChildClass.members = targetMembers;

        // Populate our constructed prototype object
        ChildClass.prototype = prototype;

        // Enforce the constructor to be what we expect
        ChildClass.prototype.constructor = ChildClass;

        // And make this class extendable
        ChildClass.extend = Extend;
        ChildClass.event = ClassEvent;
        ChildClass.member = ClassMember;

        return ChildClass;
    };

    Class.member = ClassMember;
    Class.event = ClassEvent;

    return Class;
});","Your guesses sounds perfectly applicable.
You can reuse functionality defined in parent controllers by simply calling methods attached to the parent scope:
HTML
<div ng-controller=""ParentCtrl"">
    <!-- Something here ... -->
    <div ng-controller=""ChildCtrl"">
        <!-- Something here ... -->
    </div>
    <!-- Something here ... -->
</div>

JavaScript
function ParentCtrl($scope) {
    $scope.parentMethod = function () {
        //method body
    };
}

function ChildCtrl($scope) {
    $scope.childMethod = function () {
        //functionality
        $scope.parentMethod();
        //functionality
    };
}

If you want to use the JavaScript approach with prototype inheritance you can use:
var myApp = angular.module('myApp',[]);

function Parent($scope) {
    $scope.name = 'Superhero';    

    $scope.clickParent = function() {
        $scope.name = 'Clicked from base controller';
    }    
}

function Child($scope, $injector) {

    debugger;
    $injector.invoke(Parent, this, {$scope: $scope});

    $scope.name = 'Superhero Child';

    $scope.clickChild = function(){
        $scope.clickParent();
    }       
}
Child.prototype = Object.create(Parent.prototype);

http://jsfiddle.net/mhevery/u6s88/12/
For services, for example, you can use:
(function () {

function ParentService(arg1) {
   this.arg1 = arg1;
}

function ChildService(arg1, arg2) {
   ParentService.call(this, arg1);
   this.arg2 = arg2;
}

ChildService.prototype = new ParentService();

app.service('ChildService', ChildService);

}());

Also check this discussion and the blog post about inheritance in AngularJS I posted.",https://stackoverflow.com/questions/17389291/angularjs-with-oop-inheritance-in-action,3,1
SQL: Normalization of database while retaining constraints,"Suppose I have the following tables:
     ____________________             ____________________
    |     Organisms      |           |       Species      |
    |--------------------|           |--------------------|
    |OrganismId (int, PK)|           |SpeciesId (int, PK) |
    |SpeciesId (int, FK) |_---------1|Name (varchar)      |
    |Name (varchar)      |           |____________________|
    |____________________|                      1
              1                                 |
              |                                 |
              |                                 |
              _                                 _
    ______________________        ____________________          _______________
   | OrganismPropsValues  |      |   SpeciesProps     |        |     Props     |
   |----------------------|      |--------------------|        |---------------|
   |OrganismId (int, FK)  |      |PropId (int,PK,FK)  | _-----1|PropId (int,PK)|
   |PropId (int, FK)      |      |SpeciesId(int,PK,FK)|        |Name (varchar) |
   |Value (varchar)       |      |____________________|        |_______________|
   |______________________|                                             1
              _                                                         |
              |                                                         |
              -----------------------------------------------------------

A quick explanation of what I am trying to represent here: suppose we have a list of species, such as cat, dog, human, etc.  We also have a set of properties (abbreviated Props so I could fit it more easily in the diagram) which apply to some but not necessarily all species--for example, this may be tail length (for species with tails), eye color (for those with eyes), etc.
SpeciesProps is a linker table that defines which properties apply to which species-- so here we would have {Human, Eye Color}, {Dog, Eye Color}, {Cat, Eye Color}, {Dog, Tail Length}, {Cat, Tail Length}.  We do not have {Human, Tail Length} because Tail Length is obviously not a valid property to apply to a human.
The Organisms table holds actual ""implementations"" of the species-- So here we might have {Human, Bob}, {Dog, Rufus}, and {Cat, Felix}.
Here is now my issue: in the OrganismPropsValues table, I want to store the 'values' of the properties for each organism--so for example, for Bob I want to store {Bob, Eye Color, Blue}.  For Rufus, I would want to store {Rufus, Eye Color, Brown} and {Rufus, Tail Length, 20} (similar for Felix).  My problem however, is that in the schema that I have detailed, it is perfectly possible to store {Bob, Tail Length, 10}, even though the {Human, Tail Length} tuple does not exist in SpeciesProps.  How can I modify this schema so I can enforce the constraints defined in SpeciesProps in OrganismPropsValues, while maintaining adequate normalization?","You're implementing the Entity-Attribute-Value antipattern.  This can't be a normalized database design, because it's not relational.
What I would suggest instead is the Class Table Inheritance design pattern:

Create one table for Organisms, containing properties common to all species.
Create one table per species, containing properties specific to that species.  Each of these tables has a 1-to-1 relationship with Organisms, but each property belongs in its own column.
 ____________________             ____________________
|     Organisms      |           |       Species      |
|--------------------|           |--------------------|
|OrganismId (int, PK)|           |SpeciesId (int, PK) |
|SpeciesId (int, FK) |_---------1|Name (varchar)      |
|Name (varchar)      |           |____________________|
|____________________|
          1
          |
          |
          1
 ______________________ 
|    HumanOrganism     |
|----------------------|
|OrganismId (int, FK)  |
|Sex      (enum)       |
|Race     (int, FK)    |
|EyeColor (int, FK)    |
|....                  |
|______________________|


This does mean you will create many tables, but consider this as a tradeoff with the many practical benefits to storing properties in a relationally correct way:

You can use SQL data types appropriately, instead of treating everything a free-form varchar.
You can use constraints or lookup tables to restrict certain properties by a predefined set of values.
You can make properties mandatory (i.e. NOT NULL) or use other constraints.
Data and indexes are stored more efficiently.
Queries are easier for you to write and easier for the RDBMS to execute.

For more on this design, see Martin Fowler's book Patterns of Enterprise Application Architecture, or my presentation Practical Object-Oriented Models in SQL, or my book, SQL Antipatterns: Avoiding the Pitfalls of Database Programming.",https://stackoverflow.com/questions/7183039/sql-normalization-of-database-while-retaining-constraints/7183463,4,1
Optimize a query that is using multiple left joins on the same tables,"I've come across a query that is taking ""too long"". The query has 50+ left joins between 10 or so tables. To give a brief overview of the database model, the tables joined are tables that store data for a particular data type (ex: date_fields, integer_fields, text_fields, etc.) and each has a column for the value, a ""datafield"" id, and a ticket id. The query is built programmatically based on an association table between a ""ticket"" and its ""data fields"".
The join statements look something like the following:
...FROM tickets t
LEFT JOIN ticket_text_fields t001 ON(t.id=t001.ticket_id AND t001.textfield_id=7)
...
LEFT JOIN ticket_date_fields t056 ON(t.id=t056.ticket_id AND t056.datafield_id=434)

When using explain on the query shows the following:
1   SIMPLE   t       ref   idx_dataset_id                   idx_dataset_id  5   const   2871   Using where; Using temporary; Using filesort
1   SIMPLE   t001   ref   idx_ticket_id,idx_datafield_id   idx_ticket_id   5   t.id   5   
... 
1   SIMPLE   t056   ref   idx_ticket_id,idx_datafield_id   idx_ticket_id   5   t.id   8

What direction can I take to tune this query? All the indexes seem to be in place. Perhaps the t table (tickets) row number (2871) should be reduced. How many left joins is too much? Should the datafield tables be joined only once and then queried each for the data that is required?","You're using a variation of the terrible antipattern called Entity-Attribute-Value.  You're storing attributes on separate rows, so if you want to reconstruct something that looks like a conventional row of data, you need to make one join per attribute.  
It's not surprising this creates a query with 50 joins.  This is far too many for most databases to run efficiently (you haven't identified which database you're using).  Eventually you'll want a few more attributes and you might exceed some architectural limit of the database on the number of joins it can do.
The solution is: don't reconstruct the row in SQL.
Instead, query the attributes as multiple rows, instead of trying to combine them onto a single row.
SELECT ... FROM tickets t
INNER JOIN ticket_text_fields f ON t.id=f.ticket_id
WHERE f.textfield_id IN (7, 8, 9, ...)
UNION ALL
SELECT ... FROM tickets t
INNER JOIN ticket_date_fields d ON t.id=d.ticket_id
WHERE d.datafield_id IN (434, 435, 436, ...)

Then you have to write a function in your application to loop over the resulting rowset, and collect the attributes one by one into an object in application space, so then you can use it as if it's a single entity.",https://stackoverflow.com/questions/3908366/optimize-a-query-that-is-using-multiple-left-joins-on-the-same-tables/3908511,2,1
microservices and domain logic joins,"Microservices are deployed hosting their own database.
What strategies do you employ when business requirements necessitate joins across data in multiple services?
Example problem:  You are implementing a movie review site.  You have a movie microservice that holds the movie DB.  You also have a review microservice that manages reviews in its own separate DB.  Reviews are linked to movies via a GUID; but as these are implemented as separate data stores, not a key constraint.
You would like to have available, accurate to the last minute, a report that tells you the total number of reviews for each review level grouped by the first letter of the movie having a review word count > 25 words.  You currently host 5 million reviews for 40,000 movies.
E.G.   Reviews with more than 25 words:

A  [8457 ""1 star""] [16615 ""2 star""] [...
B  [98445 ""1 star""] [80210 ""2 star""] [...
...

Having chosen a microservice architecture for your project, what strategies would you now employ to implement this feature?","I think at this point I would ask myself what exactly is the domain you are trying to model against. If the domain is strictly rendering movies and the reviews for the movies, my question would be why are there two separate services, the movie and movie review service.
In essence, I would merge the two services together into a single service and call it a movie-reviews-service since reviews for the movies is all thats cared about. In this case, there would no longer be a problem with joins.
Personally, I think the question to really ask is whether the movie service should exist and what kind of role it plays. In your example, it seems a extraneous to be broken into a separate service. While this may not be a satisfactory answer, the example provided is technically a little too simple to make a microservices architecture worthwhile since there are less components requiring the separation of concern to really break them down further into multiple services. 
If the example was complex enough to warrant a microservices architecture to have these two separate services, it would just be a matter of redundancy of data in the movie-reviews service and the movies-service in order to fully denormalize. The idea being that a service should try to entirely rely on itself as much as possible rather than making multiple requests to very granular services leading to an antipattern -- the nanoservices architecture. Hope this helps!",https://stackoverflow.com/questions/32831192/microservices-and-domain-logic-joins/32910715,3,1
Enterprise ASP.NET MVC 3 architecture outline [closed],"I've gotten lucky enough to work on a completely new enterprise MVC 3 project. And by new, I mean source control is literally empty at this point.
We're trying to figure out how to lay out out the solution and various projects that will be needed.
We'll be using Razor, WCF, Entity Framework, Moq, Ninject, SpecFlow, MSTest, and CodedUI. 
Can anyone point me to a well-laid-out enterprise application that I might be able to use a model for our application structure? I.e. solution and project structure.
All the examples of MVC projects I've found have very poor separation of concerns, and we want to make sure we do this thing right.
Help? :)",I've found Project Silk to be pretty good structurally.,https://stackoverflow.com/questions/6036692/enterprise-asp-net-mvc-3-architecture-outline/6050927,4,1
Save external Tweets in database in Rails,"I am new to rails developement and to the MVC architecture. I have a little application where I can add Videos' URLs from Dailymotion or Youtube and get the tweets related to that URL using the twitter gem in Ruby on Rails. 
Now i'm able to store the tweets like this : (This is the video controller)
 def show
  @video = Video.find(params[:id])

  # Creating a URL variable 
  url = @video.url

  # Search tweets for the given video/url
  @search = get_client.search(""#{@video.url} -rt"")

  # Save tweets in database
  @search.collect do |t|
    tweet = Tweet.create do |u|
    u.from_user =  t.user.screen_name.to_s
    u.from_user_id_str = t.id.to_s
    u.profile_image_url = t.user.profile_image_url.to_s
    u.text = t.text.to_s
    u.twitter_created_at = t.created_at.to_s
  end
end

I'm not sure if this is the right way to do it (doing it in the controller ?), and what I want to do now is to specify that those tweets that have just been stored belong to the current video. Also I would like to have some sort of validation that makes the controller look in the database before doing this to only save the new tweets. Can someone help me with that ?
My models :
class Video < ActiveRecord::Base
   attr_accessible :url
   has_many :tweets
end

class Tweet < ActiveRecord::Base
belongs_to :video
end

My routes.rb 
 resources :videos do
  resources :tweets
end","This is an example of a ""fat controller"", an antipattern in any MVC architecture (here's a good read on the topic).
Have you considered introducing a few new objects to encapsulate this behavior? For example, I might do something like this:
# app/models/twitter_search.rb
class TwitterSearch
  def initialize(url)
    @url = url
  end

  def results
    get_client.search(""#{@url} -rt"")
  end
end

# app/models/twitter_persistence.rb
class TwitterPersistence
  def self.persist(results)
    results.map do |result|
      self.new(result).persist
    end
  end

  def initialize(result)
    @result = result
  end

  def persist
    Tweet.find_or_create_by(remote_id: id) do |tweet|
      tweet.from_user =  screen_name
      tweet.from_user_id_str = from_user_id
      tweet.profile_image_url = profile_image_url
      tweet.text = text
      tweet.twitter_created_at = created_at
    end
  end

  private

  attr_reader :result

  delegate :screen_name, :profile_image_url, to: :user
  delegate :id, :user, :from_user_id, :text, :created_at, to: :result
end

Notice the use of find_or_create_by ... Twitter results should have a unique identifier that you can use to guarantee that you don't create duplicates. This means you'll need a remote_id or something on your tweets table, and of course I just guessed at the attribute name (id) that the service you're using will return.
Then, in your controller:
# app/controllers/videos_controller.rb
class VideosController < ApplicationController
  def show
    @tweets = TwitterPersistence.persist(search.results)
  end

  private

  def search
    @search ||= TwitterSearch.new(video.url)
  end

  def video
    @video ||= Video.find(params[:id])
  end
end

Also note that I've removed calls to to_s ... ActiveRecord should automatically convert attributes to the correct types before saving them to the database.
Hope this helps!",https://stackoverflow.com/questions/21791305/save-external-tweets-in-database-in-rails/21791934,1,1
Do you use design patterns?,"What's the penetration of design patterns in the real world? Do you use them in your day to day job - discussing how and where to apply them with your coworkers - or do they remain more of an academic concept? 
Do they actually provide actual value to your job? Or are they just something that people talk about to sound smart?
Note: For the purpose of this question ignore 'simple' design patterns like Singleton. I'm talking about designing your code so you can take advantage of Model View Controller, etc.","Any large program that is well written will use design patterns, even if they aren't named or recognized as such.  That's what design patterns are, designs that repeatedly and naturally occur.  If you're interfacing with an ugly API, you'll likely find yourself implementing a Facade to clean it up.  If you've got messaging between components that you need to decouple, you may find yourself using Observer.  If you've got several interchangeable algorithms, you might end up using Strategy.
It's worth knowing the design patterns because you're more likely to recognize them and then converge on a clean solution more quickly.  However, even if you don't know them at all, you'll end up creating them eventually (if you are a decent programmer).
And of course, if you are using a modern language, you'll probably be forced to use them for some things, because they're baked into the standard libraries.",https://stackoverflow.com/questions/11586/do-you-use-design-patterns/2544884,15,1
Provisioning SQL 2008 Database with C# Application,"I have an internal enterprise application I've developed for my company built on .Net 3.5 / SQL 2008. 
I have two types of databases.  The main system database which contains all of our global data, such as user names, and customers, etc.  And project databases, which contain the actual data pertaining to our clients project.
When the system creates a new project for a customer, it needs to provision a new SQL database using a custom schema providing the tables, views, sps, etc.  The name of the project database corresponds to the Project ID of the project stored in the system database.  So a new project will create a new project database with the name:  Project_XXX where XXX is the project id.
My question is what is the best way to provision a custom database programatically? Right now the only way I can think to do it is have a class which reads a SQL script from the file system and does a parse to replace the project ID for the name of the database.  This is easy, but seems rather inelegant.  
Any approaches some veterans out there prefer over this?","Generally if there is only ever one (or a handful) of databases, and you have direct control over them (typicall corporate environment), I'd recommend not auto upgrading the databases as it is more hassle than it's worth. Just pass the script on to the people doing the install.
For more widespread releases in the past I have used a script as you suggest along with the SQL Server SMO library (Server.CurrentContext.ExecuteNonQuery()). I don't find it inelegant is it is simple and it works.
For the first release we would include a full DB build script, then add an upgrade script for each subsquent release. So if someone installs v1.2 over v1.1 we would only run the v1.2 script. However if they did a fresh install we would run v1.0, v1.1 and v1.2.",https://stackoverflow.com/questions/297652/provisioning-sql-2008-database-with-c-sharp-application/297715,3,1
Why is Hibernate Open Session in View considered a bad practice?,"And what kind of alternative strategies do you use for avoiding LazyLoadExceptions?
I do understand that open session in view has issues with:

Layered applications running in different jvm's
Transactions are committed only at the end, and most probably you would like the results before.

But, if you know that your application is running on a single vm, why not ease your pain by using an open session in view strategy?","Because sending possibly uninitialised Proxies, especially collections, in the view layer and triggering hibernate loading from there can be troubling from both a performance and understanding point of view.
Understanding:
Using OSIV 'pollutes' the view layer with concerns related to the data access layer.
The view layer is not prepare to handle a HibernateException which may happen when lazy loading, but presumably the data access layer is.
Performance:
OSIV tends to tug proper entity loading under the carpet - you tend not to notice that your collections or entities are lazily initialised ( perhaps N+1 ). More convenience, less control.

Update: see The OpenSessionInView antipattern for a larger discussion regarding this subject. The author lists three important points:


each lazy initialization will get you a query meaning each entity will need N + 1 queries, where N is the number of lazy associations. If your screen presents tabular data, reading Hibernate__ log is a big hint that you do not do as you should
this completely defeats layered architecture, since you sully your nails with DB in the presentation layer. This is a conceptual con, so I could live with it but there is a corollary
last but not least, if an exception occurs while fetching the session, it will occur during the writing of the page: you cannot present a clean error page to the user and the only thing you can do is write an error message in the body",https://stackoverflow.com/questions/1103363/why-is-hibernate-open-session-in-view-considered-a-bad-practice/1103371,9,1
Zend framework - updating two tables from the same model,"I have a model (call it Model_A) that I use to CRUD on a particular table defined by $name var. I also need to update another table (call it Model_B with different $name var) if the first operation in the other model is successful.
I'm trying to update table represented in Model_B from an action in class Model_A. I'm doing this by creating an instance of Model_B inside an action in Model_A. The update action fails fails because it seems to try to write to the table $name from Model_A, instead of the table $name from the instance of Model_B.
I suppose I could pass the result of the first update back to the controller and then go to the second model, but I'd rather do both updates at the same time.
Apart from an error in my coding, is there any obvious reason why my second update is not finding the right table.. and is this a reasonable aproach for what I'm wanting to do?","Most people think of a Model as coincident with a table class.  In fact, they make models extend a base table class.  For most simple CRUD type work, this is more or less fine.  But as you get into more complex updates across multiple tables, or transaction scripts, you find awkward situations like the one you are asking about.
In fact, a model is not a table.  A model is an encapsulation of some logical part of your application's business, not the implementation of storing it in a database.
I worked on the code of Zend_Db_Table quite a bit a few years ago, and when I was writing the manual, I was careful not to refer to table classes as Models.
So you may need to introduce a true Model class layer (which extends no base class), that knows how to do certain application-specific operations against one or more database tables, yet it is not your application's Controller.
Controller -> Model -> Table(s)

This architecture is a great way to avoid repeating code if you have similar business tasks that are called from multiple controllers.  It helps to decouple application functions and persistence details from your Controllers, which has benefits in code maintenance, testability, encapsulation, etc.
Check out the free mini-book Domain-Driven Design Quickly (based on Eric Evans books), or the chapter ""Magic Beans"" in my book, SQL Antipatterns: Avoiding the Pitfalls of Database Programming.",https://stackoverflow.com/questions/7168684/zend-framework-updating-two-tables-from-the-same-model/7168745,2,1
General N-Tier Architecture Question,"In an N-Tier app you're supposed to have a business logic layer and a data access layer.
Is it bad to simply have two assemblies: BusinessLogicLayer.dll and DataAccessLayer.dll to handle all this logic? How do you actually represent these layers. It seems silly, the way I've seen it, to have a BusinessLogic class library containing classes like: CustomerBusinessLogic.cs, OrderBusinessLogic.cs, etc. each calling their appropriately named cousin in the DataAccessLayer class library, i.e. CustomerDataAccess.cs, OrderDataAccess.cs.
I want to create a web app using MVP and it doesn't seem so cut and dry as this. There are lots of opinions about where the business logic is supposed to be put in MVP and I'm not sure I've found a really great answer yet.
I want this project to be easily testable, and I am trying to adhere to TDD methodologies as best I can. I intend to use MSTest and Rhino Mocks for testing.
I was thinking of something like the following for my architecture:
I'd use LINQ-To-SQL to talk to the database. WCF services to define data contract interfaces for the business logic layer. Then use MVP with ASP.NET Forms for the UI/BLL.
Now, this isn't the start of this project, most of the LINQ stuff is already done, so it's stuck. The WCF service would replace the existing DataAccessLayer assembly and the UI/BLL would replace the BusinessLogicLayer assembly etc.
This sort of makes sense in my head, but its getting really late. Anyone that's traveled down this path have any guidance? Good links? Warnings?
Thanks!","the way I've seen it, to have a
  BusinessLogic class library containing
  classes like:
  CustomerBusinessLogic.cs,
  OrderBusinessLogic.cs, etc

Ouch. Get and read Scott Ambler's ""Building Object Applications That Work"". Your approach does not and is a maintenance enightmare - no objects.

I'd use LINQ-To-SQL to talk to the
  database. WCF services to define data
  contract interfaces for the business
  logic layer. Then use MVP with ASP.NET
  Forms for the UI/BLL.

Yes. Great way to make the application artificially more complicated and slower than it has to be. Throw out the complete WCF service - what are they for? WCF is for SOA, and SOA lives in the user interface (i.e. it is a trust boudary and a user interface for another application to use). Unless you have that requirement.... it is stupid tointroduce additional slow technologies that just have overhead.

The WCF service would replace the
  existing DataAccessLayer assembly

The Daily WTF - what the heck do you have a DAL assembly for when you use LINQ to SQL? LINQ to SQL (the runtime) is your DAL.

Anyone that's traveled down this path
  have any guidance? Good links?

You basically picked every antipattern I can think of - maintenance nightmare, overdesigned with tons of useless technologies in there. You force layer technologies into a tiered architecture.
Read the book I mentioned.",https://stackoverflow.com/questions/2475246/general-n-tier-architecture-question/2475332,2,1
Does model check data or presenter do it?,I have question about MVP. If presenter sends request to model for data. Does model check these data or does presenter have to do it? For example: are data ok etc. Thank you for answers.,"I think there are a couple approaches you can take to validate your data: Either the Domain objects or a Service.
You model is your domain. In domain driven development, your domain should know how to validate itself. So, you might have a standard Validate method on either class in the model. That can get a little tricky, though, if you need to make a database call to do the validation, although you could require the relevant data to be passed to your validation method, or provide a delegate to get the data if it's necessary.
Alternatively, you could put all the validation in the Service later, which your presenter would be calling to retrieve and to persist the model. This would result in the so-called antipattern ""anemic domain"". But, if that fits your application and architecture best, it might be the correct choice.
I would caution against having the presenter do it. That's not really it's job and it does not get reused like the model and service.",https://stackoverflow.com/questions/14920578/does-model-check-data-or-presenter-do-it/15139696,1,1
Assistance in querying an Entity Attribute Value model ( __ynamic attributes_ ) in SQL Server 2012,"I need assistance to query from a model that contains one table of a specific object (i.e. Products) and one table of its dynamic attributes.   
Let's say that I can store a Chocolate with attributes such as Price, Color, Weight and also a Car with attributes such as Engine, Gears, Color.   
In my example I have a table called Products with the following columns :   
 Id (Int),   
 Name (NVarchar)

I have another table called dynamicAttributes with the following columns : 
Id (int)               -- of the attribute
ProductId (int)        -- of the specific product
AttributeType (int)    -- enum with the following values (""Color"", ""Price"",""Height"", ""Width"".... )
StringValue -- of the product
IntValue -- of the product
DoubleValue -- of the product
BooleanValue -- of the product

I get from the client a list of attributes codes and a list of there values.    
I can get value type (aka boolean, string, int) for each attribute.
What are my best options to query this model from my app ? 
Dynamic sqls only ? Using Pivot keyword?","As others have noted, doing a lot of PIVOT queries is pretty inefficient and it's laborious to write and to debug SQL queries that use PIVOT.
An alternative is to fetch the data back from the database in the way it's stored, i.e. in multiple rows.  Then write code in your database access layer to massage the rows into a single object instance, adding one attribute to your object per database row.  This is called the Table Module pattern in Martin Fowler's awesome book Patterns of Enterprise Application Architecture.
If you invest some time writing DBAL code in a reusable fashion, you may be able to make it pretty easy for subsequent code in your app to read and save objects stored in an EAV table. 
But yeah, I agree with other commenters.  I'm generally against using the EAV design.  It takes a lot of work to write code to compensate for the ways EAV breaks database conventions.  I would think you have better things to do with your time!
For alternatives, see:

My answer to How to design a product table for many kinds of product where each product has many parameters
My presentation Practical Object Oriented Models In SQL
My book SQL Antipatterns: Avoiding the Pitfalls of Database Programming",https://stackoverflow.com/questions/13440285/assistance-in-querying-an-entity-attribute-value-model-dynamic-attributes/13520358,1,1
How to design a product table for many kinds of product where each product has many parameters,"I do not have much experience in table design. My goal is to create one or more product tables that meet the requirements below:

Support many kinds of products (TV, Phone, PC, ...). Each kind of product has a different set of parameters, like:

Phone will have Color, Size, Weight, OS...
PC will have CPU, HDD, RAM...

The set of parameters must be dynamic. You can add or edit any parameter you like.

How can I meet these requirements without a separate table for each kind of product?","You have at least these five options for modeling the type hierarchy you describe:

Single Table Inheritance: one table for all Product types, with enough columns to store all attributes of all types.  This means a lot of columns, most of which are NULL on any given row.
Class Table Inheritance: one table for Products, storing attributes common to all product types.  Then one table per product type, storing attributes specific to that product type.
Concrete Table Inheritance: no table for common Products attributes.  Instead, one table per product type, storing both common product attributes, and product-specific attributes.
Serialized LOB: One table for Products, storing attributes common to all product types.  One extra column stores a BLOB of semi-structured data, in XML, YAML, JSON, or some other format.  This BLOB allows you to store the attributes specific to each product type.  You can use fancy Design Patterns to describe this, such as Facade and Memento.  But regardless you have a blob of attributes that can't be easily queried within SQL; you have to fetch the whole blob back to the application and sort it out there.
Entity-Attribute-Value: One table for Products, and one table that pivots attributes to rows, instead of columns.  EAV is not a valid design with respect to the  relational paradigm, but many people use it anyway.  This is the ""Properties Pattern"" mentioned by another answer.  See other questions with the eav tag on StackOverflow for some of the pitfalls.

I have written more about this in a presentation, Extensible Data Modeling.

Additional thoughts about EAV:  Although many people seem to favor EAV, I don't.  It seems like the most flexible solution, and therefore the best.  However, keep in mind the adage TANSTAAFL.  Here are some of the disadvantages of EAV:

No way to make a column mandatory (equivalent of NOT NULL).
No way to use SQL data types to validate entries.
No way to ensure that attribute names are spelled consistently.
No way to put a foreign key on the values of any given attribute, e.g. for a lookup table.
Fetching results in a conventional tabular layout is complex and expensive, because to get attributes from multiple rows you need to do JOIN for each attribute.

The degree of flexibility EAV gives you requires sacrifices in other areas, probably making your code as complex (or worse) than it would have been to solve the original problem in a more conventional way.
And in most cases, it's unnecessary to have that degree of flexibility.  In the OP's question about product types, it's much simpler to create a table per product type for product-specific attributes, so you have some consistent structure enforced at least for entries of the same product type.
I'd use EAV only if every row must be permitted to potentially have a distinct set of attributes.  When you have a finite set of product types, EAV is overkill.  Class Table Inheritance would be my first choice.

Update 2019: The more I see people using JSON as a solution for the ""many custom attributes"" problem, the less I like that solution. It makes queries too complex, even when using special JSON functions to support them. It takes a lot more storage space to store JSON documents, versus storing in normal rows and columns.
Basically, none of these solutions are easy or efficient in a relational database. The whole idea of having ""variable attributes"" is fundamentally at odds with relational theory.
What it comes down to is that you have to choose one of the solutions based on which is the least bad for your app. Therefore you need to know how you're going to query the data before you choose a database design. There's no way to choose one solution that is ""best"" because any of the solutions might be best for a given application.",https://stackoverflow.com/questions/695752/product-table-many-kinds-of-product-each-product-has-many-parameters/695860,4,1
Is IDependencyResolver an anti-pattern?,"I am designing some architectural changes into a legacy ASP.NET application. I prototyped some classes for dependency resolution that mimic the ASP.NET MVC's IDependencyResolver. I won't post because it is pretty much the same interface, but in other natural language.
I figured out it might be considered Service Location, which in turn is usually (not fully in some cases) condemned in favor of Dependency Injection. Nevertheless, I couldn't find any recommendation against the use of the ASP.NET MVC's dependency resolution implementation.
Is the ASP.NET MVC's IDependencyResolver considered an anti-pattern? Is it a bad thing?","If you look at the signature you will see that it's just a Service Locator with another name. Service Locator is an anti-pattern and I consider the relationship transitive, so I consider IDependencyResolver an anti-pattern.
Apart from that, the interface is also broken because it has no Release method.",https://stackoverflow.com/questions/5653783/is-idependencyresolver-an-anti-pattern,2,1
Is Service Locator an anti pattern in a pluggable architecture?,"I know this question might look like it's a duplicate but please let me explain.
So I created several components that use a pluggable architecture, basically I can freely add new implementations and they will be injected and processed automatically for me. This is really handy in several scenarios.
I'm going to talk about the simplest one, validating components.
One of the reasons to use a design like this is that I like to expose my roles explicitly as explained by Udi Dahan
Basically I have code like this:
public interface IValidatorRuner
{
    void Run<TTarget>(TTarget target);
}

public class ValidatorRunenr : IValidatorRuner
{
    private readonly IServiceLocator _serviceLocator;

    public ValidatorRunenr(IServiceLocator serviceLocator)
    {
        _serviceLocator = serviceLocator;
    }

    public void Run<TTarget>(TTarget target)
    {
        // this is the dynamic/pluggable phase
        // is this an antipattern???
        var foundValdiators = _serviceLocator.GetAllInstances<IValidator<TTarget>>();

        foreach (var valdiator in foundValdiators)
        {
            valdiator.IsSatisfiedBy(target);
        }
    }
}

This code lets me expose my validation rules explicitly like this:
//this will allow me to create validators in this way
//and they will be automatically injected and resolved for me 
//(easy, to read, easy to write, easy to test, pff I could even smoke this validator easily)
public class OneValdiationRuleExplicitlyExposedAndEasyToTest : IValidator<Person>
{
    public bool IsSatisfiedBy(Person target)
    {
        return target.Age > 18;
    }
}

public class Person
{
    public int Age { get; set; }
}

public interface IValidator<TTarget>
{
    bool IsSatisfiedBy(TTarget target);
}

And I will use this code like this:
//usage
public class SomeCommandHandler
{
    private readonly IValidatorRuner _validatorRuner;

    public SomeCommandHandler(IValidatorRuner validatorRuner)
    {
        _validatorRuner = validatorRuner;
    }

    public void SomeMethod()
    {
        _validatorRuner.Run(new Person{Age = 16});
    }
}

Validation was just one example, I also use it to fire domain events and to run pipelines and filters in the same pluggable way
Is using the service locator in this way an anti-pattern?
I know I might be hiding some dependencies, but the thing is that the dependencies are dynamically injected and discovered when the application initializes (Composition root)
Your thoughts will be greatly appreciated","In my opinion, the primary issue with your code sample is that the service locator is itself injected into the implementation of ValidatorRunner. For me, this is an anti-pattern, but perhaps not the one you're asking about.
Any answer I might give boils down to the capabilities of your service locator implementation. But for sure it should not be passed into the constructor of your class. Instead, the service locator should itself pass these things in when you ask it for an implementation of ""IValidatorRuner""
As an example, you can inject a factory that knows how to load the dynamic validator instances for a given type.",https://stackoverflow.com/questions/28574472/is-service-locator-an-anti-pattern-in-a-pluggable-architecture,2,1
Resolving a call-chain anti-pattern,"I've begun to notice something of an anti-pattern in my ASP.NET development. It bothers me because it feels like the right thing to do to maintain good design, but at the same time it smells wrong.
The problem is this: we have a multi-layered application, the bottom layer is a class handling calls to a service that provides us with data. Above that is a layer of classes that possible transform, manipulate, and check the data. Above that are the ASP.NET pages.
In many cases, the methods from the the service layer don't need any changes before going on the view, so the model is just a straight pass through, like:
public List<IData> GetData(int id, string filter, bool check)
{
    return DataService.GetData(id, filter, check);
}

It's not wrong, nor necessarily awful to work on, but it creates an odd kind of copy/paste dependency. I'm also working on the underlying service, and it also replicates this patter a lot, and there are interfaces throughout. So what happens is, ""I need to add int someotherID to GetData"" So I add it to the model, the service caller, the service itself, and the interfaces. It doesn't help that GetData is actually representative of several methods that all use the same signature but return different information. The interfaces help a bit with that repetition, but it still crops up here and there.
Is there a name for this anti-pattern? Is there a fix, or is a major change to the architecture the only real way? It sounds like I need to flatten my object model, but sometimes the data layer is doing transformations so it has value. I also like keeping my code separated between ""calls an outside service"" and ""supplies page data.""","I would suggest you use the query object pattern to resolve this. Basically, your service could have a signature like:
IEnumerable<IData> GetData(IQuery<IData> query);

Inside the IQuery interface, you could have a method that takes a unit of work as input, for example a transaction context or something like ISession if you are using an ORM such as NHibernate and returns a list of IData objects.
public interface IQuery<T> 
{
 IEnumerable<T> DoQuery(IUnitOfWork unitOfWork);
}

This way, you can create strongly typed query objects that match your requirements, and have a clean interface for your services. This article from Ayende makes good reading about the subject.",https://stackoverflow.com/questions/5394328/resolving-a-call-chain-anti-pattern,7,1
Concrete examples on why the 'Anemic Domain Model' is considered an anti-pattern [closed],"I apologize if this is a duplicate, but I couldn't find any concrete examples on the topic in related questions.
After reading Martin Fowler's article on the 'Anemic Domain Model', I'm left wandering as to why is this considered an anti-pattern. Even does the majority of enterprise developers consider it an anti-pattern, since AFAIK probably 90% of the j2ee applications are designed in an 'anemic' way ?
Can someone recommend further reading on the topic (other than the 'Domain Driven Design' book), or even better, give a concrete examples on how this anti-pattern is affecting application design in a bad way.
Thanks,","Given the following two classes:
class CalculatorBean  
{  
    //getters and setters  
}  

class CalculatorBeanService  
{  
   Number calculate(Number first, Number second);  
    {  
       //do calculation  
    }  
} 

If I understand correctly, Fowler is stating that because your CalculatorBean is just a bunch of getters/setters you don't gain any real value from it and if you port that object to another system it will do nothing.  The problem seems that your CalculatorBeanService contains everything that the CalculatorBean should be responsible for.  Which is not the best as now the CalculatorBean delegates all of its responsibility to the CalculatorBeanService",https://stackoverflow.com/questions/6293981/concrete-examples-on-why-the-anemic-domain-model-is-considered-an-anti-pattern,6,1
What could be considered an anti-pattern in JMS architectures?,"Is there something we could consider anti-patterns when using JMS/Asynchronous Messaging architectures ?
Sometimes, designs may lead us to prove side effects instead of achieving that performance we are looking for.
What are common cases in which we should take another solution, such as batch processing for example, in place of choosing asynchronous messaging?","There is nothing inherent in the JMS architecture which states you should use batch over asynchronous messaging. 
Note: some JMS solutions support batch processing and other messaging solutions often have asynchronous messaging.
What you should consider is; are the features and the performance of the JMS solution suitable for my requirements.  If not, consider something else.  You should design your application so that you can easily change your transport later if it turns out it is unsuitable.",https://stackoverflow.com/questions/39711713/what-could-be-considered-an-anti-pattern-in-jms-architectures,1,1
Book for Anti Design Patterns [closed],"I have heard a lot about anti patterns and would like to read a book on this,which book would you suggest for Anti patterns.",The Daily WTF is always a good place to find some choice examples of antipatterns.,https://stackoverflow.com/questions/2497811/book-for-anti-design-patterns,5,1
Is an application framework an anti-pattern?,"I've recently implemented a couple of similar-sized web applications, one of which used a ""framework"" and one of which I coded myself but used a set of existing (mostly open source) libraries to provide certain common functionality that I would otherwise have used a framework for.
I noticed the following:

The framework-based app was certainly quicker to set up - it effectively worked ""out of the box"". However over time, and as more functionality was added, it started to become increasingly complex to maintain. As soon as I needed something that didn't ""fit"" the framework then I found myself forced to resort to some ugly workarounds.
The library-based app required more code at the start to bring in and integrate the necessary libraries, i.e. it was necessary to write a reasonable amount of glue code at the beginning. But it turned out easier to extend and re-factor over time, because there weren't any constraints posed by the need to fit within the framework design.

From this personal experience I got the impression that the use of a framework might be considered an anti-pattern for long term application maintainability.
Is this really the case?","I think it's highly subjective. In my opinion application frameworks indeed are an anti-pattern, especially for larger, more complex projects. I think there are two reasons to it.
The biggest problem I see with frameworks is that by using a framework you give up control. JB Nizet writes ""nobody forces you to use the framework for every part of the application"", and that's great when it's the case, but unfortunately it usually isn't. Usually the framework has the control, you have your callbacks or derived classes, and when you want to do something extraordinary you can't.
This gets worse because, in general, it's very difficult to design a framework well. The worse the framework the more limiting it is and the more it gets on the way of the framework user. A lot of frameworks I've seen have some fundamental limitations that end up restricting the final application one way or the other. I wouldn't say only the framework creators are to blame, it's just a very difficult, if not impossible, task to try to create a framework that doesn't restrict the framework users.
Frameworks aren't all bad, though. If your project fits well to the framework and it won't grow beyond it, the framework does speed up the development. Small frameworks within an application tend to simplify things as well when they only take care of one problem domain, with usually no risk of getting too much in the way. But in the end frameworks, on one hand, add complexity you probably won't need, and on the other hand, couple the interface with the engine, restricting the framework users.",https://stackoverflow.com/questions/8026182/is-an-application-framework-an-anti-pattern,3,1
"Setting state form a data fetch, is this an Anti Pattern?","I'm trying to work out how to do an API call then get a timestamp for a specific location then start a clock for that location? I've got it working but I'm just wondering if this is correct? I'm going to stick this in a flux architecture eventually. I'm setting state from props, anti pattern? I only want to make one initial request then manipulate the interval, so I figured it would be best to use state.
Fiddle Here
class App extends React.Component {
  constructor(props) {
    super(props);
    this.state = {
      time: 0
    };
  }

  componentDidMount() {
    const URL = 'http://api.timezonedb.com/?lat=37.773972&lng=-122.431297&format=json&&key=XXXXXXXXXXX';
    const _this = this;
    fetch(URL)
      .then(function(response) {
        return response.json();
      }).then(function(data) {
        console.log(data);
        _this.setState({
          time: data.timestamp
        });
      }).catch(function() {
       reject(err);
      });
      this.interval = setInterval(this.tick.bind(this), 1000);
  }
  tick() {
    this.setState({
      time: this.state.time + 1
    });
  }
  componentWillUnmount() {
    clearInterval(this.interval);
  }
  render() {
     return (
        <div>
            <p>Time on West Coast:</p> {moment.unix(this.state.time).format('YYYY-MM-DD HH:mm:ss')}
        </div>

     );
  }
}

ReactDOM.render(
  <App/>,
  document.getElementById('container')
);","This is absolutely not an anti-pattern. Check out this example on the Flux Github repository where a component registers for data updates (like your setInterval) in its componentDidMount method.
  componentDidMount: function() {
    this._scrollToBottom();
    MessageStore.addChangeListener(this._onChange);
    ThreadStore.addChangeListener(this._onChange);
  },

Then, just like you, it unregisters in componentWillUnmount.
  componentWillUnmount: function() {
    MessageStore.removeChangeListener(this._onChange);
    ThreadStore.removeChangeListener(this._onChange);
  },

The only difference is in how it gets its data initially, in its ""constructor""
getInitialState: function() {
    return getStateFromStores();
  },

It gets its state from the store. You are collecting data from an api in componentDidMount. This is a common thing to do before stores get involved. Once you have a store, all data access should go through it. This allows multiple components to get the same data without multiple requests being made. It also encapsulates the collection methods, which helps with testability.
You say you are setting state from props, but I don't see this in your code.",https://stackoverflow.com/questions/34455218/setting-state-form-a-data-fetch-is-this-an-anti-pattern,1,1
C# Dependency injection side effect (two step initialization anti-pattern)? [duplicate],"This question already has an answer here:


Is there a pattern for initializing objects created via a DI container

                    5 answers
                



I'm working on a project in which my constructors contain - only - behavioral dependencies. i.e. I never pass values / state.
Example:
class ProductProcessor : IProductProcessor
{
   public double SomeMethod(){ ... }
}
class PackageProcessor
{
   private readonly IProductProcessor _productProcessor;
   private double _taxRate;

   public PackageProcessor(IProductProcessor productProcessor)
   {
        _productProcessor = productProcessor;
   }

   public Initialize(double taxRate)
   {
       _taxRate = taxRate;
       return this;
   }

   public double ProcessPackage()
   {
       return _taxRate * _productProcessor.SomeMethod();
   }

}

In order to pass state, it was decided to include a second step (a call to Initialize).
I know we can configure this as a named parameter in the IoC Container config class, however, we did not like the idea of creating  ""new namedParameter(paramvalue)'s"" in the configuration file as it makes it unnecessarily unreadable and creates a future maintenance pain spot.
I've seen this pattern in more than one place.
Question: I read some consider this two step initialization an anti-pattern. If that is the consensus, wouldn't this imply a limitation / weakness of sorts in the approach of dependency injection via a IoC container? 
Edit:
After looking into Mark Seeman's suggestion:
and the answers to this one, I have a few comments:
Initialize/Apply : Agree on it being an anti pattern / smell.
Yacoub Massad: I agree IoC containers are a problem when it comes to primitive dependencies. Manual (poor man's) DI, as described here sounds great for smaller or architecturally stable systems but I think it could become very hard to maintain a number of manually configured composition roots.
Options:
1)Factories as dependencies (when run time resolution is required)
2) Separate stateful object from pure services as described here.
(1): This is what I had been doing but I realized that there is a potential to incur into a another anti-pattern: the service locator.
(2): My preference for my particular case is this one about this one as I can cleanly separate both types. Pure services are a no brainer - IoC Container, whereas stateful object resolution will depend on whether they have primitive dependencies or not.
Every time I've 'had' to use dependency injection, it has been used in a dogmatic way, generally under the orders of a supervisor bent on applying DI with IoC container at any cost.","I read some consider this two step initialization an anti-pattern

The Initialize method leads to Temporal Coupling. Calling it an anti-pattern might be too strict, but it sure is a Design Smell.
How to provide this value to the component depends on what type of value it is. There are two flavors: configuration values and runtime values:

Configuration Values: If it is a constant/configuration value that won't change during the lifetime of the component, the value should be injected into the constructor directly.
Runtime values: In case the value changes during runtime (such as request specific values), the value should not be provided during initialization (neither through the constructor nor using some Initialize method). Initializing components with runtime data actually IS an anti-pattern.

I partly agree with @YacoubMassad about the configuration of primitive dependencies using DI containers. The APIs provided by containers do not enable setting those values in a maintainable way when using auto-wiring. I think this is mainly caused by limitations in C# and .NET. I struggled a long time with such API while designing and developing Simple Injector, but decided to leave out such API completely, because I didn't find a way to define an API that was both intuitive and lead to code that was easy maintainable for the user. Because of this I usually advise developers to extract the primive types into Parameter Objects and instead register and inject the Parameter Object into the consuming type. In other words, a TaxRate property can be wrapped in a ProductServiceSettings class and this Parameter Object can be injected into ProductProcessor.
But as I said, I only partly agree with Yacoub. Although it is more practical to compose some of your objects by hand (a.k.a. Pure DI), he implies that this means you should abandon DI containers completely. IMO that is too strongly put. In most of the applications I write, I batch-register about 98% of my types using the container, and I hand-wire the other two 2%, because auto-wiring them is too complex. This gives in the context of my applications the best overall result. Of course, you're mileage may vary. Not every application really benefits from using a DI container, and I don't use a container myself in all the application I write. But what I always do however, is apply the Dependency Injection pattern and the SOLID principles.",https://stackoverflow.com/questions/33838048/c-sharp-dependency-injection-side-effect-two-step-initialization-anti-pattern,2,1
What are horizontal and vertical design elements?,"I was going through this article on Architectural Anti-Patterns and it talks about horizontal and vertical design elements getting intermixed to form an unstable architecture.
I was not able to google the definition of these terms.","Horizontal and Vertical elements as the author of the article defines them are 

""Vertical design elements are dependent upon the individual
  application and specific software implementations. Horizontal design
  elements are those that are common across applications and specific
  implementations""

Usually the use of these terms is in the context of a single solution and then the roles are a little different where horizontal elements are those relating to a specific layer e.g. components for data access, UI element and vertical relate to cross cutting concerns like authentication, auditing etc.",https://stackoverflow.com/questions/31613992/what-are-horizontal-and-vertical-design-elements,2,1
"Orchard, Plug-in based architecture, design patterns","I'm doing research on software architecture, layering, and looked lots of open source .net projects, like Orchard CMS.
I think Orchard is a good example for some design patterns.
As I know, UI, Services, Repositories and Entities should be in separate assemblies, due to misusing. But in Orchard, (due to being modularity and pluggable) I see service, repository and entity classes and interfaces in same folder and same namespace.
Isn't it an anti-pattern, or is it correct for patterns?","TL;DR: assemblies are not necessarily the right separation device.
No, what's important is that they are separated, not that they are in separate assemblies. Furthermore, the way you would factor things in most applications has to be different from what you do in an extensible CMS. The right separation in an extensible CMS is into decoupled features that can be added and removed at will, whereas regular tiered applications require decoupling of layers so those can be worked on and refactored with minimal risk and impact. The right comparison is actually between one of those applications and a module or feature in Orchard, not with Orchard as a whole. But of course, good practices should be used within modules, and they usually are.
Now separation into assemblies is a separate concern, that is more technical than architectural. You can see an assembly as a container of self-contained code, created for the purpose of code reuse and dynamic linking, but not especially as a way to separate layers. This is why they coincide in Orchard with the unit of code reuse, the module.
Also consider the practical aspect of this: good architectural practices have one main goal, which is to make applications easier and cheaper to maintain (and not, surprisingly (NOT!) to make consultants rich by enabling them to set-up astronaut architectures that only they can understand). A secondary goal is to codify what makes scalable and well-performing applications (although that is a trickier goal as it can easily lead to premature optimization, the root of most software evil).
For that first goal, conceptual separation is the most important, but the way this separation is made is usually not very important.
The secondary goal unfortunately conflicts with the idea of using assemblies as a separation device: Orchard as it is already has dozens of assemblies before you even start to add optional modules. And assemblies do not come for free. They need to be dynamically compiled, loaded, jitted, come with memory overhead, etc. In other terms, for good performance, you'll usually want to reduce the number of assemblies.
If you wanted to separate an Orchard site into assemblies for modules as it is today, and then separate each of these modules into layered assemblies, you would have to multiply the number of modules by the number of layers. That would be hundreds of assemblies to load. Not good. As a matter of facts, we are even considering an option for dynamic compilation to build all modules into a single assembly.",https://stackoverflow.com/questions/10774185/orchard-plug-in-based-architecture-design-patterns,1,1
iOS Singleton class inside MVC architecture,"Most of the iOS apps are based on MVC Design pattern, and we are using Singleton classes inside our apps from many years. I know that Singleton itself is a design pattern and many consider it as anti-pattern and all that good bad stuff about singletons. 
But if you consider MVC as a overall architecture to your app development I would like to understand where does a singleton be categorised in it. Is it a Model, or View or Controller? if so why? if it does not belong to any of these three then what it is?
This might seem a silly question but it keeps bothering me as I was asked this question in an interview in a reputed company.","MVC is an architectural pattern, whereas Singleton is a design pattern. The two things are not related, and therefore Singleton is not 'anything' in MVC.
Leaving aside any political discussion about the use of Singletons, you could in theory decide to make any Model, View or Controller class a Singleton. 
As gnasher has pointed out, within the iOS SDK, ViewControllers are created and destroyed by the runtime, which means it would be illogical for them to be Singletons in that context, regardless of theory. 
A more likely scenario is that you want your Model to be available to multiple Views. You might decide to make that Model a Singleton which can be accessed through a dependency injection framework, or constructor injection. In that case, your class is both a Model class, AND a Singleton. They are not mutually exclusive.
I wonder if that is what your interviewers meant - where would you most likely want to use a Singleton in an iOS app? In that case, the correct answer would be in the Model.",https://stackoverflow.com/questions/37432973/ios-singleton-class-inside-mvc-architecture,1,1
Dependency Injection vs Layered Architecture,"I've been reading a lot about dependency injection and the service locator (anti-?) pattern - a lot of it on StackOverflow (thanks guys :). I have a question about how this pattern works when it's within a n-layer architecture.
I've seen a lot of blog posts where they describe injecting a IDataAccess component into the business objects. E.g.
public class Address
{
    IDataAccess _dataAccess;
    public Address(IDataAccess dataAccess)
    {
        this._dataAccess = dataAccess;
    }
}

However, I was under the impression that in an n-layer architecture, the UI layer should not need to have any knowledge of the data access layer... or even know that there /is/ a data access layer! If DI requires exposing the IDataAccess interface in the constructors of the BusinessObjects, this then exposes to the UI the fact that the Business Layer uses a data access layer under the hood - something the UI doesn't need to know or care about surely?
So, my fundamental question is: Does DI require that I expose all my lower layer interfaces to all upper layers and is this a good or a bad thing?
Thanks
Edit: To clarify (after a few comments), I know my business object should be ignorant of the which specific implementation of which IDataAccess it uses (hence the Dependency being injected in the constructor) but I thought that the layers above the BO should not know that the Business Object even requires a dependency on a DAL.","I think the answer is rather simple. Your bottom layers (interface, bll, dal, entities) are just a bunch of libraries. It is up to the client to decide which libraries to be used and it will increase client's flexibility. Moreover they are libraries, so any application-related configurations (connection strings, data caching, etc) lies on the client. Those configuration itself, sometimes also need to be injected and included into Composition Root.
However, if you want to has an uniform logic and not client's flexibility, you can choose web/app services as an additional layer.
1st Layer        Entities

2nd Layer       Interface

3rd Layer       BLL  &  DAL

4th Layer    Web/App Services

5th Layer           UI

This way, your composition root exists in one layer (4th). And add your UI just need to add service reference to 4th layer (or 1st if needed). However, this implies the same Mark Seeman's article again, layering is worth the mapping. I assume that you can change the app/web service to Composition Root.
Moreover, this (app/web service) design has pros/cons. Pros:

Your app is encapsulated
Your app is being bridged by app/web services. It is guranteed that your UI don't know the DataAccess, thus fulfill your requirements. 
Your app is secured
Simply said, having UI need to access app service is a huge gain in security aspect.
Access Portability
Now your app can be accessed everywhere. It can be connected by 3rd party app (other web) without has relying on dlls.

Cons:

Overhead cost during service call
Authentication, network connection, etc, will cause overhead during webservice call. I'm inexperienced for the performance impact but it should be enough for high traffic app.
Inflexibility of client
Client now need to access BLL/Services by using services instead of normal objects.
More Service for Different Type of Client
Now you need to provide more service than needed. Such as WebRequestRetriever, MobileRequestRetriever instead of accessing to a mere IRequestRetriever and let the composition root wire up the rest.

Apologize if this answer boarden the topic (just realized after finished).",https://stackoverflow.com/questions/17919823/dependency-injection-vs-layered-architecture,3,1
Need help in iOS app's architecture. Use Singletons or not? [closed],"I have a class ServiceClass, a closure function in this class eg LoginViaEmail, calls APILayer. APILayer calls HttpLayer and JSONHandler and returns output to ServiceClass. ServiceClass further returns some info to ViewController when completionblock is received. 
My questions are

Shall I make ServiceClass as Singleton? But then everybody says that they are anti-pattern and not a good approach.
If they are so bad, why does Apple use them? eg NSFileManager, NSWorkspace, UIApplication etc,
If not Singleton, then what shall I use? Make instance of ServiceClass every time I use it? Wouldn't that take too much memory?","Don't use a bunch of singletons if you want to write effective, independent and uncoupled, tests. If you don't care about tests, then singletons are fine.
Here's a half-way-house architecture that I use:

Have an overall App or Container class that owns all the services, and creates them, and injects them into each other as needed (so your services have all dependencies as properties, and don't access any statics)
Have your UIAppDelegate class own the single instance of the Container. This is the only 'singleton' in the app.
UIViewControllers etc get references to the Container, and its services through the UIAppDelegate, at viewDidLoad time, and save them into local properties. Eg they inject themselves. It would be nice to make this automatic.
Tests can set up and tear down the Container, or individual services as needed, knowing that other tests can't effect them.",https://stackoverflow.com/questions/44671130/need-help-in-ios-apps-architecture-use-singletons-or-not,3,1
What is so bad about singletons? [closed],"The singleton pattern is a fully paid up member of the GoF's patterns book, but it lately seems rather orphaned by the developer world. I still use quite a lot of singletons, especially for factory classes, and while you have to be a bit careful about multithreading issues (like any class actually), I fail to see why they are so awful.
Stack Overflow especially seems to assume that everyone agrees that Singletons are evil. Why?
Please support your answers with ""facts, references, or specific expertise""","Paraphrased from Brian Button:

They are generally used as a global instance, why is that so bad? Because you hide the dependencies of your application in your code, instead of exposing them through the interfaces. Making something global to avoid passing it around is a code smell.
They violate the single responsibility principle: by virtue of the fact that they control their own creation and lifecycle.
They inherently cause code to be tightly coupled. This makes faking them out under test rather difficult in many cases.
They carry state around for the lifetime of the application. Another hit to testing since you can end up with a situation where tests need to be ordered which is a big no no for unit tests. Why? Because each unit test should be independent from the other.",https://stackoverflow.com/questions/137975/what-is-so-bad-about-singletons?rq=1,36,1
Is it anti-pattern to alter domain model on front end?,"We are making a quiz application, I'm trying to integrate my Angular 2 UI with the REST api.
Our Quiz domain model consist of the following (simplified) hierarchy:

Quiz
Category
Question
Choice

where parent doesn't know it's children, but child knows it's parent. For example, Choice has a reference for a Question, but Question doesn't have reference to choice. We chose this approach to be able to fetch the quiz data more flexible and modular approach, also avoiding circular references.
However, in front end it's counter-intuitive to use inverted linking, as views are built naturally layer-by-layer iterating deeper in the domain object structure. It makes sense to render view for Question first, and render sub-view for Choices after. It just seems impossible with the current domain model, where I should start from Choice.
My question is, if it's common or approved to convert the domain model on the front end, so I wold gather all data and add Choice reference to Question afterwards, making the model compatible for top-down approach? And of course convert it back when POSTing to REST api.
Does this indicate bad design, or is it approved to alter the domain model?","It's not ""convert domain model on front-end"" because your front-end is not playing with domain model. It's playing with Data Transfer Object (json object returned from calling server API in this case). So feel free to do everything you want with DTO on client.",https://stackoverflow.com/questions/37707672/is-it-anti-pattern-to-alter-domain-model-on-front-end,1,1
3-layer architecture - passing data between layers,"Trying to implement 3-layer (not: tier, I just want to separate my project logically, on one machine) architecture I've found so many different approaches that I'm confused, what's the best way (if there's any) to make that in WinForms app.  
Now I have no doubts only about 3 layers that should be present in the project:  

UI (Presentation Layer)  
BLL (Business Logic Layer)  
DAL (Data Acces Layer)  

In UI I put all the WinForms. There must be also some logic to fill the object with data from controls and pass it to BLL layer.  
In DAL I want to put classes and methods for data manipulations using ADO.NET, like: 
public class OrderDAL
{
    public OrderDAL()
    {
    }

    public int Add(Order order)
    {
        //...add order to database
    }

    public int Update(Order order)
    {
        //...update order in database
    }

    //...etc.
}

The problem is with BLL and the question - should I use Data Transfer Objects to pass data between layers, or should I pass the whole Class?
If I choose to use DTO, then I've to create additional common class, Order, that reference to UI, BLL and DAL:  
public class Order
{
    public int Id { get; set; }
    public DateTime Date { get; set; }
    public string Number { get; set; }
    public string CustomerName { get; set; }

    public Order ()
    {
    }
}

and put the logic separated into BLL:
public class OrderBLL
{
    public OrderBLL()
    {
    }

    public int Add(Order order)
    {
        OrderDAL orderDAL = new OrderDAL();
        return orderDAL.Add(order);
    }

    public int Update(Order order)
    {
        OrderDAL orderDAL = new OrderDAL();
        return orderDAL.Update(order);
    }

    //...etc.
}

This approach, under different names, is used among others: here or here.
On the other hand, some ""wise guys"" and their followers (like here) call it Anemic Domain Model and complain it's a bad design and anti-pattern that should not be used.  
The pros: 

DTO can easily by design to represent Database table,
it's light and clear, contains only fields needed for database,
DAL doesn't have to reference BLL,

The cons: 

anti-pattern (sounds scary ;P),
violation of OOP (separated properties from methods),
because logic is in different class, it may be more difficult to maintain when something changes.

So, the opposite approach is to pass the whole object between layers, like here: no DTO, just BLL looking like that:
public class Order
{
    public int Id { get; set; }
    public DateTime Date { get; set; }
    public string Number { get; set; }
    public string CustomerName { get; set; }

    public Order()
    {
    }

    public int Add()
    {
        OrderDAL orderDAL = new OrderDAL();
        return orderDAL.Add(this);
    }

    public int Update(Order order)
    {
        OrderDAL orderDAL = new OrderDAL();
        return orderDAL.Update(order);
    }
}

The pros: 

it's a nicely encapsulated object, following OOP rules (I suppose ;)). 
both logic and properties are in one place, easier to maintain and debug.

The cons: 

to use the object, DAL has to reference BLL (that's not how the 3-tier layer should do, isn't it?). 
class may contain some fields that are not used in Database, as well as some fields from Database (like Id) do not represent ""real life"" object.

So, it looks like whatever I choose, I'll violate some rules. What's better way then, which should I choose? Maybe there is other approach I haven't found?","I don't like DTOs, because they mean creating a dual hierarchy with little or no value.
I also don't like the idea of making model objects responsible for their own persistence.  I prefer a separate persistence layer.  Why?  Model objects don't always need to be persisted to be useful.  Business logic and functionality are orthogonal to persistence.  
If you have two layers it's possible to keep a one way dependency graph: persistence knows about model, but model does not know about persistence.  You end up with a cyclic dependency if model objects are responsible for persistence.  You can never test or use model objects without persistence.
My advice?  Don't do DTOs.  Break out a separate persistence layer.",https://stackoverflow.com/questions/11259679/3-layer-architecture-passing-data-between-layers,1,1
Understanding DTO and Anemic Domain Model,"I am new to Domain Pattern, I need to ensure that I understand what I had read so far!!,  Please tell me whether the following sentences are true or did not violate a principle related to DDD

0) DAL will receive parameters in DTO and return fetched data in LIST of DTO (Entity)
1) De-couple BLL and DAL through repository pattern.
2) Entity is DTO object.
3) ProductCategoryData contains a list of ProductData.
4) It will be Anemic Domain Model ANTI Pattern if BLL.ProductCategory does not contain properties that describe the business object.
5) BLL.ProductCategory contains a List of BLL.Product_︹I have bad feeling about this
6) I avoid in that design anemic domain model anti pattern.
7) I successfully Apply Domain Model Pattern.
8) I used DTO objects to transfer data between tiers.
Please talk to me :)","If that is the interface and there are no methods on your objects then that is still an anemic model.
The Repository should be associate to the aggregates of the model. 
I your model only contains those entities then does not really matter how bad or good the design is
because the overall complexity will be low.
Also choose better names for your model, and avoid generic names like ""Data"". 
The reader immediately asks: what kind of data ?",https://stackoverflow.com/questions/6428508/understanding-dto-and-anemic-domain-model,2,1
What's the best way to implement clean UI functionality in WinForms while maintaining a decent decoupled architecture?,"I tend to implement UI functionality using fairly self-documenting void doSomething() methods, i.e. if the user presses this button then perform this action then enable this list box, disable that button, etc. Is this the best approach? Is there a better pattern for general UI management i.e. how to control when controls are enabled/disabled/etc. etc. depending on user input?
Often I feel like I'm veering towards the 'big class that does everything' anti-pattern as so much seems to interact with the 'main' form class. Often, even if I'm including private state variables in the class that have been implemented using a relatively modular design, I'm still finding it grows so quickly it's ridiculous.
So could people give me some good advice towards producing quality, testable, decoupled WinForms design without falling into these traps?","You can try MVP if you want to put the logic of the UI in a separate class..
In model view presenter just as Martin Fowler or Michael Feathers say, the logic of the UI is separated into a class called presenter, that handles all the input from the user and that tells the ""dumb"" view what and when to display. The special testability of the pattern comes from the fact that the entire view can be replaced with a mock object and in this way the presenter, which is the most important part, can be easily unit tested in isolation.",https://stackoverflow.com/questions/124096/whats-the-best-way-to-implement-clean-ui-functionality-in-winforms-while-mainta,4,1
Api gateway or No Api Gateway,"I am developing an application based on the microservice architecture. Here, each service is an independently deployable play-scala application exposing rest apis. I want to implement an Api gateway on top of these services for mapping incoming requests.I am following the architecture discussed here :Building Microservices
There are very few projects with substantial maturity that are based on the microservice architecture. One of them is Reactive Microservices.But this project is not using the api gateway pattern and seems to be following the Anti Pattern There is an issue opened for this project regarding the missing Api Gateway here .The contributors here claim that they did not follow the api gateway pattern because it has the risk of single-point of failure. 
This varying opinion is very confusing to me. So,I am looking for the suggestions on whether I should be using Api Gateway or not. What is the right practice here ?","The API gateway doesn't introduce a single point of failure any more than a load balancer does. Any serious API gateway should be able to run in high availability mode removing the single point of failure.
The API gateway encourages good documentation & planning within teams. Some API gateways allow you to import Swagger Specifications https://swagger.io/ in order to create the API.
Some gateways allow you to create virtual endpoints to mock responses of an upstream target. That way, if your service is not available quite yet, you can still code to it, and switch to the targets when ready.
API gateways should be able to round robin load balance your upstream targets, negating the necessity for adding a dedicated load balancer. You can also configure your gateway to periodically hit a healthcheck endpoint, and automatically remove targets from LB if service is not available.
Gateways will handle auth for you. Whether that be via JWT, Oauth, Simple, Open etc. Your developers can concentrate on building their microservices. Your microservices can be micro. The gateway would sit at the edge of the infrastructure, and handle security for you.",https://stackoverflow.com/questions/34442192/api-gateway-or-no-api-gateway,1,1
How to answer the interview question: What is a singleton and how would you use one? [closed],"I've read the questions on S.O. regarding Singleton and just watched an hour long google tech talk.  As far as I can tell, the consensus in the OO world seems to be that singletons are more of an anti-pattern rather than a useful design pattern.
That said, I am interviewing these days and the question comes up a lot--what is a singleton, and how would you use it?
What is the best way to answer this question?  Should I simply describe the design pattern and then say the only acceptable use I've heard of is for logging, and that it is often mis-used for global state?","Tricky. There are plenty of people who feel that singletons are essentially an anti-pattern like you said (me included), but there are also a lot who feel that it is simply an OOP-acceptable way to do globals.
If the interviewer is in that camp then yes, I think he's wrong, but that might not be the best thing to say during the interview. ;)
So I'd probably try to be neutral and stick to the facts. You don't know which camp your interviewer falls in, so stick to the indisputable facts. What does a singleton do? And to demonstrate usage, stick to the few cases where most people can agree that a singleton is an acceptable answer. Or explain your experience (since people can't disagree with that either).
But whether you're ""for"" or ""against"" singletons, an interview is probably not the right time to crusade for that cause. ;)",https://stackoverflow.com/questions/815528/how-to-answer-the-interview-question-what-is-a-singleton-and-how-would-you-use,9,1
centralized settings with microservices,"Microservices are all about decomposing your system into separate components.
However, some things in a system seem like centralized in nature.
My concern is about the system settings.
In a monolith you have one big file / db with all the parameters, settings and preferences.
This can be updated, backup, restore, export, import etc (think about Windows registry). More than this, your customers are used to go to this one ""place"" and set the system.
With microservices architecture this ""centralism"" seems like an anti pattern.
What are the mechanisms/ frameworks to deal with such contradiction?","Have you already looked at projects like ZooKeeper, etcd or Consul? These can provide facilities to manage your configuration settings and service discovery.",https://stackoverflow.com/questions/36541906/centralized-settings-with-microservices,1,1
What is this ORM pattern called,"I have been used to frameworks like JPA to do the object-relational mapping between database rows and Java objects.
However, in my company we use proprietary framework for ORM, which doesn't use entity classes to represent an entity but just a java.util.Map class with database column values mapped to their names.
Typically such a map works also as a model for the presentation tier to render the form. Posting the form again injects the parameters to the handler methods as a map. It is an old framework from early 2000's.
Even thought the use of maps over entity classes smells like obsolescent and an anti pattern, I actually like the ""dynamic"" nature of this model. You can easily add any data to the map in the business logic layer before map gets passed to the presentation tier, and it becomes part of the form with only changes to the template itself. It allows you to augment any entity with anything, if necessary. And this is ultimately useful, if you must for example show certain notification for a certain field under certain conditions. You just check the condition, add the notification to map if necessary, and in template render it if it exists in the map. If I used entity classes, I would have needed to refactor the entity class interface with an attribute, which is not even real attribute of the entity. The business logic is full of these special conditions and they are constantly evolving.
Is this dangerous thinking - am I falling for an anti pattern? Or is it justified to use this kind of pattern with complex business domains, and does this pattern have a name?","I don't know a pattern that would fit your description, but using Maps over entity classes also has disadvantages.

You can easily add any data to the map in the business logic layer before map gets passed to the presentation tier, and it becomes part of the form with only changes to the template itself.

What you see as an advantage means that you can never be sure in which state the Map is. If you load an entity through service 1 the map might contains some values. If you load the same entity through another service 2 it might not contains some values. The state of the Map is use case specific.
Since you are using a Map it also means that from a client's perspective each property has the same type, e.g. Map<String, Object>. So if you see an entity map in your code you also have to know which type it has. Without knowing that it would be hard e.g. to calculate an orders total.
As long as you are the only one that works on the code and you know all Map states you will think that it is flexible and fast. If you haven't worked on such code for some time you might forget the different states that the Map can have. You have then to go through the whole code for each use case to see if some piece of code adds or removes properties or just replaces the properties type.
So try to think of that design from a documentation perspective. The big advantage of a dedicated entity class is that it is named. You can give a class a name and therefore a meaning. E.g.
public class PlacedOrder { ... }
public class Refund { ... }

You can also easily add javadoc to the class to give a bit more detailed information of what this class means in your context. This is usually called the Ubiquitous Language. I often just use the term domain language, because it's easier to speak.
In my experience such design often lead to a hard to maintain system. At the beginning when the code base is small you are really fast. But this is not amazing, because you often save documentation time by not applying clean code principles.
Since types can't be checked by the compiler you will find type problems only at runtime. This means that you must create a big test harness, because the tests must also uncover the bugs that otherwise a compiler would report. Or you just develop the trial and error way, but this is not my way and does not satisfy me.",https://stackoverflow.com/questions/48354022/what-is-this-orm-pattern-called,1,1
Is it wrong to store all shared data in the parent component?,"I am working with AngularJS 1.6.
I am working in a system in which the shared data is stored in a top level component. The child components access the parent through:
require: {
  'parentView': '?^'
}

and access the data with {{thisView.parentView.variable}} and this.parentView.changeVar(var);
This seems like an anti-pattern. It seems that the ""right"" solution would be to use a service to store and change the data.
What is actually the issue with the parentView approach? 
Thanks for your help.","The main issue I see is that child components are hardly reusable in such scenario since they are tightly coupled with parent component.
So that's why injecting service with data is more preferable and easier to test with unit tests. And unit tests prevent your code to become legacy.",https://stackoverflow.com/questions/48935487/is-it-wrong-to-store-all-shared-data-in-the-parent-component,2,1
Javascript - Retaining the value of an iteration in a jQuery function call,"Edit (torazaburo was correct):
For future reference, and since there appears to be a lot of confusion about closures by many who commented, here are some notes.
You can write the code as I did below, in fact it was correct. The problem was, I simplified my specific case too much. It was not the way I wrote the closure. My problem was the DOM element was a table cell td, which had not been appended to the row (newRow.appendChild(newCol)) at that specific point in time when I was adding the hover. Consequently the cell was not in the DOM at that point in time. Consequently jQuery did not know it existed.
The original question:
My code:
var i, strId;

for(i=0;i<10;i++){
    // strId is the id of the element without the # needed for jquery
    strId = ""idString"" + i.toString();

    // Some stuff ...

    // It is done this way so the value of i is retained in the functions in the hover
    (function (i, strId) {
        $(""#"" + strId).hover(function () { MyFunctionIn(i);}, function () { MyFunctionOut(i); });
    })(i, strId);
}

This does not work as written.
The value of i is being retained, as I have tested this with just $(""#idString0"") in place of $(""#"" + strId) and this works fine.
But when I put the variable strId back into the hover it fails.
Anyone know what I am doing wrong?","This is not a direct answer
You are experiencing problems with function scopes.  Despite the scope problem, I think you may have an architectural anti-pattern forming. In one of your comments you mentioned that you cannot use on because ""different elements have different things they do when I am hovering over them"". This can simply be handled by a strategy pattern. 
Reference this Plunker and the Jquery documentation
HTML
<div data-type=""a"" data-id=""1"" class=""hover"">A Type Element</div>
<div data-type=""a"" data-id=""2"" class=""hover"">A Type Element</div>
<div data-type=""b"" data-id=""1"" class=""hover"">B Type Element</div>

Javascript
// Define a service to help with hovering behaviours
function hoverService() {
  var eventMap = {
    mouseenter: 'On',
    mouseleave: 'Off'
  };
  var service = {
    hoverStrategy: hoverStrategy,
    aOn: aOn,
    aOff: aOff,
    bOn: bOn,
    bOff: bOff
  };
  return service;
  // Use this as a proxy to determine the right action to take
  function hoverStrategy(event) {
    if (!event || !event.currentTarget) {
      return;
    }
    var $element = $(event.currentTarget);
    var id = $element.data('id');
    var elementType = $element.data('type');
    var eventType = eventMap[event.type];
    var strat = elementType + eventType;
    return service[strat] ? service[strat](id, $element) : false;
  }

  function aOn(id, $element) {
    console.log(id, $element, 'aOn');
  }

  function aOff(id, $element) {
    console.log(id, $element, 'aOff');
  }

  function bOn(id, $element) {
    console.log(id, $element, 'bOn');
  }

  function bOff(id, $element) {
    console.log(id, $element, 'bOff');
  }

}

$(document).ready(function() {
  var service = hoverService();
  $('.hover').on('mouseenter', service.hoverStrategy);
  $('.hover').on('mouseleave', service.hoverStrategy);
});",https://stackoverflow.com/questions/38152313/javascript-retaining-the-value-of-an-iteration-in-a-jquery-function-call/38152668,1,1
"Seemann's Dependency Injection, __hree Calls Pattern_ vs Service Locator Anit-Pattern","I have created a WinForms MVC application using Dependency Injection (DI) and Ninject as the DI Container. The basic architecture is as follows 
Program.cs (the main entry point of the WinForms application):
static class Program
{
    [STAThread]
    static void Main()
    {
        ...

        CompositionRoot.Initialize(new DependencyModule());
        Application.EnableVisualStyles();
        Application.SetCompatibleTextRenderingDefault(false);
        Application.Run(CompositionRoot.Resolve<ApplicationShellView>());
    }
}

DependencyModule.cs 
public class DependencyModule : NinjectModule
{
    public override void Load()
    {
        Bind<IApplicationShellView>().To<ApplicationShellView>();
        Bind<IDocumentController>().To<SpreadsheetController>();
        Bind<ISpreadsheetView>().To<SpreadsheetView>();
    }
}

CompositionRoot.cs
public class CompositionRoot
{
    private static IKernel ninjectKernel;

    public static void Initialize(INinjectModule module)
    {
        ninjectKernel = new StandardKernel(module);
    }

    public static T Resolve<T>()
    {
        return ninjectKernel.Get<T>();
    }

    public static IEnumerable<T> ResolveAll<T>()
    {
        return ninjectKernel.GetAll<T>();
    }
}

ApplicationShellView.cs (the main form of the application)
public partial class ApplicationShellView : C1RibbonForm, IApplicationShellView
{
    private ApplicationShellController controller; 

    public ApplicationShellView()
    {
        this.controller = new ApplicationShellController(this);
        InitializeComponent();
    }

    public void InitializeView()
    {
        dockPanel.Extender.FloatWindowFactory = new CustomFloatWindowFactory();
        dockPanel.Theme = vS2012LightTheme;
    }

    private void ribbonButtonTest_Click(object sender, EventArgs e)
    {
        controller.OpenNewSpreadsheet();
    }

    public DockPanel DockPanel
    {
        get { return dockPanel; }
    }
}

where 
public interface IApplicationShellView
{
    void InitializeView();
    DockPanel DockPanel { get; }
}

ApplicationShellController.cs
public class ApplicationShellController
{
    private IApplicationShellView shellView;

    public ApplicationShellController(IApplicationShellView view)
    {
        this.shellView = view;
    }

    public void OpenNewSpreadsheet(DockState dockState = DockState.Document)
    {
        SpreadsheetController controller = (SpreadsheetController)GetDocumentController(""new.xlsx"");
        SpreadsheetView view = (SpreadsheetView)controller.New(""new.xlsx"");
        view.Show(shellView.DockPanel, dockState);
    }

    private IDocumentController GetDocumentController(string path)
    {
        return CompositionRoot.ResolveAll<IDocumentController>()
            .SingleOrDefault(provider => provider.Handles(path));
    }

    public IApplicationShellView ShellView { get { return shellView; } }
}

SpreadsheetController.cs
public class SpreadsheetController : IDocumentController 
{
    private ISpreadsheetView view;

    public SpreadsheetController(ISpreadsheetView view)
    {
        this.view = view;
        this.view.SetController(this);
    }

    public bool Handles(string path)
    {
        string extension = Path.GetExtension(path);
        if (!String.IsNullOrEmpty(extension))
        {
            if (FileTypes.Any(ft => ft.FileExtension.CompareNoCase(extension)))
                return true;
        }
        return false;
    }

    public void SetViewActive(bool isActive)
    {
        ((SpreadsheetView)view).ShowIcon = isActive;
    }

    public IDocumentView New(string fileName)
    {
        // Opens a new file correctly.
    }

    public IDocumentView Open(string path)
    {
        // Opens an Excel file correctly.
    }

    public IEnumerable<DocumentFileType> FileTypes
    {
        get
        {
            return new List<DocumentFileType>()
            {
                new DocumentFileType(""CSV"",  "".csv"" ),
                new DocumentFileType(""Excel"", "".xls""),
                new DocumentFileType(""Excel10"", "".xlsx"")
            };
        }
    }
}

where the implemented interface is 
public interface IDocumentController
{
    bool Handles(string path);

    void SetViewActive(bool isActive);

    IDocumentView New(string fileName);

    IDocumentView Open(string path);

    IEnumerable<DocumentFileType> FileTypes { get; }
}

Now the view ascociated with this controller is 
public partial class SpreadsheetView : DockContent, ISpreadsheetView
{
    private IDocumentController controller;

    public SpreadsheetView()
    {
        InitializeComponent();
    }

    private void SpreadsheetView_Activated(object sender, EventArgs e)
    {
        controller.SetViewActive(true);
    }

    private void SpreadsheetView_Deactivate(object sender, EventArgs e)
    {
        controller.SetViewActive(false);
    }

    public void SetController(IDocumentController controller)
    {
        this.controller = controller;
        Log.Trace(""SpreadsheetView.SetController(): Controller set successfully"");
    }

    public string DisplayName
    {
        get { return Text; }
        set { Text = value; }
    }

    public WorkbookView WorkbookView
    {
        get { return workbookView; }
        set { workbookView = value; }
    }
    ...
}

Finally the view interfaces are 
public interface ISpreadsheetView : IDocumentView
{
    WorkbookView WorkbookView { get; set; } 
}

and 
public interface IDocumentView
{
    void SetController(IDocumentController controller);

    string DisplayName { get; set; }

    bool StatusBarVisible { get; set; }
}

Now for my questions. In Seemann's book ""Dependency Injection in .NET"" he talks about the ""Three Calls Pattern"" and this is what I have attempted to implement in the above. The code works, the shell view displays and via the MVC pattern my controllers correctly open views etc. However, I am confused as the above definately has the flavour of the ""Service Locator Anti-Pattern"". In chapter 3 of Seemann's book he states 

The COMPOSITION ROOT pattern describes where you should use a DI CONTAINER. However,
  it doesn__ state how to use it. The REGISTER RESOLVE RELEASE pattern addresses
  this question [...] A DI CONTAINER should be used in three successive
  phases called Register, Resolve, and Release. 
In its pure form, the REGISTER RESOLVE RELEASE pattern states that you should only
  make a single method call in each phase. Krzysztof Kozimic calls this the Three Calls Pattern.
Configuring a DI CONTAINER in a single method call requires more explanation. The
  reason that registration of components should happen in a single method call is
  because you should regard configuration of a DI CONTAINER as a single, atomic action.
  Once configuration is completed, the container should be regarded as read-only.

This sounds like the dredded ""Service locator"", why is this not deemed service location? 

In order to adjust my code to instead use Contstructor Injection, I changed my entry code to 
[STAThread]
static void Main()
{
    var kernel = new StandardKernel();
    kernel.Bind(t => t.FromThisAssembly()
                      .SelectAllClasses()
                      .BindAllInterfaces());

    FileLogHandler fileLogHandler = new FileLogHandler(Utils.GetLogFilePath());
    Log.LogHandler = fileLogHandler;
    Log.Trace(""Program.Main(): Logging initialized"");

    Application.EnableVisualStyles();
    Application.SetCompatibleTextRenderingDefault(false);
    Application.Run(kernel.Get<ApplicationShellView>());
}

using Ninject.Extensions.Conventions, I then changed ApplicationShellController in order to correct my code to inject the IDocumentControllers via ctor injection:
public class ApplicationShellController
{
    private IApplicationShellView shellView;
    private IEnumerable<IDocumentController> controllers; 

    public ApplicationShellController(IApplicationShellView shellView, IEnumerable<IDocumentController> controllers)
    {
        this.shellView = shellView;
        this.controllers = controllers;
        Log.Trace(""ApplicationShellController.Ctor(): Shell initialized successfully""); 
    }
    ...
}

where 
public class SpreadsheetController : IDocumentController 
{
    private ISpreadsheetView view;

    public SpreadsheetController(ISpreadsheetView view)
    {
        this.view = view;
        this.view.SetController(this);
    }
    ...
}

but this leads to a circular dependency, how do I handle this? 
Question Summary:

Why is my initial use of Ninject using ""Thee Calls Pattern"" and CompositionRoot.Resolve<T>() bad or different to the Service Locator Anti-Pattern?
How can I resolve the circular dependency issue above if I want to switch to pure ctor injection? 

Thanks very much for your time.","At some point in the process, you have to use service location.  However, the difference between DI and SL is that in SL, you are resolving your services at the point they are requested, whereas in DI you resolve them in some kind of factory (such as a controller factory) and then construct your objects and pass the reference in.
You should create some kind of infrastructure that dispatches your commands and uses a factory of some kind to locate the dependencies used by the created objects.  
In this way, the rest of your code doesn't have dependency resolution, and you are following a DI pattern except at the construction point.",https://stackoverflow.com/questions/35895538/seemanns-dependency-injection-three-calls-pattern-vs-service-locator-anit-pa,1,1
Is anaemic model or transaction script good enough for micro service?,"While we always talking about anaemic model being anti-pattern, I'm thinking whether it's just good enough for microservice.
As many has mentioned, anaemic model(or transaction script as Martin Fowler call it), is actually good with small applications. Though with monolithic architecture it's understandable we must use more sophisticated structure to handle complexity.
However, with microservice, it's unlikely we pack too much logic in one single service. Instead a service usually only contains related logic in one single domain, which is usually easy to understand and work upon. In this case, is it totally fine to use transaction script model inside microservice?","Hi That depends on the project,you keep in mind that we need to use rich models in ddd's approach because the nature of these projects with a domain approach is rich, and we need to use rich domains in those projects, and now in projects that They do not have an ddd__ approach, and I mean data driven projects are. We, too, have Anemic models that answer our work. 
So That depends on the project and the approach taken for that project.
below link can help you:
https://blog.pragmatists.com/domain-driven-design-vs-anemic-model-how-do-they-differ-ffdee9371a86",https://stackoverflow.com/questions/51996128/is-anaemic-model-or-transaction-script-good-enough-for-micro-service,2,1
Rich Domain Model and ORM,"Martin Fowler considers Anemic Domain Model as an anti-pattern.
Rolling the Persistence Model as the Domain Model seems severely off too due to Object Relational Impedence Missmatch. For persistence and normalization sakes, we tend to break down classes to very small tiny pieces, slapping methods on top of these classes is silly. Plus persistence rarely changes, but business logic changes a fair bit.
So we need a DomainModel that builds on the persistence model (instead of being one and the same). This Domain Model will then contains business logic properties and method.
But now these Domain Models are still behind the service, and in order to expose them out to the outside world we need to convert them over to DTOs.
We are doing so manny mappings here.

Persistence to Domain Model
To convert Domain Model into DTOs to pass along between services

It doesn't end there, since the DTO may need to be mapped into the ViewModel.
All that and the problem of duplicating validation logic still doesn't go away, because the client wants real time validation. ViewModel knows nothing about validation, so in an SPA for example, you're forced to rewrite the validation logic again, on the client side (usually in javascript).
Also services are by nature stateless (message or RPC oriented), so we're doing all these mapping, between Persistence, to OO then back to Procedural, to what benefit? How would you justify the cost in practical terms of most IT budget?
I get how having full DDD, with Aggregate Roots, Domain Models etc. would be ""cool"" but how can you justify the cost and the hit on dev productivity?

anti-pattern (or antipattern) is a pattern used in social or business
  operations or software engineering that may be commonly used but is
  ineffective and/or counterproductive in practice

And if so, wouldn't DDD and Rich Domain Model fit into the anti-pattern definition above than the ""Lean"" Domain Model. Sorry, I despise the loaded word, ""Anemic"". 
By keeping the Domain Model, ""Lean"" you actually allow it to be shared without violating the ""Abstract Dependency Principle"", ""Don't Repeat Yourself"" and the time consuming, tedious and error prone process of mapping one data carrier to another, and whatever associated Unit Test that goes on top of that (unless you're thinking of doing mapping w/o unit testing and hope for the best).","It seems you're mixing up a lot of concepts, blaming the rich domain model approach for things it isn't directly responsible for. 

Rich domain model is orthogonal to layered architecture, especially having a rich domain model doesn't dictate the number of layers you have, what data structures should be exchanged between these layers and how they should be mapped. 
Rich domain model is orthogonal to validation and says nothing about the need for client-side checking in addition to back-end validation.

In other words, making your domain model anemic with all business logic in services won't necessarily save you from writing a lot of boilerplate DTO mapping code, nor will it remove the need for client-side ""double checking"" (which is by the way a commonly accepted best practice).
This doesn't mean your point about the cost and weight of a full-fledged multi-layered architecture isn't valid. You might find interest in this post by Mark Seemann discussing similar concerns : http://blog.ploeh.dk/2012/02/09/IsLayeringWorthTheMapping.aspx",https://stackoverflow.com/questions/14372065/rich-domain-model-and-orm,3,1
Alternatives to NTier architecture for web apps,"I was recently discussing with colleagues a push toward disciplined n-tier structure for our web application. It struck me that I couldn't think of any alternative.
Is it always the case that one should seperate out web applications into layers?","An alternative to the traditional N-tiered architecture is the Command-Query Responsibility Segregation (CQRS) architecture  as discussed by Udi Dahan.
Like all architectural decisions you should really think about when to use it as discussed here 
Personally, I tend to see a lot of ""over architecture"" in my software travels which can really over complicate matters and make things much more difficult to maintain and obviously cost a lot more too.  You really need to think a lot about the business problem first rather than just picking an architecture.  
Keep things as simple as possible for best results and easy refactoring.",https://stackoverflow.com/questions/7329413/alternatives-to-ntier-architecture-for-web-apps/7393171,4,1
Get Current User in Models,"I am working on a project that has an Investment and Invoice class in the models.py. The way they work is that a user places an investment and while saving the Investment model creates an invoice. Here is the relevant code:
class Investment(models.Model):
   ...
   def save(self, *args, **kwargs):
            current_investment = Investment.objects.get(pk=self.pk)
            create_invoice = Invoice.objects.create(investment=current_investment, 
                                                    fee_type='Mngmt.', 
                                                    amount=self.amount*fee_amount)

class Invoice(models.Model):
    user = models.ForeignKey(User) 
    investment = models.ForeignKey(Investment)
    fee_type = models.CharField(max_length=24)
    amount = models.DecimalField(max_digits=12, decimal_places=2, default=0)
    date = models.DateTimeField(default=timezone.now, blank=True)

    def __str__(self):
        return self.investment.fund.name    

So basically when the Investment class is saved an Invoice object is created, however I want to get the current user and set it to the user field in the Invoice class. Any ideas on how I can get this done? Thanks.","It's somehow anti-pattern to access current request and current request.user in model's scope.
MVC Architectural Pattern
Basically these operation are done within view sections.
So it's better to pass the current user from responsible view for saving Investment or Invoice",https://stackoverflow.com/questions/34560230/get-current-user-in-models/34560290,2,1
UnityContainer.Resolve or ServiceLocator.GetInstance?,"It could seem a stupid question because in my code everything is working, but I've registered a singleton this way with my Unity container _ambientContainer:
 _ambientContainer.RegisterType<Application.StateContext>(new ContainerControlledLifetimeManager());

In order to avoid to use my local field, I use:
get {
    return ServiceLocator.Current.GetInstance<Application.StateContext>();
}

inside my get property to get an instance of my object.
This way I get always the same instance (Application.StateContext is still a singleton) or does GetInstance create a new one?
Is it better to use the local _ambientContainer field instead?
get {
    return _ambientContainer.Resolve<Application.StateContext>();
}

Thank you.","I'm assuming that the ServiceLocator type is from the CommonServiceLocator project, and that you're using the Unity adapter, in which case GetInstance invokes container.Resolve, so both lines are equivalent.
You can view the source here - http://commonservicelocator.codeplex.com/wikipage?title=Unity%20Adapter&referringTitle=Home",https://stackoverflow.com/questions/9308424/unitycontainer-resolve-or-servicelocator-getinstance/9308756,3,1
What is a domain model ? Why is it preferred than a dataset in .net?,"I was wondering, in assignments I have been using datasets. Now when I started working in this software company people are using something called DTO - data transfer object. Where does domain model come in ? What is it really ?
Thanks","DTO = Data Transfer Object are as it sounds. Object that transfer data between system layers. The purpose is often to adapt the request and response data so it suits the use case. Example can be that you request a CV through a HR system's CandidateService in application layer. The Candidate Service loads information than spans over different domain entities: WorkExperince, Education, Personal Letter etc. To avoid a complex and massive response object graph we can flatten the repsponse by building a DTO object that is exactly design for what the client (GUI) needs. 
There are a lot to say about DTO. But I do not want to write a novel :) But DTO do not belong in the Domain Model, in the Core. DTO is mostly refered in DDD as tool for communication between application services to clients, especially if you use web services (WCF etc). Then DTO is a perfect way of serializing part of your domain into a web service message (serialized DTO).
Hopefully you can ask your collegue/co-workers as well what they intended to accomplish with DTO's. There are several drawback with DTO's, usually it gives you an extra layer and that means more to do during maintenance phase...
(almost a novel by now) I use DTO's only when there is a really benefit and thats when you can deliver a complex responses with DTO that matches the clients needs exactly. Otherwise the client usually need to call different services or methods to gather enough information.",https://stackoverflow.com/questions/5560699/what-is-a-domain-model-why-is-it-preferred-than-a-dataset-in-net/5560744,2,1
Injecting $state (ui-router) into $http interceptor causes circular dependency,"What I'm trying to achieve
I would like to to transition to a certain state (login) in case an $http request returns a 401 error. I have therefore created an $http interceptor.
The problem
When I am trying to insert '$state' into the interceptor I get a circular dependency. Why and how do i fix it?
Code
//Inside Config function

    var interceptor = ['$location', '$q', '$state', function($location, $q, $state) {
        function success(response) {
            return response;
        }

        function error(response) {

            if(response.status === 401) {
                $state.transitionTo('public.login');
                return $q.reject(response);
            }
            else {
                return $q.reject(response);
            }
        }

        return function(promise) {
            return promise.then(success, error);
        }
    }];

    $httpProvider.responseInterceptors.push(interceptor);","The Fix
Use the $injector service to get a reference to the $state service.
var interceptor = ['$location', '$q', '$injector', function($location, $q, $injector) {
    function success(response) {
        return response;
    }

    function error(response) {

        if(response.status === 401) {
            $injector.get('$state').transitionTo('public.login');
            return $q.reject(response);
        }
        else {
            return $q.reject(response);
        }
    }

    return function(promise) {
        return promise.then(success, error);
    }
}];

$httpProvider.responseInterceptors.push(interceptor);

The Cause
angular-ui-router injects the $http service as a dependency into $TemplateFactory which then creates a circular reference to $http within the $httpProvider itself upon dispatching the interceptor.
The same circular dependency exception would be thrown if you attempt to inject the $http service directly into an interceptor like so.
var interceptor = ['$location', '$q', '$http', function($location, $q, $http) {

Separation of Concerns
Circular dependency exceptions can indicate that there is a mixing of concerns within your application which could cause stability issues. If you find yourself with this exception you should take the time to look at your architecture to ensure you avoid any dependencies that end up referencing themselves.  
@Stephen Friedrich's answer
I agree with the answer below that using the $injector to directly get a reference to the desired service is not ideal and could be considered an anti pattern. 
Emitting an event is a much more elegant and also decoupled solution.",https://stackoverflow.com/questions/20230691/injecting-state-ui-router-into-http-interceptor-causes-circular-dependency/20230786,3,1
AngularJS: Injecting service into a HTTP interceptor (Circular dependency),"I'm trying to write a HTTP interceptor for my AngularJS app to handle authentication.
This code works, but I'm concerned about manually injecting a service since I thought Angular is supposed to handle this automatically:
    app.config(['$httpProvider', function ($httpProvider) {
    $httpProvider.interceptors.push(function ($location, $injector) {
        return {
            'request': function (config) {
                //injected manually to get around circular dependency problem.
                var AuthService = $injector.get('AuthService');
                console.log(AuthService);
                console.log('in request interceptor');
                if (!AuthService.isAuthenticated() && $location.path != '/login') {
                    console.log('user is not logged in.');
                    $location.path('/login');
                }
                return config;
            }
        };
    })
}]);

What I started out doing, but ran into circular dependency problems:
    app.config(function ($provide, $httpProvider) {
    $provide.factory('HttpInterceptor', function ($q, $location, AuthService) {
        return {
            'request': function (config) {
                console.log('in request interceptor.');
                if (!AuthService.isAuthenticated() && $location.path != '/login') {
                    console.log('user is not logged in.');
                    $location.path('/login');
                }
                return config;
            }
        };
    });

    $httpProvider.interceptors.push('HttpInterceptor');
});

Another reason why I'm concerned is that the section on $http in the Angular Docs seem to show a way to get dependencies injected the ""regular way"" into a Http interceptor. See their code snippet under ""Interceptors"":
// register the interceptor as a service
$provide.factory('myHttpInterceptor', function($q, dependency1, dependency2) {
  return {
    // optional method
    'request': function(config) {
      // do something on success
      return config || $q.when(config);
    },

    // optional method
   'requestError': function(rejection) {
      // do something on error
      if (canRecover(rejection)) {
        return responseOrNewPromise
      }
      return $q.reject(rejection);
    },



    // optional method
    'response': function(response) {
      // do something on success
      return response || $q.when(response);
    },

    // optional method
   'responseError': function(rejection) {
      // do something on error
      if (canRecover(rejection)) {
        return responseOrNewPromise
      }
      return $q.reject(rejection);
    };
  }
});

$httpProvider.interceptors.push('myHttpInterceptor');

Where should the above code go?
I guess my question is what's the right way to go about doing this?
Thanks, and I hope my question was clear enough.","You have a circular dependency between $http and your AuthService.
What you are doing by using the $injector service is solving the chicken-and-egg problem by delaying the dependency of $http on the AuthService.
I believe that what you did is actually the simplest way of doing it.
You could also do this by: 

Registering the interceptor later (doing so in a run() block instead of a config() block might already do the trick). But can you guarantee that $http hasn't been called already?
""Injecting"" $http manually into the AuthService when you're registering the interceptor by calling AuthService.setHttp() or something.
...",https://stackoverflow.com/questions/20647483/angularjs-injecting-service-into-a-http-interceptor-circular-dependency,5,1
What use are EJBs,"I'm currently learning Jave-EE, having plenty of C++ experience and having learned Java SE. I don't understand the purpose of Enterprise Java Beans; can someone clarify this for me. I'm not interested in legacy uses: this is in the context of EJB-3.1 and Java-EE 6.
It seems that some people use them to contain the business logic, for implementing the business layer of the conventional 3-layer architecture. That separates the domain logic from the domain objects, leading to an anemic domain model. But that goes against all my OOD instincts; I agree with Martin Fowler that it is an anti-pattern. Should I relax my objections to an anemic domain model? Or do EJBs have other uses?","Use of Java EE does not automatically imply a anemic domain model, just as you can write code in say java what does not make good use of best practices doesn't mean it's not possible in java.  I believe Martin Fowler's point was J2EE (note the use of J2EE and not Java EE) pretty much enforced operation of logic and data.  Using POJO based entities allows data and behaviour to modelled appropriately.  The ""business logic"" in your EJBs typically orchestrates application of business logic but more often than not does not actually perform it, it is usually a very thin wrapper.
EJBs thus form your Service API, you need this whichever platform/framework you are using, you need to have something you can physically invoke, it is an entry point.  Whether you are implementing using spring, web services etc... You need a service layer, there is nothing stopping this been implemented in Java EE.  A rather contrived example
@Stateless
public SomeServiceImpl implements SomeService
    someServiceMethod() {
       delegate.doSomething();
    }
}

public SomeServiceDelegate implements SomeService
    someServiceMethod() {
       modelObject.doSomething();
    }
}

I'm not going into the reasons to prefer EJBs over any other technology, just wanting to point out that using them doesn't mean your implementation can't use best practice.",https://stackoverflow.com/questions/5579890/what-use-are-ejbs,7,1
Frameworks for Layering reusable Architectures,"My question is very simple, my intention is to generate a repository with your responses so it could serve to the community when selecting frameworks for developing enterprise general purpose applications. 
This could apply very well for general purpose languages such as C++, C# or Java.

What Framework do you recommend for generating Layered Architectures?
Based on you experience why do you prefer the usage of some Framework versus your own architecture?
How long do you believe your selected Framework will stay as a preferred option in the software development industry?","This is indeed an overly general question, especially since there are so many interpretations of the very word framework, and within the world of frameworks many different kinds for different tasks. Nevertheless, I'll give it a shot for Java.
Java
Java EE
The default overall enterprise framework of Java is called Java EE. Java EE strongly emphasis a layered architecture. It's a quite large framework and learning every aspect of it can take some time. It supports several types of applications. Extremely small and simple ones may only use JSP files with some scriptlets, while larger ones may use much more. 
Java EE doesn't really enforce you to use all parts of it, but you pick and choose what you like.
Top down it consists of the following parts:
Web layer
For the web layer Java EE primarily defines a component and MVC based Web Framework called JSF - JavaServer Faces. JSF utilizes an XML based view description language (templating language) called Facelets. Pages are created by defining templates and letting template clients provide content for them, including other facelets and finally placing components and general markup on them.
JSF provides a well defined life-cyle for doing all the things that every web app should do: converting request values, validating them, calling out to business logic (the model) and finally delegating to a (Facelets) view for rendering.
For a more elaborate description look up some of the articles by BalusC here, e.g. What are the main disadvantages of Java Server Faces 2.0? 
Business layer
The business layer in the Java EE framework is represented by a light-weight business component framework called EJB - Enterprise JavaBeans. EJBs are supposed to contain the pure business logic of an application. Among others EJBs take care of transactions, concurrency and when needed remoting.
An ordinary Java class becomes an EJB by applying the @Stateless annotation. By default, every method of that bean is then automatically transactional. Meaning, if the method is called and no transaction is active one is started, otherwise one is joined. If needed this behavior can be tuned or even disabled. In the majority of cases transactions will be transparent to the programmer, but if needed there is an explicit API in Java EE to manage them manually. This is the JTA API - Java Transaction API.
Methods on an EJB can easily be made to execute asynchronous by using the @Asynchronous annotation.
Java EE explicitly supports layering via the concept of a separate module specifically for EJBs. This isolates those beans and prevents them from accessing their higher layer. See this Packaging EJB in JavaEE 6 WAR vs EAR for a more elaborate explanation.
Persistence layer
For persistence the Java EE framework comes with a standard ORM framework called JPA - Java Persistence API. This is based on annotating plain java classes with the @Entity annotation and a property or field on them with @Id. Optionally (if needed) further information can be specified via annotations on how objects and object relations map to a relational database.
JPA heavily emphasizes slim entities. This means the entities themselves are as much as possible POJOs that can be easily send to other layers and even remote clients. An entity in Java EE typically does not take care of its own persistence (i.e. it does not hold any references to DB connections and such). Instead, a separate class called the EntityManager is provided to work with entities.
The most convenient way of working with this EntityManager is from within an EJB bean, which makes obtaining an instance and the handling of transactions a breeze. However, using JPA in any other layer, even outside the framework (e.g. in Java SE) is supported as well.

These are the most important services related to the traditional layers in a typical enterprise app, but the Java EE framework supports a great many additional services. Some of which are:
Messaging
Messaging is directly supported in the Java EE framework via the JMS API - Java Messaging Service. This allows business code to send messages to so-called queues and topics. Various parts of the application or even remote applications can listen to such a queue or topic.
The EJB component framework even has a type of bean that is specifically tailored for messaging; the message driven bean which has a onMessage method that is automatically invoked when a new message for the queue or topic that the bean is listening to comes in.
Next to JMS, Java EE also provides an event-bus, which is a simple light-weight alternative to full blown messaging. This is provided via the CDI API, which is a comprehensive API that among others provides scopes for the web layer and takes care of dependency injections. Being a rather new API it currently partially overlaps with EJB and the so-called managed beans from JSF.
Remoting
Java EE provides a lot of options for remoting out of the box. EJBs can be exposed to external code willing and able to communicate via a binary protocol by merely letting them implement a remote interface.
If binary communication is not an option, Java EE also provides various web service implementations. This is done via among others JAX-WS (web services, soap) and JAX-RS (Rest).
Scheduling
For scheduling periodic or timed jobs, Java EE offers a simple timer API. This API supports CRON-like timers using natural language, as well as timers for delayed execution of code or follow up checks.
This part of Java EE is usable but as mentioned fairly basic.

There are quite some more things in Java EE, but I think this about covers the most important things.
Spring
An alternative enterprise framework for Java is Spring. This is a proprietary, though fully open source framework.
Just as the Java EE framework, the Spring framework contains a web framework (called Spring MVC), a business component framework (simply called Spring, or Core Spring Framework) and a web services stack (called Spring Web Services).
Although many parts of the Java EE framework can be used standalone, Spring puts more emphasis on building up your own stack than Java EE does.
The choice of Java EE vs Spring is often a religiously influenced one. Technically both frameworks offer a similar programming model and a comparable amount of features. Java EE may be seen as slightly more light-weight (emphasis convention over configuration) and having the benefit of type-safe injections, while Spring may offer more of those smaller convenience methods that developers often need. 
Additionally Spring offers a more thoroughly and directly usable security API (called Spring Security), where Java EE leaves a lot of security details open to (third party) vendors.",https://stackoverflow.com/questions/4475899/frameworks-for-layering-reusable-architectures/4547358,4,1
In what situations are EJBs used ? Are they required in websites/ web-application development?,Are EJBS used in database backed websites(that are accessible to all)?,"Nothing is ever required of course. If you wanted you could build a web-application as a single large C function behind CGI.
That said, EJBs do make web application development a lot easier. It's not for nothing that they are included in the ultra-lightweight Web Profile of Java EE 6. 
EJB does not contain any Database APIs of itself, but it integrates extremely well with JPA. You can inject the EntityManager in it, and the requirement of having to start and commit/rollback transactions yourself disappears. This greatly simplifies your code.
Although you could put DB related code (JPA or JDBC) directly into your Servlets or even JSP pages, this is a practice generally frowned upon. Servlets and JSPs are for display and any business or persistence logic just doesn't belong there. A very practical reason for that is that you can't call into the middle of a JSP page to re-use some piece of business logic.
Keeping your business logic separate is thus a prime virtue of good web applications and EJBs are the designated beans for that in Java EE.
For additional information, see these two answers to similar questions:

Database table access via JPA Vs. EJB in a Web-Application
EJB 3 or Hibernate 3

For the role of EJB in the bigger picture of your web application architecture:

Frameworks for Layering reusable Architectures",https://stackoverflow.com/questions/4773927/in-what-situations-are-ejbs-used-are-they-required-in-websites-web-application/4775719,2,1
EJB 3 or Hibernate 3,"Regarding a Java EE Web application which is going to be served by a full Java EE Application server e.g. GlassFish, which is the best ORM Solution? EJB 3 or Hibernate 3
And why?","Those two are completely different.
EJB3 is a component model and has itself nothing directly to do with ORM. It does help with easily managing transactions and giving you easy access to the entity manager from JPA, which is a standardized ORM solution in Java EE.
Hibernate (3) is indeed an ORM solution, and as it happens one that implements JPA.
So a more logical question is whether to use the standardized JPA interfaces, or to use the Hibernate core API directly. Then a followup question could be whether to use JPA standalone, or in combination with EJB 3.
The answer depends a little on what you need exactly, but typically using JPA in combination with EJB 3 is the easiest solution. Using JPA or Hibernate standalone requires much more verbose code and you manually have to manage transactions, which can be a pain.
JPA vs Hibernate is another debate. JPA has the benefit of having the standardized interfaces, so more developers will likely be familiar with it. On the other hand, the native Hibernate APIs are always a super set of those of JPA and thus offer more power. 
Typically developers mainly base their code on JPA, and then use some Hibernate specific annotations or API calls where it makes sense. In 99.99% of the cases such mixed API usage is supported. 
Do also note that Glassfish is bundled with EclipseLink, not with Hibernate. EclipseLink is comparable with Hibernate but predates it with more than a decade. Hibernate took a lot from EclipseLink (called TopLink back then).
See also this answer I gave to a similar question: Database table access via JPA Vs. EJB in a Web-Application",https://stackoverflow.com/questions/4639535/ejb-3-or-hibernate-3/4639967,3,1
@EJB injection vs lookup - performance issue,"I have a question related with possible performance issue while using @EJB annotation. Imagine following scenario
public class MyBean1 implements MyBean1Remote{
 @EJB
 private MyBean2Remote myBean2;
 @EJB
 private MyBean2Remote myBean3;
 ...
 @EJB
 private MyBean20Remote myBean20;
}  

There is a bean with many dependencies to other beans. According to EJB spec if I would like to inject MyBean1Remote to some other bean, container would have to take all required dependencies from its pool inject it into MyBean1Remote and then inject reference to MyBean1Remote stub.
so in following scenario container needs to reserved 20 ejbs (myBean1 and its 19 dependencies)
public class MyAnotherBean implement MyAnotherRemote{
  @EJB
  private MyBean1Remote myBean1
}

Let say that in most cases we will use only single dependency per each business method of myBean1. As a result each time  we want to inject that bean we force container to reserves many unnecessery EJBs. Lets also assume that we are operating on remote beans so probably container would also need to perform some load balancing algorithm prior injecting dependent beans. 
Questions:

Wouldn't that cause unnecessary resource reservation and more over performance issue while operating in cluster environment?  
Maybe good old ServiceLocator could be better solution because with this approach we would ask for specific EJB when we really need it ?","The container does not inject an instance of the EJB; it injects an instance of a lightweight container-generated proxy object that implements the desired interface.
public class MyBean1 implements MyBean1Remote {
   ...
}

public class MyAnotherBean implement MyAnotherRemote {
   @EJB
   private MyBean1Remote myBean1;
}

In your example, MyAnotherBean.myBean1 will be injected with a proxy object that implements the MyBean1Remote interface.
Assuming a stateless session bean (since you mention pooling), the container does not allocate an actual EJB instance from the method-ready pool until a method is called on the proxy, and the instance is returned to the pool before the proxy method call returns.",https://stackoverflow.com/questions/4753449/ejb-injection-vs-lookup-performance-issue/4765261,2,1
Stateless and Stateful Enterprise Java Beans,"I am going through the Java EE 6 tutorial and I am trying to understand the difference between stateless and stateful session beans. If stateless session beans do not retain their state in between method calls, why is my program acting the way it is?
package mybeans;

import javax.ejb.LocalBean;
import javax.ejb.Stateless;

@LocalBean
@Stateless
public class MyBean {

    private int number = 0;

    public int getNumber() {
        return number;
    }

    public void increment() {
        this.number++;
    }
}

The client
import java.io.IOException;
import javax.ejb.EJB;
import javax.servlet.*;
import javax.servlet.http.*;
import javax.servlet.annotation.WebServlet;
import mybeans.MyBean;
import java.io.PrintWriter;

@WebServlet(name = ""ServletClient"", urlPatterns = { ""/ServletClient"" })
public class ServletClient extends HttpServlet {
    private static final long serialVersionUID = 1L;

    @EJB
    MyBean mybean;

    protected void doGet(HttpServletRequest request,
            HttpServletResponse response) throws ServletException, IOException {

        PrintWriter out = response.getWriter();
        mybean.increment();
        out.println(mybean.getNumber());
    }

}

I was expecting getNumber to return 0 every time but it is returning 1 and reloads of the servlet in my browser increase it more. The problem is with my understanding of how stateless session beans work and not with the libraries or application server, of course. Can somebody give me a simple hello world type example of a stateless session bean that behaves differently when you change it to stateful?","The important difference is not private member variables, but associating state with a particular user (think ""shopping cart"").
The stateful piece of stateful session bean is like the session in servlets.   Stateful session beans allow your app to still have that session even if there isn't a web client.  When the app server fetches a stateless session bean out of the object pool, it knows that it can be used to satisfy ANY request, because it's not associated with a particular user.
A stateful session bean has to be doled out to the user that got it in the first place, because their shopping cart info should be known only to them.  The app server ensures that this is so.  Imagine how popular your app would be if you could start shopping and then the app server gave your stateful session bean to me when I came along!
So your private data member is indeed ""state"", but it's not ""shopping cart"".  Try to redo your (very good) example to make it so the incremented variable is associated with a particular user.  Increment it, create a new user, and see if they can still see the incremented value.  If done correctly, every user should see just their version of the counter.",https://stackoverflow.com/questions/2351220/stateless-and-stateful-enterprise-java-beans,7,1
EJB stateless session beans and stateful session bean,I have gone through various books on stateful and stateless session bean and how they work. I want to know the real usage of these ejbs and advantages over using plain java classes. Basically when do you go for stateless ejbs and when do you go for stateful ejbs. I want a real time application.,"the usage of these type of ejbs are usually in service layer as service classes.
EJB3 stateless and stateful bean are actually POJO (with some annotations) and they don't have any big difference with normal classes.
but in term of usage, they have some abilities that you can't find in normal classes like:

they can be called remotely (e.g. RMI protocol). 
they can use application server context resources like DB Connection and Transactions.

stateless or stateful:
- if a task or process can be done in a single step (by a single method call) stateless is the right option
    like a authentication process 
- if a task needs a series of method calls (more than one) and you need to keep previous results to use them in next call, then go for stateful.
like a shipping process (select items, add/remove and then do the transaction)
http session or stateful?
ejbs can be served in application server and they may have different type of clients like a normal swing application or ..., so you can't relay on http session in these cases.
if your appserver and  webserver are different (distributed) its not good idea keep data in http session and pass/getback it to/from app server (network overhead).",https://stackoverflow.com/questions/3587289/ejb-stateless-session-beans-and-stateful-session-bean,4,1
Best architectural approaches for building iOS networking applications (REST clients),"I'm an iOS developer with some experience and this question is really interesting to me. I saw a lot of different resources and materials on this topic, but nevertheless I'm still confused. What is the best architecture for an iOS networked application? I mean basic abstract framework, patterns, which will fit every networking application whether it is a small app which only have a few server requests or a complex REST client. Apple recommends to use MVC as a basic architectural approach for all iOS applications, but neither MVC nor the more modern MVVM patterns explain where to put network logic code and how to organize it in general.
Do I need to develop something like MVCS(S for Service) and in this Service layer put all API requests and other networking logic, which in perspective may be really complex? After doing some research I found two basic approaches for this. Here it was recommended to create a separate class for every network request to web-service API (like LoginRequest class or PostCommentRequest class and so on) which all inherits from the base request abstract class AbstractBaseRequest and in addition to create some global network manager which encapsulates common networking code and other preferences (it may be AFNetworking customisation or RestKit tuning, if the we have complex object mappings and persistence, or even an own network communication implementation with standard API). But this approach seems an overhead for me. Another approach is to have some singleton API dispatcher or manager class as in the first approach, but not to create classes for every request and instead to encapsulate every request as an instance public method of this manager class like: fetchContacts, loginUser methods, etc. So, what is the best and correct way? Are there other interesting approaches I don't know yet? 
And should I create another layer for all this networking stuff like Service, or NetworkProvider layer or whatever on top of my MVC architecture, or this layer should be integrated (injected) into existing MVC layers e.g. Model?
I know there exists beautiful approaches, or how then such mobile monsters like Facebook client or LinkedIn client deal with exponentially growing complexity of networking logic?
I know there are no exact and formal answer to the problem. The goal of this question is to collect the most interesting approaches from experienced iOS developers. The best suggested approach will be marked as accepted and awarded with a reputation bounty, others will be upvoted. It is mostly a theoretical and research question. I want to understand basic, abstract and correct architectural approach for networking applications in iOS. I hope for detailed explanation from experienced developers.","I want to understand basic, abstract and correct architectural approach for networking applications in iOS : there is no ""the best"", or ""the most correct"" approach for building an application architecture. It is a very creative job. You should always choose the most straightforward and extensible architecture, which will be clear for any developer, who begin to work on your project or for other developers in your team, but I agree, that there can be a ""good"" and a ""bad"" architecture. 
You said: collect the most interesting approaches from experienced iOS developers, I don't think that my approach is the most interesting or correct, but I've used it in several projects and satisfied with it. It is a hybrid approach of the ones you have mentioned above, and also with improvements from my own research efforts. I'm interesting in the problems of building approaches, which combine several well-known patterns and idioms. I think a lot of Fowler's enterprise patterns can be successfully applied to the mobile applications. Here is a list of the most interesting ones, which we can apply for creating an iOS application architecture (in my opinion): Service Layer, Unit Of Work, Remote Facade, Data Transfer Object, Gateway, Layer Supertype, Special Case, Domain Model. You should always correctly design a model layer and always don't forget about the persistence (it can significantly increase your app's performance). You can use Core Data for this. But you should not forget, that Core Data is not an ORM or a database, but an object graph manager with persistence as a good option of it. So, very often Core Data can be too heavy for your needs and you can look at new solutions such as Realm and Couchbase Lite, or  build your own lightweight object mapping/persistence layer, based on raw SQLite or LevelDB. Also I advice you to familiarize yourself with the Domain Driven Design and CQRS.
At first, I think, we should create another layer for networking, because we don't want fat controllers or heavy, overwhelmed models. I don't believe in those fat model, skinny controller things. But I do believe in skinny everything approach, because no class should be fat, ever. All networking can be generally abstracted as business logic, consequently we should have another layer, where we can put it. Service Layer is what we need:
It encapsulates the application's business logic,  controlling transactions 
and coordinating responses in the implementation of its operations.

 In our MVC realm Service Layer is something like a mediator between domain model and controllers. There is a rather similar variation of this approach called MVCS where a Store is actually our Service layer. Store vends model instances and handles the networking, caching etc. I want to mention that you should not write all your networking and business logic in your service layer. This also can be considered as a bad design. For more info look at the Anemic and Rich domain models. Some service methods and business logic can be handled in the model, so it will be a ""rich"" (with behaviour) model. 
I always extensively use two libraries: AFNetworking 2.0 and ReactiveCocoa. I think it is a must have for any modern application that interacts with the network and web-services or contains complex UI logic. 
ARCHITECTURE
At first I create a general APIClient class, which is a subclass of AFHTTPSessionManager. This is a workhorse of all networking in the application: all service classes delegate actual REST requests to it. It contains all the customizations of HTTP client, which I need in the particular application: SSL pinning, error processing and creating straightforward NSError objects with detailed failure reasons and descriptions of all API and connection errors (in such case controller will be able to show correct messages for the user), setting request and response serializers, http headers and other network-related stuff. Then I logically divide all the API requests into subservices or, more correctly, microservices: UserSerivces, CommonServices, SecurityServices, FriendsServices and so on, accordingly to business logic they implement. Each of these microservices is a separate class. They, together, form a Service Layer. These classes contain methods for each API request, process domain models and always returns a RACSignal with the parsed response model or NSError to the caller.
 I want to mention that if you have complex model serialisation logic - then create another layer for it: something like Data Mapper but more general e.g. JSON/XML -> Model mapper. If you have cache: then create it as a separate layer/service too (you shouldn't mix business logic with caching). Why? Because correct caching layer can be quite complex with its own gotchas. People implement complex logic to get valid, predictable caching like e.g. monoidal caching with projections based on profunctors. You can read about this beautiful library called Carlos to understand more. And don't forget that Core Data can really help you with all caching issues and will allow you to write less logic. Also, if you have some logic between NSManagedObjectContext and server requests models, you can use Repository pattern, which separates the logic that retrieves the data and maps it to the entity model from the business logic that acts on the model. So, I advice to use Repository pattern even when you have a Core Data based architecture. Repository can abstract things, like NSFetchRequest,NSEntityDescription, NSPredicate and so on to plain methods like get or put. 
After all these actions in the Service layer, caller (view controller) can do some complex asynchronous stuff with the response: signal manipulations, chaining, mapping, etc. with the help of ReactiveCocoa primitives , or just subscribe to it and show results in the view. I inject with the Dependency Injection in all these service classes my APIClient, which will translate a particular service call into corresponding GET, POST, PUT, DELETE, etc. request to the REST endpoint. In this case APIClient is passed implicitly to all controllers, you can make this explicit with a parametrised over APIClient service classes. This can make sense if you want to use different customisations of the APIClient for particular service classes, but if you ,for some reasons, don't want extra copies or you are sure that you always will use one particular instance (without customisations) of the APIClient - make it a singleton, but DON'T, please DON'T make service classes as singletons. 
 Then each view controller again with the DI injects the service class it needs, calls appropriate service methods and composes their results with the UI logic. For dependency injection I like to use BloodMagic or a more powerful framework Typhoon. I never use singletons, God APIManagerWhatever class or other wrong stuff. Because if you call your class WhateverManager, this indicates than you don't know its purpose and it is a bad design choice. Singletons is also an anti-pattern, and in most cases (except rare ones) is a wrong solution. Singleton should be considered only if all three of the following criteria are satisfied:

Ownership of the single instance cannot be reasonably assigned;
Lazy initialization is desirable;
Global access is not otherwise provided for.

In our case ownership of the single instance is not an issue and also we don't need global access after we divided our god manager into services, because now only one or several dedicated controllers need a particular service (e.g. UserProfile controller needs UserServices and so on).
We should always respect S principle in SOLID and use separation of concerns, so don't put all your service methods and networks calls in one class, because it's crazy, especially if you develop a large enterprise application. That's why we should consider dependency injection and services approach. I consider this approach as modern and post-OO. In this case we split our application into two parts: control logic (controllers and events) and parameters.

One kind of parameters would be ordinary __ata_ parameters. That__ what we pass around functions, manipulate, modify, persist, etc. These are entities, aggregates, collections, case classes. The other kind would be __ervice_ parameters. These are classes which encapsulate business logic, allow communicating with external systems, provide data access.

Here is a general workflow of my architecture by example. Let's suppose we have a FriendsViewController, which displays list of user's friends and we have an option to remove from friends. I create a method in my FriendsServices class called:
- (RACSignal *)removeFriend:(Friend * const)friend

where Friend is a model/domain object (or it can be just a User object if they have similar attributes). Underhood this method parses Friend to NSDictionary of JSON parameters friend_id, name, surname, friend_request_id and so on. I always use Mantle library for this kind of boilerplate and for my model layer (parsing back and forward, managing nested object hierarchies in JSON and so on). After parsing it calls APIClient DELETE method to make an actual REST request and returns Response in RACSignal to the caller (FriendsViewController in our case) to display appropriate message for the user or whatever.
 If our application is a very big one, we have to separate our logic even clearer. E.g. it is not always good to mix Repository or model logic with Service one.  When I described my approach I had said that removeFriend method should be in the Service layer, but if we will be more pedantic we can notice that it better belongs to Repository. Let's remember what Repository is. Eric Evans gave it a precise description in his book [DDD]:

A Repository represents all objects of a certain type as a conceptual set. It acts like a collection, except with more elaborate querying capability.

So, a Repository is essentially a facade that uses Collection style semantics (Add, Update, Remove) to supply access to data/objects. That's why when you have something like: getFriendsList, getUserGroups, removeFriend you can place it in the Repository, because collection-like semantics is pretty clear here. And code like: 
- (RACSignal *)approveFriendRequest:(FriendRequest * const)request;

is definitely a business logic, because it is beyond basic CRUD operations and connect two domain objects (Friend and Request), that's why it should be placed in the Service layer. Also I want to notice: don't create unnecessary abstractions. Use all these approaches wisely. Because if you will overwhelm your application with abstractions, this will increase its accidental complexity, and complexity causes more problems in software systems than anything else
 I describe you an ""old"" Objective-C example but this approach can be very easy adapted for Swift language with a lot more improvements, because it has more useful features and functional sugar. I highly recommend to use this library: Moya. It allows you to create a more elegant APIClient layer (our workhorse as you remember). Now our APIClient provider will be a value type (enum) with extensions conforming to protocols and leveraging destructuring pattern matching. Swift enums + pattern matching allows us to create algebraic data types as in classic functional programming. Our microservices will use this improved APIClient provider as in usual Objective-C approach. For model layer instead of Mantle you can use ObjectMapper library or I like to use more elegant and functional Argo library.
So, I described my general architectural approach, which can be adapted for any application, I think. There can be a lot more improvements, of course. I advice you to learn functional programming, because you can benefit from it a lot, but don't go too far with it too. Eliminating excessive, shared, global mutable state, creating an immutable domain model or creating pure functions without external side-effects is, generally, a good practice, and new Swift language encourages this. But always remember, that overloading your code with heavy pure functional patterns, category-theoretical approaches is a bad idea, because other developers will read and support your code, and they can be frustrated or scary of the prismatic profunctors and such kind of stuff in your immutable model. The same thing with the ReactiveCocoa: don't RACify your code too much, because it can become unreadable really fast, especially for newbies. Use it when it can really simplify your goals and logic.
 So, read a lot, mix, experiment, and try to pick up the best from different architectural approaches. It is the best advice I can give you.",https://stackoverflow.com/questions/24162051/best-architectural-approaches-for-building-ios-networking-applications-rest-cli/24168881,12,1
Best architecture for an iOS application that makes many network requests?,"I'm in the process of rethinking my approach to the request architecture of a large app I'm developing. I'm currently using ASIHTTPRequest to actually make requests, but since I need many different types of requests as a result of many different actions taken in different view controllers, I'm trying to work out the best system of organizing these requests.
I'm currently building singleton ""requesters"" that are retained by the app delegate and sit around listening for NSNotifications that signal a request needs to be made; they make the request, listen for the response, and send out a new NSNotification with the response data. This solves most of my problems, but doesn't elegantly handle failed requests or simultaneous requests to the same singleton requester.
Anyone have any success devising a clear, OO architecture for making many different types of requests in an iOS app?","After having tried several approaches, this is one architecture that is giving me excellent results, is easy to document, understand, maintain and extend:

I have a single object taking care of network connectivity, let's call it a ""network manager"". Typically this object is a singleton (created using Matt Gallagher's Cocoa singleton macro).
Since you use ASIHTTPRequest (which I always do, wonderful API) I add an ASINetworkQueue ivar inside my network manager. I make the network manager the delegate of that queue.
I create subclasses of ASIHTTPRequest for each kind of network request that my app requires (typically, for each backend REST interaction or SOAP endpoint). This has another benefit (see below for details :)
Every time one of my controllers requires some data (refresh, viewDidAppear, etc), the network manager creates an instance of the required ASIHTTPRequest subclass, and then adds it to the queue.
The ASINetworkQueue takes care of bandwidth issues (depending on whether you are on 3G, EDGE or GPRS or Wifi, you have more bandwidth, and you can process more requests, etc). This is done by the queue, which is cool (at least, that's one of the things I understand this queue does, I hope I'm not mistaken :).
Whenever a request finishes or fails, the network manager is called (remember, the network manager is the queue's delegate).
The network manager doesn't know squat about what to do with the result of each request; hence, it just calls a method on the request! Remember, requests are subclasses of ASIHTTPRequest, so you can just put the code that manages the result of the request (typically, deserialization of JSON or XML into real objects, triggering other network connections, updating Core Data stores, etc). Putting the code into each separate request subclass, using a polymorphic method with a common name accross request classes, makes it very easy to debug and manage IMHO.
Finally, I notify the controllers above about interesting events using notifications; using a delegate protocol is not a good idea, because in your app you typically have many controllers talking to your network manager, and then notifications are more flexible (you can have several controllers responding to the same notification, etc).

Anyway, this is how I've been doing it for a while, and frankly it works pretty well. I can extend the system horizontally, adding more ASIHTTPRequest subclasses as I need them, and the core of the network manager stays intact.
Hope it helps!",https://stackoverflow.com/questions/4810289/best-architecture-for-an-ios-application-that-makes-many-network-requests,4,1
Architecture/Design with Interfaces (Refactoring help),"I need your help about a design I made that I'm not happy with.
The application is consuming RSS news (The articles's RSS, and the comments's RSS of each article).
I made an interface called IDataService which provide the basic behavior of the data provider.
public interface IDataService
{
    Task<List<Item>> GetItemsAsync(string url, IItemsParser parser);
    Task<List<Comment>> GetItemCommentsAsync(string url, ICommentsParser parser);
}

As you see, each function get as parameters the web-service url (the RSS feed url, in my case) and an interface of some parser that knows how to deal with the data.
These are the interfaces of the two parses:
public interface IItemsParser
{
    List<Item> ParseRawData(string rawData);
}

public interface ICommentsParser
{
    List<Comment> ParseRawData(string rawData);
}

Now let's be concrete for a bit, this is the implementation class:
public class MyRSSDataService : IDataService
{
    public async Task<List<Item>> GetItemsAsync(string url, IItemsParser parser)
    {
        using (var httpClient = new HttpClient())
        {
            var response = await httpClient.GetAsync(new Uri(url));

            if (response.IsSuccessStatusCode)
            {
                var jsonResponse = await response.Content.ReadAsStringAsync();
                List<Item> items = parser.ParseRawData(jsonResponse);

                return items;
            }
            else
            {
                throw new NetworkConnectionException(response.StatusCode.ToString());
            }
        }
    }


    public async Task<List<Comment>> GetItemCommentsAsync(string url, ICommentsParser parser)
    {
        using (var httpClient = new HttpClient())
        {
            var response = await httpClient.GetAsync(new Uri(url));

            if (response.IsSuccessStatusCode)
            {
                var jsonResponse = await response.Content.ReadAsStringAsync();
                List<Comment> comments = parser.ParseRawData(jsonResponse);

                return comments;
            }
            else
            {
                throw new NetworkConnectionException(response.StatusCode.ToString());
            }
        }
    }
}

I feel like I really violate the DRY principle, and I'm not feeling great with the interfaces I made that looks so similar.","Use generics:
public interface IDataService
{
    Task<List<T>> GetDataAsync<T>(string url, IParser<T> parser);
}

public interface IParser<T>
{
    List<T> ParseRawData(string rawData);
}

Then your implementation looks like this:
public class DataService : IDataService
{
    public async Task<List<T>> GetDataAsync<T>(string url, IParser<T> parser)
    {
        // Do work
        return parser.ParseRawData(""blah"");
    }
}

public class ItemParser : IParser<Item>
{
    public List<Item> ParseRawData(string rawData)
    {
        // Do work
    }
}

public class CommentParser : IParser<Comment>
{
    public List<Comment> ParseRawData(string rawData)
    {
        // Do work
    }
}

Now when you use it, it is as simple as:
var x = new DataService();
var y = new ItemParser();

await x.GetDataAsync("""", y);

var z = new CommentParser();
await x.GetDataAsync("""", z);",https://stackoverflow.com/questions/28997293/architecture-design-with-interfaces-refactoring-help,1,1
What are the limitations of refactoring?,I'm making a study on refactoring limitations on improving existing software architecture and I would be interested to hear your experiences where you have found refactoring to be not enough or still too immature to accomplish your goals.,Refactoring code that doesn't have a corresponding set suite of unit test can be risky. If the project already has an established unit test suite then provided that you maintain a TDD approach there should be little reason for concern.,https://stackoverflow.com/questions/146124/what-are-the-limitations-of-refactoring,6,1
What is refactoring?,"I hear the word refactoring everywhere. Any programming tool has some blah-blah about how it helps refactoring, every programmer or a manager will tell me something about refactoring. But to me it still sounds like a magic word without any meaning. It seems that refactoring is just editing your code or what?
Wikipedia quote

Code refactoring is a ""disciplined technique for restructuring an existing body of code, altering its internal structure without changing its external behavior"",[1] undertaken in order to improve some of the nonfunctional attributes of the software. Advantages include improved code readability and reduced complexity to improve the maintainability of the source code, as well as a more expressive internal architecture or object model to improve extensibility.

WHAT? Does every(any)body understand this? Are all those people who talk to me about refactoring, really do mean this?
And why is the name? What's ""factoring"" then?","Refactoring is modifying existing code to improve its readability, re-usability, performance, extensibility and maintainability.  Have you ever looked at code and thought, ""Wow this is a mess"" or ""this could be done better""?  When you start to clean up the code and improve different aspects of it, this is considered refactoring.  Many times code will often repeat itself, requiring you to create abstractions to adhere to the DRY principle, another demonstration of refactoring.  During most refactoring it is important to not break anything, which can be assured by using good unit tests.
Sometimes its best just to get some working code established that solves a particular problem.  Think of this as a rough draft, it just gets the basic ideas established and allows you to think about the problem at hand.  After the rough draft is finished, you return to the code and edit it, making improvements that leads to a final copy (refactoring).  You may eventually receive further requirements that require further code modifications.  At this point the cycle repeats.  Get the initial ideas down in code, then revisit the code and clean it up (refactor it).
One of the main premises behind refactoring is that code can always be improved.  When you make these improvements its refactoring.",https://stackoverflow.com/questions/20624340/what-is-refactoring,4,1
RealmDb : Clean architecture in Android,"I am evaluating RealmDb, I feel RealmDb is tightly coupled with model layer. Which makes me feel if tomorrow I need to replace with some other local database than it will be a huge refactoring effort.
My question is how to achieve clean architecture with RealmDB? Any examples I can follow?","Realm just makes it easier to re-use your database models as your view models if you want. But there is nothing stopping you having data layer entities and view layer entities and then doing the mapping on the boundaries.
E.g.
// Data layer
public class FooEntity extends RealmObject {

  // Realm fields and methods... 

  public static FooEntity fromViewModel(FooViewModel viewModel) {
    // Convert to entity
  }

  public static FooViewModel toViewModel(FooEntity entity) {
    // Convert to view model
  }


}

// View layer
public class FooViewModel {
  // Standard POJO used by the View layer
}

In many cases that is most likely overkill, but it will give you the seperation you want.",https://stackoverflow.com/questions/37085259/realmdb-clean-architecture-in-android,2,1
C++ smart pointer circular link [duplicate],"This question already has an answer here:


How to avoid memory leak with shared_ptr?

                    1 answer
                



I have two types: A & B.
A 'smartly' points B and B 'smartly' points A.
In the main scope I have a smart pointer to A.
class A;
class B;
typedef shared_ptr<A> pA;
typedef shared_ptr<B> pB;

class B {
public:
    B() {}
    pA a;
    virtual ~B() {cout << ""delete b"" << endl;}
};

class A {
public:
    A() : b(new B()) {}
    pB b;
    virtual ~A() {cout << ""delete a"" << endl;}
};

int main(int argc, char **argv)
{
    {
        pA pa(new A());
        pa->b->a = pa;
    }
    cout << ""here"" << endl;

}

I want the two objects to be deleted in the end of the scope.
None of the objects is deleted, because A has two pointers to itself (one in b and one in the main scoop).
This is a simple example. Actually I have more than two types and more pointers inside them.
One can imagine a big structure of objects points to each other floating in the RAM, with only one pointer to this structure from the main scope. I want this structure to be deleted once this pointer is freed.","For places where circular dependency is possible but architectural refactoring is not, the best is to use weak_ptr along with shared_ptr.
struct A {
    std::shared_ptr<B> b;
};

struct B {
    std::weak_ptr<A> a;

};",https://stackoverflow.com/questions/22017671/c-smart-pointer-circular-link/22017766,2,1
How to avoid memory leak with shared_ptr?,"Consider the following code.
using boost::shared_ptr;
struct B;
struct A{
    ~A() { std::cout << ""~A"" << std::endl; }
    shared_ptr<B> b;    
};
struct B {
    ~B() { std::cout << ""~B"" << std::endl; }
    shared_ptr<A> a;
};

int main() {
    shared_ptr<A> a (new A);
    shared_ptr<B> b (new B);
    a->b = b;
    b->a = a;

    return 0;
}

There is no output. No desctructor is called. Memory leak.
I have always believed that the smart pointer helps avoid memory leaks.
What should I do if I need cross-references in the classes?","If you have circular references like this, one object should hold a weak_ptr to the other, not a shared_ptr.
From the shared_ptr introduction:

Because the implementation uses reference counting, cycles of shared_ptr instances will not be reclaimed. For example, if main() holds a shared_ptr to A, which directly or indirectly holds a shared_ptr back to A, A's use count will be 2. Destruction of the original shared_ptr will leave A dangling with a use count of 1. Use weak_ptr  to ""break cycles.""

Thanks, Glen, for the link.",https://stackoverflow.com/questions/1826902/how-to-avoid-memory-leak-with-shared-ptr,1,1
Make sure bad patterns don't come back after refactoring,"I'm refactoring an old C code. The code has absolutely no layered architecture (everything is being accessed by everything) and I'm trying to change that.
I would like to cut direct access to structure members (at least write for now) and only allow access through access functions. Is there some tool (or perhaps directly the compiler) that could check this rule for me?
I need this since I'm maintaining a fork and the upstream isn't very concerned with code quality.","The best way to ensure no new code accesses structures directly is to not make them available using total encapsulation. This comes at the cost of not being able to use a structure on the stack anymore. You provide a function to allocate the structure, another to free it, and all module functions accept a pointer to the structure. However, the definition of the structure itself is in the C file, and not the header file. Another disadvantage is that you may need to write a lot of functions to manipulate/query the structure.
I will provide snippets from an old code base where I've used this approach. The header contains:
#ifndef INC_QUEUE_H
#define INC_QUEUE_H

typedef enum {
    QUE_OK,
    QUE_BAD_PARAM,
    QUE_NO_MEMORY,
    QUE_SYS_ERROR
} QUE_RV;

typedef struct Queue_st Queue_t;

QUE_RV QUE_New(Queue_t **ppQueue);
QUE_RV QUE_Put(Queue_t *pQueue, int priority, void *pData);
QUE_RV QUE_Get(Queue_t *pQueue, int *priority, void **ppData);
void QUE_Free(Queue_t *pQueue);

#endif /* INC_QUEUE_H */

The C file defines the structure Queue_st, and implementations of the functions (heavily modified to highlight the approach):
#include ""queue.h""
#include ""log.h""

#define QUE_INITIAL_CAPACITY 128

struct Queue_st {
    /* SNIP: structure contents go here */
};

QUE_RV QUE_New(Queue_t **ppQueue)
{
    QUE_RV rv;

    *ppQueue = malloc(sizeof(Queue_t));

    /* SNIP: Check malloc, Initialize the structure here ... */

    return QUE_OK;
}

void QUE_Free(Queue_t *pQueue)
{
    if (pQueue != NULL)
    {
        /* SNIP: Free contents of the structure before the free below... */
        free(pQueue);
    }
}

An alternative approach is to use typedef struct StructName *StructHandle;, and replace all the pointers in the API with StructHandle. One less * to worry about.

EDIT: If you want some members visible, and some not, it is also possible with an extension of the above approach. In your header, define:
typedef struct StructPriv StructPriv;

typedef struct {
    /* public members go here */

    StructPriv *private;
} Struct;

Struct *STRUCT_Create();
void STRUCT_Free();

In the C file, define the private members, and the functions that manipulate them.",https://stackoverflow.com/questions/6544877/make-sure-bad-patterns-dont-come-back-after-refactoring,3,1
How do you convince your manager that your project needs a huge refactoring? [closed],"I have joined a rails project as a contractor. The project has been going for more than a year. The code is written by about 10 different developers and most of them are contractors as well. They have different code style. Some of them came from Java. The code has horrible scores with metric_fu. Many functions are very long (100 - 300 lines). Some functions have insane amount of logical branches, loops, and recursions. Each request generates a ton of sql queries. Performance is very bad. Many obsolete code that are never used but never got the chance to be cleaned up. The core architecture is plain wrong or over engineered. Code coverage is only about 25%. Views and partials are chaotic and terrible to read and understand.
The manager is in a position trying to satisfy the CEO by continuously adding new features, however newer features are increasingly hard to get implemented correctly without breaking something else. He knows the code is bad, but doesn't want to put too much effort in fixing them as refactoring will take too long.
As a contractor / developer, what is a good way to clear this situation and convenience the Manager or CEO to partition some time for refactoring? 
Related Questions
How can I convince skeptical management and colleagues to allow refactoring of awful code?
How to refactor on a budget
Dealing with illogical managers","In my limited experiance:

It's impossible to convince a manager that it's necessary to set aside time to refactor. You can make him aware of it, and reinforce the point every time that you run into an issue because of bad code. Then just move on. Hopefully your boss will figure it out.
It's quite common to get in on a running project and think ""this is total junk"". Give it some time. You might begin to see a pattern in the madness.",https://stackoverflow.com/questions/1333133/how-do-you-convince-your-manager-that-your-project-needs-a-huge-refactoring,10,1
Dealing with illogical managers [closed],"At a place I used to work they typical response to any problem was to blame the hardware or the users for not using the system perfectly.  I had adopted the philosophy that it's my fault until I can prove otherwise prior to that job (and so far, at least 99 times out of 100 it's correct).
One of the last ""unsolvable"" problems when I was there was an abundance of database timeouts.  After months of research, I still only had theories but couldn't prove any of them.  One of my developers adamantly suggested replacing the network (every router, switch, access point) but couldn't provide any evidence that the network was the cause; it was, however, ""obviously the cause"" according to my manager (no development/IT experience) so he took over the problem.  One caveat and Fog Creek plug:  He couldn't account for the fact that the error reporting via FogBugz worked perfectly and to the same SQL Server as the rest of the data.
A couple, timeout-free months later, my manager boasted that he had fixed the timeouts (""Look, no timeouts!"").  I had to hold back from grabbing a rock and saying ""Look, no tigers!"" but I did ask how he knew they would have occurred to which I got no response.  The timeouts did return (and in greater numbers) a couple months later.
I'm pretty content with how I handled the situation but I'm curious how the SO crowd would  have responded to letting a superior/colleague implement a solution you know (or are very sure) is wrong and will likely waste thousands of dollars?","Let them, but at the same time continue searching for the real cause.
A couple thousand dollars is money well spent if it keeps me from going against that kind of thinking (which is futile).",https://stackoverflow.com/questions/497234/dealing-with-illogical-managers,5,1
Term describing code that is solely there to fix problems of a suboptimal architecture,"So, there are terms describing pieces of code based on their purpose - like for example:
""Boilerplate Code"": sections of code that have to be included in many places with little or no alteration

or
""Glue Code"":  code that does not contribute any functionality towards meeting the program's requirements, but instead serves solely to ""glue together"" different parts of code that would not otherwise be compatible

Now, if you work with a project that was not properly engineered, you find yourself often adding code (like setting state flags and checking for them in various places) that solely fixes/circumvents arising problems that a properly engineered/refactored architecture would not exhibit.
Is there any term describing such ""code that is only necessary because the architecture is bad""?","I had my own term for this: ""ball of sticky tape"" or ""ball of bandages"".
Some anti patterns that might describe this :
https://en.wikipedia.org/wiki/Big_ball_of_mud
https://en.wikipedia.org/wiki/Stovepipe_system
https://en.wikipedia.org/wiki/Yo-yo_problem
https://en.wikipedia.org/wiki/No_Silver_Bullet (or accidental complexity)
and the old classic :
https://en.wikipedia.org/wiki/Spaghetti_code",https://stackoverflow.com/questions/25804489/term-describing-code-that-is-solely-there-to-fix-problems-of-a-suboptimal-archit,1,1
Looking for some refactoring advice,"I have some code that I had to write to replace a function that was literally used thousands of times.  The problem with the function was that return a pointer to a static allocated buffer and was ridiculously problematic.  I was finally able to prove that intermittent high load errors were caused by the bad practice. 
The function I was replacing has a signature of char * paddandtruncate(char *,int), char * paddandtruncate(float,int), or char * paddandtruncat(int,int).  Each function returned a pointer to a static allocated buffer which was overwritten on subsequent calls. 
I had three constants one the

Code had to be replaceable with no impact on the callers.
Very little time to fix the issue.
Acceptable performance.

I wanted some opinion on the style and possible refactoring ideas.
The system is based upon fixed width fields padded with spaces, and has some architectural issues.  These are not addressable since the size of the project is around 1,000,000 lines.
I was at first planning on allowing the data to be changed after creation, but thought that immutable objects offered a more secure solution.
    using namespace std;
class SYSTEM_DECLSPEC CoreString
{
private:
     friend ostream & operator<<(ostream &os,CoreString &cs);

     stringstream   m_SS          ;
     float          m_FltData     ;
     long           m_lngData     ;
     long           m_Width       ;
     string         m_strData     ;
     string                  m_FormatedData;
     bool           m_Formated    ;
     stringstream    SS            ;


public:

     CoreString(const string &InStr,long Width):
             m_Formated(false),
             m_Width(Width),
             m_strData(InStr)
             {
                     long OldFlags = SS.flags();
                     SS.fill(' ');
                     SS.width(Width);
                     SS.flags(ios::left);
                     SS<<InStr;
                     m_FormatedData = SS.str();
             }

     CoreString(long longData , long Width):
             m_Formated(false),
             m_Width(Width),
             m_lngData(longData)
             {
                     long OldFlags = SS.flags();
                     SS.fill('0');
                     SS.precision(0);
                     SS.width(Width);
                     SS.flags(ios::right);
                     SS<<longData;
                     m_FormatedData = SS.str();
             }

     CoreString(float FltData, long width,long lPerprecision):
             m_Formated(false),
             m_Width(width),
             m_FltData(FltData)
             {
                     long OldFlags = SS.flags();
                     SS.fill('0');
                     SS.precision(lPerprecision);
                     SS.width(width);
                     SS.flags(ios::right);
                     SS<<FltData;
                     m_FormatedData = SS.str();
             }

     CoreString(const string &InStr):
             m_Formated(false),
             m_strData(InStr)
             {
                     long OldFlags = SS.flags();
                     SS.fill(' ');
                     SS.width(32);
                     SS.flags(ios::left);
                     SS<<InStr;
                     m_FormatedData = SS.str();
             }
 public:
     operator const char   *() {return m_FormatedData.c_str();}
     operator const string& () const {return m_FormatedData;}
     const string& str() const ; 

};

const string& CoreString::str() const
{
     return m_FormatedData;
}

ostream & operator<<(ostream &os,CoreString &cs)
{
     os<< cs.m_Formated;
     return os;
 }","If you really do mean ""no impact on the callers"", your choices are very limited. You can't return anything that needs to be freed by the caller.
At the risk of replacing one bad solution with another, the quickest and easiest solution might be this: instead of using a single static buffer, use a pool of them and rotate through them with each call of your function. Make sure the code that chooses a buffer is thread safe.",https://stackoverflow.com/questions/1798538/looking-for-some-refactoring-advice,5,1
Refactoring an ASP.NET 2.0 app to be more __odern_ [closed],"This is a hypothetical scenario.  Let's say you've just been hired at a company with a small development team.  The company uses an internal CRM/ERP type system written in .NET 2.0 to manage all of it's day to day things (let's simplify and say customer accounts and records).  The app was written a couple of years ago when .NET 2.0 was just out and uses the following architectural designs:

Webforms
Data layer is a thin wrapper around SqlCommand that calls stored procedures
Rudimentary DTO-style business objects that are populated via the sprocs
A ""business logic"" layer that acts as a gateway between the webform and database (i.e. code behind calls that layer)

Let's say that as there are more changes and requirements added to the application, you start to feel that the old architecture is showing its age, and changes are increasingly more difficult to make.  How would you go about introducing refactoring steps to A) Modernize the app (i.e. proper separation of concerns) and B) Make sure that the app can readily adapt to change in the organization?
IMO the changes would involve:

Introduce an ORM like Linq to Sql and get rid of the sprocs for CRUD
Assuming that you can't just throw out Webforms, introduce the M-V-P pattern to the forms
Make sure the gateway classes conform to SRP and the other SOLID principles.
Change the logic that is re-used to be web service methods instead of having to reuse code

What are your thoughts?  Again this is a totally hypothetical scenario that many of us have faced in the past, or may end up facing.","You missed the first step that I would go through:
Cost-Benefit Analysis
Refactoring an app because you think it feels old is not a good reason. It's still running (I'm guessing fairly reliably by this point) and your company already has a lot of time and money invested in the code.
You probably also have a team of developers that are familiar with .NET 2.0 and WebForms whereas many may not understand the concepts/code you're trying to introduce.
Before changing anything, figure out how much money is invested, how much money you're going to spend on your changes, and how much money it'll save in the future...
If the numbers don't add up, no business is going to let you proceed.",https://stackoverflow.com/questions/2653639/refactoring-an-asp-net-2-0-app-to-be-more-modern,4,1
How to phrase to business why refactoring is important [closed],"I am currently working on a project with some (in my opinion) architectural problems.
For example - every dependency is retrieved when needed from a common Framework Bean that every object has access to via a static method. This is in effect just a wrapper around spring that will return a Spring bean. No dependency injection.
Entities have references to DAO's to retrieve relational data - hidden in the Entity getters.
Exceptions have references to services to parse and translate error messages.
Every service or DAO inherits from some common abstract framework bean that have their own dependencies and configuration requirements. If you try to do anything else you will get a 'Framework not initialized' error. It should also be mentioned that the Framework is a black box that noone dares touch.
Every test is in effect an integrationn test as all of them requires a working database connection to a central, shared database.
We have a dependency graph that everything is connected to everything, basically. 
And no tests.
Imagine how hard it is to set up a unit test in this environment. In fact every test tries to insert test data and clean up after itself - without using any kind of transaction.
And I'm only scratching the surface here.
Needless to say, I worry a little about the code quality. The project is (of course) running low on time and resources, and deadline is approaching. 
So - How to convince management in a language they understand that refactoring is needed if we shall have any hope of delivering something that resembles functional software?","In my opinion the best approach is:

Metrics: Collect all the possible data about the problems caused by the use of the current architecture. Analyze the information and separate it in topics of interest like: reliability, efficiency, security and Maintainability. (See: Software quality) 
Visual: Make beautiful bar charts and pie charts with the analyzed info.
Diff: Compare your results with an standard or ideal metrics.

With this you are ready to present your point of view of the project to the management.
Usually this work is responsibility of project manager, team manager, or team leader.
Good luck!",https://stackoverflow.com/questions/14714397/how-to-phrase-to-business-why-refactoring-is-important,4,1
What should I keep in mind in order to refactor huge code base?,"I'm going to refactor certain parts in a huge code base (18000+ Java classes). Goal is to be able to extract lower layers as independent libraries to be reused in other projects that currently use duplicate of this code base. Especially one part is of interest to be refactored into a framework independent of business logic. Ultimately I would like the code to have a clean architectural layering.
I've looked at the code with a tool called Structure 101 for java and found lots (!) of architectural layering issues where lower layers are referencing upper layers.
I don't want to simply start messing with the code but try to come up with a reasonable strategy to go about this problem. What things should I keep in mind?
I'm thinking about at least taking small steps. I'm also thinking about have unit tests in place, but that requires creating them, since there are none.
Any thoughts on this?","You should also take a look at Working with legacy code by Michael Feathers:
http://www.amazon.com/Working-Effectively-Legacy-Robert-Martin/dp/0131177052/ref=sr_1_1?ie=UTF8&s=books&qid=1242430219&sr=8-1
I think one of the most important things you can put in place to facilitate this are tests to ensure that everything still works post refactoring/pulling out into separate modules.  Add to this by introducing a continuous integration system that runs your tests when you check something in.",https://stackoverflow.com/questions/871238/what-should-i-keep-in-mind-in-order-to-refactor-huge-code-base,9,1
Are scenario tests groups of sequential unit tests?,"I read the Wikipedia article on scenario testing, but I am sad to say it is very short.  I am left wondering: are scenario tests a collection of sequential unit tests?  Or, perhaps, like a single multi-step unit test?  Do many frameworks support scenario tests, or are they covered by unit testing?
If they have nothing to do with automation, what are they?","I don't think there's any fixed relationship between the number and distribution of tests and scenario tests.
I think the most common code-representation of a scenario is a specific set of business data required to support a specific story (scenario). This is often provided in the form of database data, fake stub data or a combination of both.
The idea is that this dataset has known and well-defined characteristics that will provide well defined results all across a given business process. 
For a web application I could have a single web-test (or several for variations) that click through the full scenario. In other cases the scenario is used at a lower level, possibly testing a part of the scenario in a functional test or a unit test. In this case I normally never group the tests by scenario, but choose the functional grouping of tests I normally use for unit/functional tests. Quite often there's a method within ""Subsystem1Test"" that is called ""testScenario1"" or maybe ""testScenarioInsufficientCredit"". I prefer to give my scenarios names.",https://stackoverflow.com/questions/351983/are-scenario-tests-groups-of-sequential-unit-tests,3,1
How do you maintain development code and production code? [closed],"What are the best practices and rules-of-thumb to follow while maintaining code?  Is it good practice to have only the production ready code in the development branch, or should untested latest code be available in the development branch?
How do you guys maintain your development code and production code?
Edit - Supplementary question - Does your development team follow ""commit-as-soon-as-possible-and-often-even-if-the-code-contains-minor-bugs-or-is-incomplete"" protocol or ""commit-ONLY-perfect-code"" protocol while committing code to DEVELOPMENT branch?","Update 2019:
These days, the question would be seen in a context using Git, and 10 years of using that distributed development workflow (collaborating mainly through GitHub) shows the general best practices:

master is the branch ready to be deployed into production at any time: the next release, with a selected set of feature branches merged in master.
dev (or integration branch, or 'next') is the one where the feature branch selected for the next release are tested together
maintenance (or hot-fix) branch is the one for the current release evolution/bug fixes, with possible merges back to dev and or master

That kind of workflow (where you don't merge dev to master, but where you merge only feature branch to dev, then if selected, to master, in order to be able to drop easily feature branches not ready for the next release) is implemented in the Git repo itself, with the gitworkflow (one word, illustrated here).
See more at rocketraman/gitworkflow.
 
(source: Gitworkflow: A Task-Oriented Primer)
Note: in that distributed workflow, you can commit whenever you want and push to a personal branch some WIP (Work In Progress) without issue: you will be able to reorganize (git rebase) your commits before making them part of a feature branch.

Original answer (Oct. 2008, 10+ years ago)
It all depends of the sequential nature of your release management
First, is everything in your trunk really for the next release?
You might find out that some of the currently developed functions are:

too complicated and still need to be refined
not ready in time
interesting but not for this next release

In this case, trunk should contain any current development efforts, but a release branch defined early before the next release can serve as consolidation branch in which only the appropriate code (validated for the next release) is merged, then fixed during the homologation phase, and finally freezed as it goes into production.
When it comes to production code, you also need to manage your patch branches, while keeping in mind that:

the first set of patches might actually begin before to first release into production (meaning you know you will go into production with some bugs you can not fix in time, but you can initiate work for those bugs in a separate branch)
the other patch branches will have the luxury to start from a well-defined production label

When it comes to dev branch, you can have one trunk, unless you have other development efforts you need to make in parallel like:

massive refactoring
testing of a new technical library which might change the way you calls things in other classes
beginning of a new release cycle where important architectural changes need to be incorporated.

Now, if your development-release cycle is very sequential, you can just go as the other answers suggest: one trunk and several release branches. That works for small projects where all the development is sure to go into the next release, and can just be freezed and serve as a starting point for release branch, where patches can take place. That is the nominal process, but as soon as you have a more complex project... it is not enough anymore.

To answer Ville M.'s comment:

keep in mind that dev branch does not mean 'one branch per developer' (which would trigger 'merge madness', in that each developer would have to merge the work of other to see/get their work), but one dev branch per development effort.
When those efforts need to be merged back into trunk (or any other ""main"" or release branch you define), this is the work of the developer, not - I repeat, NOT - the SC Manager (who would not know how to solve any conflicting merge). The project leader may supervise the merge, meaning make sure it starts/finish on time.
whoever you choose for actually doing the merge, the most important is:


to have unit tests and/or assembly environment in which you can deploy/test the result of the merge.
to have defined a tag before the beginning of the merge in order to be able to get back to previous state if said merge proves itself too complex or rather long to resolve.",https://stackoverflow.com/questions/216212,12,1